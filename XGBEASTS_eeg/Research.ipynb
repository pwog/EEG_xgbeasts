{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4074: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from googlenet_lite import create_model\n",
    "from keras import optimizers\n",
    "\n",
    "input_shape = (28,28,3)\n",
    "classes = 2\n",
    "\n",
    "optimizer = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "GoogLeNet = create_model(input_shape=input_shape, classes=classes)\n",
    "# loss_weights - weighting losses of main classifier and auxiliary classifiers\n",
    "GoogLeNet.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics = ['accuracy'], loss_weights=[1., 0.3, 0.3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 1592 samples, validate on 796 samples\n",
      "Epoch 1/15\n",
      "1592/1592 [==============================] - 52s 33ms/step - loss: 3.1068 - dense_5_loss: 0.8152 - dense_2_loss: 0.8573 - dense_4_loss: 0.7580 - dense_5_accuracy: 0.4950 - dense_2_accuracy: 0.5151 - dense_4_accuracy: 0.4906 - val_loss: 2.5779 - val_dense_5_loss: 0.6931 - val_dense_2_loss: 0.6935 - val_dense_4_loss: 0.6934 - val_dense_5_accuracy: 0.5000 - val_dense_2_accuracy: 0.5000 - val_dense_4_accuracy: 0.5000\n",
      "Epoch 2/15\n",
      "1592/1592 [==============================] - 20s 13ms/step - loss: 2.4075 - dense_5_loss: 0.6933 - dense_2_loss: 0.6737 - dense_4_loss: 0.6929 - dense_5_accuracy: 0.5075 - dense_2_accuracy: 0.5559 - dense_4_accuracy: 0.5226 - val_loss: 2.2749 - val_dense_5_loss: 0.6932 - val_dense_2_loss: 0.6942 - val_dense_4_loss: 0.6932 - val_dense_5_accuracy: 0.5000 - val_dense_2_accuracy: 0.5226 - val_dense_4_accuracy: 0.5000\n",
      "Epoch 3/15\n",
      "1592/1592 [==============================] - 20s 13ms/step - loss: 2.1687 - dense_5_loss: 0.6892 - dense_2_loss: 0.6549 - dense_4_loss: 0.6804 - dense_5_accuracy: 0.5289 - dense_2_accuracy: 0.6062 - dense_4_accuracy: 0.5691 - val_loss: 2.1094 - val_dense_5_loss: 0.6931 - val_dense_2_loss: 0.6910 - val_dense_4_loss: 0.6931 - val_dense_5_accuracy: 0.5000 - val_dense_2_accuracy: 0.5415 - val_dense_4_accuracy: 0.5000\n",
      "Epoch 4/15\n",
      "1592/1592 [==============================] - 20s 13ms/step - loss: 2.0414 - dense_5_loss: 0.6912 - dense_2_loss: 0.6568 - dense_4_loss: 0.6911 - dense_5_accuracy: 0.5144 - dense_2_accuracy: 0.6112 - dense_4_accuracy: 0.5452 - val_loss: 2.0038 - val_dense_5_loss: 0.6932 - val_dense_2_loss: 0.7029 - val_dense_4_loss: 0.6930 - val_dense_5_accuracy: 0.5075 - val_dense_2_accuracy: 0.5025 - val_dense_4_accuracy: 0.5000\n",
      "Epoch 5/15\n",
      "1592/1592 [==============================] - 20s 13ms/step - loss: 1.9372 - dense_5_loss: 0.6862 - dense_2_loss: 0.6546 - dense_4_loss: 0.6825 - dense_5_accuracy: 0.5421 - dense_2_accuracy: 0.5999 - dense_4_accuracy: 0.5616 - val_loss: 1.9169 - val_dense_5_loss: 0.6932 - val_dense_2_loss: 0.6918 - val_dense_4_loss: 0.6914 - val_dense_5_accuracy: 0.5000 - val_dense_2_accuracy: 0.5389 - val_dense_4_accuracy: 0.5465\n",
      "Epoch 6/15\n",
      "1592/1592 [==============================] - 20s 13ms/step - loss: 1.8162 - dense_5_loss: 0.6513 - dense_2_loss: 0.6494 - dense_4_loss: 0.6470 - dense_5_accuracy: 0.6237 - dense_2_accuracy: 0.6250 - dense_4_accuracy: 0.6281 - val_loss: 1.8471 - val_dense_5_loss: 0.6900 - val_dense_2_loss: 0.6910 - val_dense_4_loss: 0.6913 - val_dense_5_accuracy: 0.5339 - val_dense_2_accuracy: 0.5440 - val_dense_4_accuracy: 0.5251\n",
      "Epoch 7/15\n",
      "1592/1592 [==============================] - 20s 13ms/step - loss: 1.7338 - dense_5_loss: 0.6396 - dense_2_loss: 0.6326 - dense_4_loss: 0.6348 - dense_5_accuracy: 0.6357 - dense_2_accuracy: 0.6558 - dense_4_accuracy: 0.6470 - val_loss: 1.8025 - val_dense_5_loss: 0.6960 - val_dense_2_loss: 0.7053 - val_dense_4_loss: 0.6962 - val_dense_5_accuracy: 0.5251 - val_dense_2_accuracy: 0.5251 - val_dense_4_accuracy: 0.5251\n",
      "Epoch 8/15\n",
      "1000/1592 [=================>............] - ETA: 6s - loss: 1.6771 - dense_5_loss: 0.6306 - dense_2_loss: 0.6255 - dense_4_loss: 0.6278 - dense_5_accuracy: 0.6650 - dense_2_accuracy: 0.6630 - dense_4_accuracy: 0.6610"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-81c80c7e7542>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                             \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                             verbose=1)\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3476\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.load('X.npy')\n",
    "y = np.load('y.npy')\n",
    "\n",
    "train_index =[8,9,10,11]\n",
    "test_index = [12,13]\n",
    "\n",
    "X_train, X_test = X[train_index].reshape((X.shape[1]*4, 28, 28, 3)), X[test_index].reshape((y.shape[1]*2, 28, 28, 3))\n",
    "y_train, y_test = y[train_index].reshape((X.shape[1]*4, 2)), y[test_index].reshape((y.shape[1]*2, 2))\n",
    "        \n",
    "GoogLeNet.fit(X_train, [y_train, y_train, y_train], epochs=15,\n",
    "                            batch_size=50,\n",
    "                            validation_data=(X_test, [y_test, y_test, y_test]),\n",
    "                            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as scs\n",
    "import re\n",
    "from numpy import genfromtxt\n",
    "from eeg_learn_functions import gen_images\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "plt.rcParams[\"figure.figsize\"] = (12,12)\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.precision = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import scale\n",
    "batch_size = 32\n",
    "num_classes = 2\n",
    "epochs = 300\n",
    "def get_model(input_shape = (28, 28, 3)):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3, 3), padding='same',input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brainwave Frequencies:\n",
    "Gamma, 30 to 50 Hz.  \n",
    "Beta, 14 to 30 Hz.  \n",
    "Alpha, 8 to 14 Hz.  \n",
    "Theta, 4 to 8 Hz.  \n",
    "Delta, 0.1 to 4 Hz.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing Bin Size: \n",
    "https://stackoverflow.com/questions/25735153/plotting-a-fast-fourier-transform-in-python  \n",
    "(Search for 'bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An EEG processing library:  \n",
    "https://github.com/pbashivan/EEGLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = (0.1, 4)\n",
    "theta = (4,8)\n",
    "alpha = (8,14)\n",
    "beta = (14,30)\n",
    "gamma = (30, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fft(snippet):\n",
    "    Fs = 128.0;  # sampling rate\n",
    "    #Ts = len(snippet)/Fs/Fs; # sampling interval\n",
    "    snippet_time = len(snippet)/Fs\n",
    "    Ts = 1.0/Fs; # sampling interval\n",
    "    t = np.arange(0,snippet_time,Ts) # time vector\n",
    "\n",
    "    # ff = 5;   # frequency of the signal\n",
    "    # y = np.sin(2*np.pi*ff*t)\n",
    "    y = snippet\n",
    "#     print('Ts: ',Ts)\n",
    "#     print(t)\n",
    "#     print(y.shape)\n",
    "    n = len(y) # length of the signal\n",
    "    k = np.arange(n)\n",
    "    T = n/Fs\n",
    "    frq = k/T # two sides frequency range\n",
    "    frq = frq[range(n//2)] # one side frequency range\n",
    "\n",
    "    Y = np.fft.fft(y)/n # fft computing and normalization\n",
    "    Y = Y[range(n//2)]\n",
    "    #Added in: (To remove bias.)\n",
    "    #Y[0] = 0\n",
    "    return frq,abs(Y)\n",
    "#f,Y = get_fft(np.hanning(len(snippet))*snippet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theta_alpha_beta_averages(f,Y):\n",
    "    theta_range = (4,8)\n",
    "    alpha_range = (8,12)\n",
    "    beta_range = (12,30)\n",
    "    theta = Y[(f>theta_range[0]) & (f<=theta_range[1])].mean()\n",
    "    alpha = Y[(f>alpha_range[0]) & (f<=alpha_range[1])].mean()\n",
    "    beta = Y[(f>beta_range[0]) & (f<=beta_range[1])].mean()\n",
    "    return theta, alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_steps(samples,frame_duration,overlap):\n",
    "    '''\n",
    "    in:\n",
    "    samples - number of samples in the session\n",
    "    frame_duration - frame duration in seconds \n",
    "    overlap - float fraction of frame to overlap in range (0,1)\n",
    "    \n",
    "    out: list of tuple ranges\n",
    "    '''\n",
    "    #steps = np.arange(0,len(df),frame_length)\n",
    "    Fs = 128.0\n",
    "    i = 0\n",
    "    intervals = []\n",
    "    samples_per_frame = 100#Fs * frame_duration\n",
    "    while i+samples_per_frame <= samples:\n",
    "        intervals.append((i,i+samples_per_frame))\n",
    "        i = i + samples_per_frame - int(samples_per_frame*overlap)\n",
    "    return intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(x):\n",
    "    new = []\n",
    "    for i in range(x.shape[0]):\n",
    "        if(i == 0):\n",
    "            new.append((x[i]+x[i+1])/2)\n",
    "        elif(i == x.shape[0]-1):\n",
    "            new.append((x[i]+x[i-1])/2)\n",
    "        else:\n",
    "            new.append((x[i-1]+x[i]+x[i+1])/3)\n",
    "    return np.array(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_frames(df,frame_duration):\n",
    "    '''\n",
    "    in: dataframe or array with all channels, frame duration in seconds\n",
    "    out: array of theta, alpha, beta averages for each probe for each time step\n",
    "        shape: (n-frames,m-probes,k-brainwave bands)\n",
    "    '''\n",
    "    Fs = 128.0\n",
    "    frame_length = 100#Fs*frame_duration\n",
    "    frames = []\n",
    "    steps = make_steps(len(df),frame_duration,overlap)\n",
    "    for i,_ in enumerate(steps):\n",
    "        frame = []\n",
    "        if i == 0:\n",
    "            continue\n",
    "        else:\n",
    "            for channel in df.columns:\n",
    "                snippet = np.array(df.loc[steps[i][0]:steps[i][1],int(channel)])\n",
    "                f,Y =  get_fft(snippet)\n",
    "                theta, alpha, beta = theta_alpha_beta_averages(f,Y)\n",
    "                frame.append([theta, alpha, beta])\n",
    "            \n",
    "        frames.append(frame)\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs_2d = [(-2.0,4.0),\n",
    "           (2.0,4.0),\n",
    "           (-1.0,3.0),\n",
    "           (1.0,3.0),\n",
    "           (-3.0,3.0),\n",
    "           (3.0,3.0),\n",
    "           (-2.0,2.0),\n",
    "           (2.0,2.0),\n",
    "           (-2.0,-2.0),\n",
    "           (2.0,-2.0),\n",
    "           (-4.0,1.0),\n",
    "           (4.0,1.0),\n",
    "           (-1.0,-3.0),\n",
    "           (1.0,-3.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "def make_data_pipeline(file_names,labels,image_size,frame_duration,overlap, normalize = False):\n",
    "    '''\n",
    "    IN: \n",
    "    file_names - list of strings for each input file (one for each subject)\n",
    "    labels - list of labels for each\n",
    "    image_size - int size of output images in form (x, x)\n",
    "    frame_duration - time length of each frame (seconds)\n",
    "    overlap - float fraction of frame to overlap in range (0,1)\n",
    "    \n",
    "    OUT:\n",
    "    X: np array of frames (unshuffled)\n",
    "    y: np array of label for each frame (1 or 0)\n",
    "    '''\n",
    "    ##################################\n",
    "    ###Still need to do the overlap###!!!\n",
    "    ##################################\n",
    "    \n",
    "    Fs = 128.0   #sampling rate\n",
    "    frame_length = 100#Fs * frame_duration\n",
    "    \n",
    "    print('Generating training data...')\n",
    "    \n",
    "    \n",
    "    for i, file in enumerate(file_names):\n",
    "        print ('Processing session: ',file, '. (',i+1,' of ',len(file_names),')')\n",
    "        data = genfromtxt(file, delimiter=',')\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        X_0 = make_frames(df,frame_duration)\n",
    "        #steps = np.arange(0,len(df),frame_length)\n",
    "        X_1 = X_0.reshape(len(X_0),14*3)\n",
    "        \n",
    "        images = gen_images(np.array(locs_2d),X_1, image_size, normalize=False)\n",
    "        images = np.swapaxes(images, 1, 3) \n",
    "        print(len(images), ' frames generated with label ', labels[i], '.')\n",
    "        print('\\n')\n",
    "        if i == 0:\n",
    "            X = images\n",
    "            y = np.ones(len(images))*labels[0]\n",
    "        else:\n",
    "            X = np.concatenate((X,images),axis = 0)\n",
    "            y = np.concatenate((y,np.ones(len(images))*labels[i]),axis = 0)\n",
    "    print(X.shape)\n",
    "    if(normalize):\n",
    "        X_r = X[:,:,:,0].reshape((X.shape[0]*image_size, image_size))\n",
    "        X_g = X[:,:,:,1].reshape((X.shape[0]*image_size, image_size))\n",
    "        X_b = X[:,:,:,2].reshape((X.shape[0]*image_size, image_size))\n",
    "\n",
    "        X[:,:,:,0] = scale(X_r, axis = 1).reshape((X.shape[0], X.shape[1], X.shape[2])) \n",
    "        X[:,:,:,1] = scale(X_g, axis = 1).reshape((X.shape[0], X.shape[1], X.shape[2])) \n",
    "        X[:,:,:,2] = scale(X_b, axis = 1).reshape((X.shape[0], X.shape[1], X.shape[2])) \n",
    "    \n",
    "        #with open('scalers_dump.pickle', 'wb') as f:\n",
    "        #    pickle.dump(scalers, f)\n",
    "    return X,np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def auroc(y_true, y_pred):\n",
    "    return tf.py_function(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training data...\n",
      "Processing session:  data/data_train_new_appr_10_label1.csv . ( 1  of  1 )\n",
      "995erpolating 995/995nterpolating 67/995Interpolating 101/995Interpolating 174/995Interpolating 208/995Interpolating 236/995Interpolating 272/995Interpolating 339/995Interpolating 376/995Interpolating 441/995Interpolating 474/995Interpolating 509/995Interpolating 545/995Interpolating 582/995Interpolating 615/995Interpolating 651/995Interpolating 689/995Interpolating 722/995Interpolating 754/995Interpolating 790/995Interpolating 827/995Interpolating 859/995Interpolating 895/995Interpolating 932/995Interpolating 968/995  frames generated with label  1 .\n",
      "\n",
      "\n",
      "(995, 28, 28, 3)\n",
      "Generating training data...\n",
      "Processing session:  data/data_train_new_appr_9_label0.csv . ( 1  of  1 )\n",
      "995  frames generated with label  0 .995Interpolating 104/995Interpolating 140/995Interpolating 209/995Interpolating 244/995Interpolating 281/995Interpolating 317/995Interpolating 346/995Interpolating 381/995Interpolating 419/995Interpolating 453/995Interpolating 486/995Interpolating 523/995Interpolating 558/995Interpolating 627/995Interpolating 661/995Interpolating 730/995Interpolating 866/995Interpolating 903/995Interpolating 941/995Interpolating 969/995\n",
      "\n",
      "\n",
      "(995, 28, 28, 3)\n",
      "Generating training data...\n",
      "Processing session:  data/data_train_new_appr_12_label1.csv . ( 1  of  1 )\n",
      "995  frames generated with label  1 .995Interpolating 77/995Interpolating 114/995Interpolating 146/995Interpolating 183/995Interpolating 220/995Interpolating 257/995Interpolating 295/995Interpolating 327/995Interpolating 361/995Interpolating 398/995Interpolating 432/995Interpolating 466/995Interpolating 503/995Interpolating 540/995Interpolating 576/995Interpolating 651/995Interpolating 688/995Interpolating 724/995Interpolating 797/995Interpolating 835/995Interpolating 871/995Interpolating 907/995Interpolating 944/995Interpolating 982/995\n",
      "\n",
      "\n",
      "(995, 28, 28, 3)\n",
      "Generating training data...\n",
      "Processing session:  data/data_train_new_appr_11_label0.csv . ( 1  of  1 )\n",
      "995  frames generated with label  0 .995Interpolating 104/995Interpolating 142/995Interpolating 216/995Interpolating 250/995Interpolating 283/995Interpolating 320/995Interpolating 357/995Interpolating 430/995Interpolating 532/995Interpolating 567/995Interpolating 602/995Interpolating 631/995Interpolating 668/995Interpolating 706/995Interpolating 739/995Interpolating 776/995Interpolating 814/995Interpolating 849/995Interpolating 880/995Interpolating 918/995Interpolating 956/995Interpolating 993/995\n",
      "\n",
      "\n",
      "(995, 28, 28, 3)\n",
      "Generating training data...\n",
      "Processing session:  data/data_train_new_appr_14_label1.csv . ( 1  of  1 )\n",
      "995  frames generated with label  1 .995Interpolating 75/995Interpolating 112/995Interpolating 149/995Interpolating 184/995Interpolating 222/995Interpolating 259/995Interpolating 297/995Interpolating 373/995Interpolating 411/995Interpolating 448/995Interpolating 486/995Interpolating 523/995Interpolating 560/995Interpolating 634/995Interpolating 671/995Interpolating 709/995Interpolating 745/995Interpolating 778/995Interpolating 838/995Interpolating 873/995Interpolating 904/995Interpolating 940/995Interpolating 972/995\n",
      "\n",
      "\n",
      "(995, 28, 28, 3)\n",
      "Generating training data...\n",
      "Processing session:  data/data_train_new_appr_13_label0.csv . ( 1  of  1 )\n",
      "995  frames generated with label  0 .995Interpolating 77/995Interpolating 115/995Interpolating 152/995Interpolating 227/995Interpolating 264/995Interpolating 302/995Interpolating 339/995Interpolating 412/995Interpolating 449/995Interpolating 487/995Interpolating 563/995Interpolating 638/995Interpolating 677/995Interpolating 714/995Interpolating 752/995Interpolating 790/995Interpolating 824/995Interpolating 862/995Interpolating 898/995Interpolating 937/995Interpolating 974/995\n",
      "\n",
      "\n",
      "(995, 28, 28, 3)\n",
      "Generating training data...\n",
      "Processing session:  data/data_train_new_appr_21_label1.csv . ( 1  of  1 )\n",
      "995  frames generated with label  1 .995Interpolating 98/995Interpolating 134/995Interpolating 171/995Interpolating 207/995Interpolating 244/995Interpolating 313/995Interpolating 351/995Interpolating 388/995Interpolating 499/995Interpolating 526/995Interpolating 582/995Interpolating 617/995Interpolating 654/995Interpolating 690/995Interpolating 726/995Interpolating 762/995Interpolating 837/995Interpolating 876/995Interpolating 913/995Interpolating 951/995Interpolating 989/995\n",
      "\n",
      "\n",
      "(995, 28, 28, 3)\n",
      "Generating training data...\n",
      "Processing session:  data/data_train_new_appr_22_label0.csv . ( 1  of  1 )\n",
      "995  frames generated with label  0 .995Interpolating 76/995Interpolating 114/995Interpolating 152/995Interpolating 190/995Interpolating 228/995Interpolating 266/995Interpolating 303/995Interpolating 341/995Interpolating 378/995Interpolating 416/995Interpolating 454/995Interpolating 489/995Interpolating 527/995Interpolating 560/995Interpolating 593/995Interpolating 680/995Interpolating 715/995Interpolating 840/995Interpolating 909/995Interpolating 944/995Interpolating 980/995\n",
      "\n",
      "\n",
      "(995, 28, 28, 3)\n",
      "Generating training data...\n",
      "Processing session:  data/data_train_new_appr_23_label1.csv . ( 1  of  1 )\n",
      "995  frames generated with label  1 .995Interpolating 71/995Interpolating 139/995Interpolating 242/995Interpolating 277/995Interpolating 312/995Interpolating 346/995Interpolating 443/995Interpolating 478/995Interpolating 513/995Interpolating 549/995Interpolating 583/995Interpolating 619/995Interpolating 654/995Interpolating 689/995Interpolating 722/995Interpolating 751/995Interpolating 782/995Interpolating 818/995Interpolating 855/995Interpolating 892/995Interpolating 928/995Interpolating 965/995\n",
      "\n",
      "\n",
      "(995, 28, 28, 3)\n",
      "Generating training data...\n",
      "Processing session:  data/data_train_new_appr_24_label0.csv . ( 1  of  1 )\n",
      "995  frames generated with label  0 .995Interpolating 61/995Interpolating 91/995Interpolating 125/995Interpolating 153/995Interpolating 214/995Interpolating 250/995Interpolating 285/995Interpolating 358/995Interpolating 395/995Interpolating 433/995Interpolating 470/995Interpolating 507/995Interpolating 581/995Interpolating 654/995Interpolating 692/995Interpolating 726/995Interpolating 848/995Interpolating 880/995Interpolating 912/995Interpolating 949/995Interpolating 986/995\n",
      "\n",
      "\n",
      "(995, 28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import keras\n",
    "image_size = 28\n",
    "frame_duration = 0.78\n",
    "overlap = 0.8\n",
    "num_classes = 2\n",
    "\n",
    "file_names = ['data/data_train_new_appr_1_label1.csv',\n",
    "              'data/data_train_new_appr_2_label0.csv',\n",
    "              'data/data_train_new_appr_3_label1.csv',\n",
    "              'data/data_train_new_appr_4_label0.csv',\n",
    "              'data/data_train_new_appr_6_label1.csv',\n",
    "              'data/data_train_new_appr_5_label0.csv',\n",
    "              'data/data_train_new_appr_8_label1.csv',\n",
    "              'data/data_train_new_appr_7_label0.csv',\n",
    "              'data/data_train_new_appr_10_label1.csv',\n",
    "              'data/data_train_new_appr_9_label0.csv',\n",
    "              'data/data_train_new_appr_12_label1.csv',\n",
    "              'data/data_train_new_appr_11_label0.csv',\n",
    "              'data/data_train_new_appr_14_label1.csv',\n",
    "              'data/data_train_new_appr_13_label0.csv',\n",
    "              'data/data_train_new_appr_15_label1.csv',\n",
    "              'data/data_train_new_appr_16_label0.csv',\n",
    "              'data/data_train_new_appr_17_label1.csv',\n",
    "              'data/data_train_new_appr_18_label0.csv',\n",
    "              'data/data_train_new_appr_19_label1.csv',\n",
    "              'data/data_train_new_appr_20_label0.csv'\n",
    "              ]\n",
    "file_names_good = ['data/data_train_new_appr_10_label1.csv',\n",
    "              'data/data_train_new_appr_9_label0.csv',\n",
    "              'data/data_train_new_appr_12_label1.csv',\n",
    "              'data/data_train_new_appr_11_label0.csv',\n",
    "              'data/data_train_new_appr_14_label1.csv',\n",
    "              'data/data_train_new_appr_13_label0.csv',\n",
    "              'data/data_train_new_appr_21_label1.csv',\n",
    "              'data/data_train_new_appr_22_label0.csv', \n",
    "              'data/data_train_new_appr_23_label1.csv',\n",
    "              'data/data_train_new_appr_24_label0.csv' ]\n",
    "labels = [1,0,1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,  1, 0, 1, 0]\n",
    "labels_good = [1,0,1,0,1,0,1,0,1,0,1,0]\n",
    "X_transformed_list = []\n",
    "y_transformed_list = []\n",
    "for f_name, lbl in zip(file_names_good, labels_good):\n",
    "    X, y = make_data_pipeline([f_name],[lbl],image_size,frame_duration,overlap, normalize = True)\n",
    "    X = X.astype('float32')\n",
    "    y = keras.utils.to_categorical(y, num_classes)\n",
    "    X_transformed_list.append(X)\n",
    "    y_transformed_list.append(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 995, 28, 28, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_transformed_list_np = np.array(X_transformed_list)\n",
    "y_transformed_list_np = np.array(y_transformed_list)\n",
    "X_transformed_list_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 2  3  4  5  6  7  8  9 10 11] TEST: [0 1]\n",
      "Train on 9950 samples, validate on 1990 samples\n",
      "Epoch 1/50\n",
      "9950/9950 [==============================] - 8s 832us/step - loss: 0.6846 - accuracy: 0.5643 - auc_2: 0.5534 - val_loss: 0.6653 - val_accuracy: 0.6251 - val_auc_2: 0.6122\n",
      "Epoch 2/50\n",
      "9950/9950 [==============================] - 8s 815us/step - loss: 0.6696 - accuracy: 0.6159 - auc_2: 0.6139 - val_loss: 0.6497 - val_accuracy: 0.6472 - val_auc_2: 0.6369\n",
      "Epoch 3/50\n",
      "9950/9950 [==============================] - 8s 808us/step - loss: 0.6568 - accuracy: 0.6302 - auc_2: 0.6400 - val_loss: 0.6341 - val_accuracy: 0.6839 - val_auc_2: 0.6497\n",
      "Epoch 4/50\n",
      "9950/9950 [==============================] - 8s 808us/step - loss: 0.6470 - accuracy: 0.6460 - auc_2: 0.6578 - val_loss: 0.6314 - val_accuracy: 0.6668 - val_auc_2: 0.6620\n",
      "Epoch 5/50\n",
      "9950/9950 [==============================] - 8s 808us/step - loss: 0.6387 - accuracy: 0.6517 - auc_2: 0.6697 - val_loss: 0.6295 - val_accuracy: 0.6714 - val_auc_2: 0.6722\n",
      "Epoch 6/50\n",
      "9950/9950 [==============================] - 8s 809us/step - loss: 0.6318 - accuracy: 0.6587 - auc_2: 0.6774 - val_loss: 0.6290 - val_accuracy: 0.6613 - val_auc_2: 0.6800\n",
      "Epoch 7/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.6269 - accuracy: 0.6649 - auc_2: 0.6838 - val_loss: 0.6321 - val_accuracy: 0.6573 - val_auc_2: 0.6852\n",
      "Epoch 8/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.6223 - accuracy: 0.6687 - auc_2: 0.6888 - val_loss: 0.6323 - val_accuracy: 0.6518 - val_auc_2: 0.6899\n",
      "Epoch 9/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.6181 - accuracy: 0.6728 - auc_2: 0.6926 - val_loss: 0.6370 - val_accuracy: 0.6462 - val_auc_2: 0.6935\n",
      "Epoch 10/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.6140 - accuracy: 0.6761 - auc_2: 0.6961 - val_loss: 0.6418 - val_accuracy: 0.6337 - val_auc_2: 0.6968\n",
      "Epoch 11/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.6105 - accuracy: 0.6783 - auc_2: 0.6991 - val_loss: 0.6507 - val_accuracy: 0.6327 - val_auc_2: 0.6991\n",
      "Epoch 12/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.6070 - accuracy: 0.6800 - auc_2: 0.7016 - val_loss: 0.6468 - val_accuracy: 0.6317 - val_auc_2: 0.7018\n",
      "Epoch 13/50\n",
      "9950/9950 [==============================] - 8s 807us/step - loss: 0.6042 - accuracy: 0.6803 - auc_2: 0.7039 - val_loss: 0.6435 - val_accuracy: 0.6377 - val_auc_2: 0.7043\n",
      "Epoch 14/50\n",
      "9950/9950 [==============================] - 8s 807us/step - loss: 0.6010 - accuracy: 0.6857 - auc_2: 0.7063 - val_loss: 0.6533 - val_accuracy: 0.6221 - val_auc_2: 0.7065\n",
      "Epoch 15/50\n",
      "9950/9950 [==============================] - 8s 808us/step - loss: 0.5979 - accuracy: 0.6880 - auc_2: 0.7083 - val_loss: 0.6524 - val_accuracy: 0.6317 - val_auc_2: 0.7084\n",
      "Epoch 16/50\n",
      "9950/9950 [==============================] - 8s 807us/step - loss: 0.5953 - accuracy: 0.6898 - auc_2: 0.7101 - val_loss: 0.6507 - val_accuracy: 0.6171 - val_auc_2: 0.7109\n",
      "Epoch 17/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5927 - accuracy: 0.6915 - auc_2: 0.7120 - val_loss: 0.6639 - val_accuracy: 0.6216 - val_auc_2: 0.7119\n",
      "Epoch 18/50\n",
      "9950/9950 [==============================] - 8s 808us/step - loss: 0.5891 - accuracy: 0.6978 - auc_2: 0.7135 - val_loss: 0.6621 - val_accuracy: 0.6186 - val_auc_2: 0.7138\n",
      "Epoch 19/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.5871 - accuracy: 0.6969 - auc_2: 0.7152 - val_loss: 0.6717 - val_accuracy: 0.6126 - val_auc_2: 0.7151\n",
      "Epoch 20/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.5840 - accuracy: 0.6973 - auc_2: 0.7167 - val_loss: 0.6737 - val_accuracy: 0.6065 - val_auc_2: 0.7165\n",
      "Epoch 21/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.5826 - accuracy: 0.6999 - auc_2: 0.7180 - val_loss: 0.6565 - val_accuracy: 0.6231 - val_auc_2: 0.7184\n",
      "Epoch 22/50\n",
      "9950/9950 [==============================] - 8s 814us/step - loss: 0.5794 - accuracy: 0.7020 - auc_2: 0.7195 - val_loss: 0.6613 - val_accuracy: 0.6226 - val_auc_2: 0.7198\n",
      "Epoch 23/50\n",
      "9950/9950 [==============================] - 8s 813us/step - loss: 0.5776 - accuracy: 0.7033 - auc_2: 0.7211 - val_loss: 0.6793 - val_accuracy: 0.5990 - val_auc_2: 0.7209\n",
      "Epoch 24/50\n",
      "9950/9950 [==============================] - 8s 809us/step - loss: 0.5748 - accuracy: 0.7088 - auc_2: 0.7223 - val_loss: 0.6729 - val_accuracy: 0.6035 - val_auc_2: 0.7223\n",
      "Epoch 25/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.5725 - accuracy: 0.7128 - auc_2: 0.7235 - val_loss: 0.6787 - val_accuracy: 0.6035 - val_auc_2: 0.7236\n",
      "Epoch 26/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.5698 - accuracy: 0.7128 - auc_2: 0.7249 - val_loss: 0.6623 - val_accuracy: 0.6171 - val_auc_2: 0.7253\n",
      "Epoch 27/50\n",
      "9950/9950 [==============================] - 8s 809us/step - loss: 0.5683 - accuracy: 0.7146 - auc_2: 0.7262 - val_loss: 0.6677 - val_accuracy: 0.6080 - val_auc_2: 0.7265\n",
      "Epoch 28/50\n",
      "9950/9950 [==============================] - 8s 809us/step - loss: 0.5663 - accuracy: 0.7144 - auc_2: 0.7276 - val_loss: 0.6652 - val_accuracy: 0.6176 - val_auc_2: 0.7278\n",
      "Epoch 29/50\n",
      "9950/9950 [==============================] - 8s 809us/step - loss: 0.5642 - accuracy: 0.7147 - auc_2: 0.7288 - val_loss: 0.6731 - val_accuracy: 0.6106 - val_auc_2: 0.7289\n",
      "Epoch 30/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5628 - accuracy: 0.7165 - auc_2: 0.7300 - val_loss: 0.6544 - val_accuracy: 0.6221 - val_auc_2: 0.7304\n",
      "Epoch 31/50\n",
      "9950/9950 [==============================] - 8s 807us/step - loss: 0.5609 - accuracy: 0.7212 - auc_2: 0.7313 - val_loss: 0.6689 - val_accuracy: 0.6126 - val_auc_2: 0.7314\n",
      "Epoch 32/50\n",
      "9950/9950 [==============================] - 8s 815us/step - loss: 0.5596 - accuracy: 0.7148 - auc_2: 0.7325 - val_loss: 0.6633 - val_accuracy: 0.6186 - val_auc_2: 0.7327\n",
      "Epoch 33/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5576 - accuracy: 0.7196 - auc_2: 0.7336 - val_loss: 0.6582 - val_accuracy: 0.6196 - val_auc_2: 0.7339\n",
      "Epoch 34/50\n",
      "9950/9950 [==============================] - 8s 809us/step - loss: 0.5557 - accuracy: 0.7182 - auc_2: 0.7348 - val_loss: 0.6739 - val_accuracy: 0.6080 - val_auc_2: 0.7349\n",
      "Epoch 35/50\n",
      "9950/9950 [==============================] - 8s 809us/step - loss: 0.5538 - accuracy: 0.7193 - auc_2: 0.7358 - val_loss: 0.6753 - val_accuracy: 0.6020 - val_auc_2: 0.7359\n",
      "Epoch 36/50\n",
      "9950/9950 [==============================] - 8s 809us/step - loss: 0.5526 - accuracy: 0.7211 - auc_2: 0.7368 - val_loss: 0.6710 - val_accuracy: 0.6085 - val_auc_2: 0.7369\n",
      "Epoch 37/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.5505 - accuracy: 0.7254 - auc_2: 0.7378 - val_loss: 0.6587 - val_accuracy: 0.6181 - val_auc_2: 0.7381\n",
      "Epoch 38/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5491 - accuracy: 0.7246 - auc_2: 0.7389 - val_loss: 0.6726 - val_accuracy: 0.6111 - val_auc_2: 0.7390\n",
      "Epoch 39/50\n",
      "9950/9950 [==============================] - 8s 808us/step - loss: 0.5483 - accuracy: 0.7241 - auc_2: 0.7399 - val_loss: 0.6610 - val_accuracy: 0.6151 - val_auc_2: 0.7402\n",
      "Epoch 40/50\n",
      "9950/9950 [==============================] - 8s 808us/step - loss: 0.5467 - accuracy: 0.7264 - auc_2: 0.7409 - val_loss: 0.6919 - val_accuracy: 0.5960 - val_auc_2: 0.7409\n",
      "Epoch 41/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.5450 - accuracy: 0.7276 - auc_2: 0.7417 - val_loss: 0.6664 - val_accuracy: 0.6171 - val_auc_2: 0.7419\n",
      "Epoch 42/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.5434 - accuracy: 0.7289 - auc_2: 0.7427 - val_loss: 0.6558 - val_accuracy: 0.6216 - val_auc_2: 0.7429\n",
      "Epoch 43/50\n",
      "9950/9950 [==============================] - 8s 809us/step - loss: 0.5430 - accuracy: 0.7305 - auc_2: 0.7436 - val_loss: 0.6695 - val_accuracy: 0.6146 - val_auc_2: 0.7438\n",
      "Epoch 44/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.5412 - accuracy: 0.7286 - auc_2: 0.7445 - val_loss: 0.6639 - val_accuracy: 0.6121 - val_auc_2: 0.7448\n",
      "Epoch 45/50\n",
      "9950/9950 [==============================] - 8s 809us/step - loss: 0.5404 - accuracy: 0.7273 - auc_2: 0.7455 - val_loss: 0.6531 - val_accuracy: 0.6191 - val_auc_2: 0.7457\n",
      "Epoch 46/50\n",
      "9950/9950 [==============================] - 8s 809us/step - loss: 0.5392 - accuracy: 0.7307 - auc_2: 0.7463 - val_loss: 0.6643 - val_accuracy: 0.6151 - val_auc_2: 0.7465\n",
      "Epoch 47/50\n",
      "9950/9950 [==============================] - 8s 808us/step - loss: 0.5378 - accuracy: 0.7296 - auc_2: 0.7472 - val_loss: 0.6815 - val_accuracy: 0.5985 - val_auc_2: 0.7473\n",
      "Epoch 48/50\n",
      "9950/9950 [==============================] - 8s 808us/step - loss: 0.5370 - accuracy: 0.7342 - auc_2: 0.7480 - val_loss: 0.6646 - val_accuracy: 0.6106 - val_auc_2: 0.7481\n",
      "Epoch 49/50\n",
      "9950/9950 [==============================] - 8s 808us/step - loss: 0.5350 - accuracy: 0.7317 - auc_2: 0.7487 - val_loss: 0.6654 - val_accuracy: 0.6095 - val_auc_2: 0.7489\n",
      "Epoch 50/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.5339 - accuracy: 0.7335 - auc_2: 0.7495 - val_loss: 0.6841 - val_accuracy: 0.5975 - val_auc_2: 0.7496\n",
      "1990/1990 [==============================] - 1s 290us/step\n",
      "TRAIN: [ 0  1  4  5  6  7  8  9 10 11] TEST: [2 3]\n",
      "Train on 9950 samples, validate on 1990 samples\n",
      "Epoch 1/50\n",
      "9950/9950 [==============================] - 8s 840us/step - loss: 0.6818 - accuracy: 0.5689 - auc_3: 0.5705 - val_loss: 0.6837 - val_accuracy: 0.5372 - val_auc_3: 0.5925\n",
      "Epoch 2/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.6705 - accuracy: 0.6000 - auc_3: 0.6061 - val_loss: 0.6744 - val_accuracy: 0.5528 - val_auc_3: 0.6122\n",
      "Epoch 3/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.6621 - accuracy: 0.6136 - auc_3: 0.6224 - val_loss: 0.6648 - val_accuracy: 0.5704 - val_auc_3: 0.6253\n",
      "Epoch 4/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.6542 - accuracy: 0.6322 - auc_3: 0.6349 - val_loss: 0.6554 - val_accuracy: 0.6045 - val_auc_3: 0.6400\n",
      "Epoch 5/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.6473 - accuracy: 0.6382 - auc_3: 0.6458 - val_loss: 0.6459 - val_accuracy: 0.6251 - val_auc_3: 0.6497\n",
      "Epoch 6/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.6418 - accuracy: 0.6449 - auc_3: 0.6551 - val_loss: 0.6401 - val_accuracy: 0.6216 - val_auc_3: 0.6572\n",
      "Epoch 7/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.6370 - accuracy: 0.6513 - auc_3: 0.6619 - val_loss: 0.6351 - val_accuracy: 0.6432 - val_auc_3: 0.6648\n",
      "Epoch 8/50\n",
      "9950/9950 [==============================] - 8s 813us/step - loss: 0.6330 - accuracy: 0.6499 - auc_3: 0.6681 - val_loss: 0.6308 - val_accuracy: 0.6477 - val_auc_3: 0.6703\n",
      "Epoch 9/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.6291 - accuracy: 0.6542 - auc_3: 0.6729 - val_loss: 0.6350 - val_accuracy: 0.6256 - val_auc_3: 0.6732\n",
      "Epoch 10/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.6262 - accuracy: 0.6550 - auc_3: 0.6766 - val_loss: 0.6251 - val_accuracy: 0.6553 - val_auc_3: 0.6791\n",
      "Epoch 11/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.6233 - accuracy: 0.6590 - auc_3: 0.6805 - val_loss: 0.6197 - val_accuracy: 0.6462 - val_auc_3: 0.6817\n",
      "Epoch 12/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.6204 - accuracy: 0.6598 - auc_3: 0.6841 - val_loss: 0.6154 - val_accuracy: 0.6573 - val_auc_3: 0.6851\n",
      "Epoch 13/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.6179 - accuracy: 0.6649 - auc_3: 0.6873 - val_loss: 0.6121 - val_accuracy: 0.6653 - val_auc_3: 0.6882\n",
      "Epoch 14/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.6149 - accuracy: 0.6633 - auc_3: 0.6901 - val_loss: 0.6120 - val_accuracy: 0.6568 - val_auc_3: 0.6909\n",
      "Epoch 15/50\n",
      "9950/9950 [==============================] - 8s 833us/step - loss: 0.6122 - accuracy: 0.6692 - auc_3: 0.6929 - val_loss: 0.6073 - val_accuracy: 0.6734 - val_auc_3: 0.6943\n",
      "Epoch 16/50\n",
      "9950/9950 [==============================] - 8s 809us/step - loss: 0.6106 - accuracy: 0.6685 - auc_3: 0.6956 - val_loss: 0.6107 - val_accuracy: 0.6467 - val_auc_3: 0.6961\n",
      "Epoch 17/50\n",
      "9950/9950 [==============================] - 8s 830us/step - loss: 0.6079 - accuracy: 0.6726 - auc_3: 0.6978 - val_loss: 0.6002 - val_accuracy: 0.6814 - val_auc_3: 0.6988\n",
      "Epoch 18/50\n",
      "9950/9950 [==============================] - 9s 879us/step - loss: 0.6059 - accuracy: 0.6731 - auc_3: 0.7003 - val_loss: 0.6003 - val_accuracy: 0.6693 - val_auc_3: 0.7010\n",
      "Epoch 19/50\n",
      "9950/9950 [==============================] - 8s 851us/step - loss: 0.6039 - accuracy: 0.6716 - auc_3: 0.7025 - val_loss: 0.5957 - val_accuracy: 0.6869 - val_auc_3: 0.7036\n",
      "Epoch 20/50\n",
      "9950/9950 [==============================] - 8s 814us/step - loss: 0.6011 - accuracy: 0.6818 - auc_3: 0.7048 - val_loss: 0.5931 - val_accuracy: 0.6824 - val_auc_3: 0.7056\n",
      "Epoch 21/50\n",
      "9950/9950 [==============================] - 8s 851us/step - loss: 0.5991 - accuracy: 0.6834 - auc_3: 0.7069 - val_loss: 0.5928 - val_accuracy: 0.6915 - val_auc_3: 0.7079\n",
      "Epoch 22/50\n",
      "9950/9950 [==============================] - 9s 913us/step - loss: 0.5970 - accuracy: 0.6846 - auc_3: 0.7090 - val_loss: 0.5895 - val_accuracy: 0.6879 - val_auc_3: 0.7098\n",
      "Epoch 23/50\n",
      "9950/9950 [==============================] - 9s 943us/step - loss: 0.5949 - accuracy: 0.6848 - auc_3: 0.7110 - val_loss: 0.5873 - val_accuracy: 0.6950 - val_auc_3: 0.7119\n",
      "Epoch 24/50\n",
      "9950/9950 [==============================] - 9s 874us/step - loss: 0.5927 - accuracy: 0.6854 - auc_3: 0.7129 - val_loss: 0.5926 - val_accuracy: 0.6744 - val_auc_3: 0.7134\n",
      "Epoch 25/50\n",
      "9950/9950 [==============================] - 9s 923us/step - loss: 0.5903 - accuracy: 0.6888 - auc_3: 0.7146 - val_loss: 0.5867 - val_accuracy: 0.6874 - val_auc_3: 0.7152\n",
      "Epoch 26/50\n",
      "9950/9950 [==============================] - 8s 850us/step - loss: 0.5882 - accuracy: 0.6908 - auc_3: 0.7165 - val_loss: 0.5848 - val_accuracy: 0.6950 - val_auc_3: 0.7173\n",
      "Epoch 27/50\n",
      "9950/9950 [==============================] - 9s 869us/step - loss: 0.5862 - accuracy: 0.6911 - auc_3: 0.7182 - val_loss: 0.5835 - val_accuracy: 0.6950 - val_auc_3: 0.7191\n",
      "Epoch 28/50\n",
      "9950/9950 [==============================] - 8s 809us/step - loss: 0.5846 - accuracy: 0.6946 - auc_3: 0.7200 - val_loss: 0.5815 - val_accuracy: 0.6980 - val_auc_3: 0.7207\n",
      "Epoch 29/50\n",
      "9950/9950 [==============================] - 8s 808us/step - loss: 0.5821 - accuracy: 0.6954 - auc_3: 0.7216 - val_loss: 0.5814 - val_accuracy: 0.6889 - val_auc_3: 0.7222\n",
      "Epoch 30/50\n",
      "9950/9950 [==============================] - 8s 809us/step - loss: 0.5803 - accuracy: 0.6982 - auc_3: 0.7232 - val_loss: 0.5796 - val_accuracy: 0.6899 - val_auc_3: 0.7239\n",
      "Epoch 31/50\n",
      "9950/9950 [==============================] - 8s 808us/step - loss: 0.5783 - accuracy: 0.6955 - auc_3: 0.7248 - val_loss: 0.5785 - val_accuracy: 0.7035 - val_auc_3: 0.7256\n",
      "Epoch 32/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.5758 - accuracy: 0.6980 - auc_3: 0.7264 - val_loss: 0.5848 - val_accuracy: 0.6784 - val_auc_3: 0.7267\n",
      "Epoch 33/50\n",
      "9950/9950 [==============================] - 8s 808us/step - loss: 0.5745 - accuracy: 0.7032 - auc_3: 0.7278 - val_loss: 0.5783 - val_accuracy: 0.7040 - val_auc_3: 0.7285\n",
      "Epoch 34/50\n",
      "9950/9950 [==============================] - 8s 809us/step - loss: 0.5733 - accuracy: 0.7025 - auc_3: 0.7293 - val_loss: 0.5761 - val_accuracy: 0.7065 - val_auc_3: 0.7300\n",
      "Epoch 35/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.5706 - accuracy: 0.7033 - auc_3: 0.7307 - val_loss: 0.5724 - val_accuracy: 0.7020 - val_auc_3: 0.7313\n",
      "Epoch 36/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.5690 - accuracy: 0.7058 - auc_3: 0.7321 - val_loss: 0.5732 - val_accuracy: 0.7020 - val_auc_3: 0.7327\n",
      "Epoch 37/50\n",
      "9950/9950 [==============================] - 8s 807us/step - loss: 0.5674 - accuracy: 0.7057 - auc_3: 0.7335 - val_loss: 0.5729 - val_accuracy: 0.6980 - val_auc_3: 0.7340\n",
      "Epoch 38/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.5656 - accuracy: 0.7097 - auc_3: 0.7348 - val_loss: 0.5690 - val_accuracy: 0.7111 - val_auc_3: 0.7354\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9950/9950 [==============================] - 8s 809us/step - loss: 0.5637 - accuracy: 0.7091 - auc_3: 0.7362 - val_loss: 0.5698 - val_accuracy: 0.7030 - val_auc_3: 0.7368\n",
      "Epoch 40/50\n",
      "9950/9950 [==============================] - 8s 808us/step - loss: 0.5626 - accuracy: 0.7124 - auc_3: 0.7375 - val_loss: 0.5709 - val_accuracy: 0.7106 - val_auc_3: 0.7381\n",
      "Epoch 41/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.5610 - accuracy: 0.7093 - auc_3: 0.7387 - val_loss: 0.5706 - val_accuracy: 0.6995 - val_auc_3: 0.7392\n",
      "Epoch 42/50\n",
      "9950/9950 [==============================] - 8s 809us/step - loss: 0.5586 - accuracy: 0.7149 - auc_3: 0.7400 - val_loss: 0.5712 - val_accuracy: 0.7136 - val_auc_3: 0.7406\n",
      "Epoch 43/50\n",
      "9950/9950 [==============================] - 8s 809us/step - loss: 0.5571 - accuracy: 0.7163 - auc_3: 0.7411 - val_loss: 0.5759 - val_accuracy: 0.6864 - val_auc_3: 0.7415\n",
      "Epoch 44/50\n",
      "9950/9950 [==============================] - 8s 809us/step - loss: 0.5558 - accuracy: 0.7173 - auc_3: 0.7422 - val_loss: 0.5768 - val_accuracy: 0.6859 - val_auc_3: 0.7426\n",
      "Epoch 45/50\n",
      "9950/9950 [==============================] - 8s 809us/step - loss: 0.5541 - accuracy: 0.7175 - auc_3: 0.7434 - val_loss: 0.5687 - val_accuracy: 0.7015 - val_auc_3: 0.7438\n",
      "Epoch 46/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.5531 - accuracy: 0.7181 - auc_3: 0.7445 - val_loss: 0.5675 - val_accuracy: 0.7020 - val_auc_3: 0.7449\n",
      "Epoch 47/50\n",
      "9950/9950 [==============================] - 8s 809us/step - loss: 0.5510 - accuracy: 0.7180 - auc_3: 0.7456 - val_loss: 0.5642 - val_accuracy: 0.7176 - val_auc_3: 0.7462\n",
      "Epoch 48/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.5500 - accuracy: 0.7173 - auc_3: 0.7467 - val_loss: 0.5655 - val_accuracy: 0.7060 - val_auc_3: 0.7472\n",
      "Epoch 49/50\n",
      "9950/9950 [==============================] - 8s 808us/step - loss: 0.5482 - accuracy: 0.7223 - auc_3: 0.7478 - val_loss: 0.5640 - val_accuracy: 0.7156 - val_auc_3: 0.7483\n",
      "Epoch 50/50\n",
      "9950/9950 [==============================] - 8s 809us/step - loss: 0.5464 - accuracy: 0.7213 - auc_3: 0.7489 - val_loss: 0.5637 - val_accuracy: 0.7156 - val_auc_3: 0.7494\n",
      "1990/1990 [==============================] - 1s 295us/step\n",
      "TRAIN: [ 0  1  2  3  6  7  8  9 10 11] TEST: [4 5]\n",
      "Train on 9950 samples, validate on 1990 samples\n",
      "Epoch 1/50\n",
      "9950/9950 [==============================] - 8s 843us/step - loss: 0.6765 - accuracy: 0.5841 - auc_4: 0.5644 - val_loss: 0.7064 - val_accuracy: 0.4894 - val_auc_4: 0.5803\n",
      "Epoch 2/50\n",
      "9950/9950 [==============================] - 8s 816us/step - loss: 0.6511 - accuracy: 0.6306 - auc_4: 0.6151 - val_loss: 0.7045 - val_accuracy: 0.4990 - val_auc_4: 0.6211\n",
      "Epoch 3/50\n",
      "9950/9950 [==============================] - 8s 815us/step - loss: 0.6353 - accuracy: 0.6517 - auc_4: 0.6391 - val_loss: 0.7177 - val_accuracy: 0.5045 - val_auc_4: 0.6398\n",
      "Epoch 4/50\n",
      "9950/9950 [==============================] - 9s 887us/step - loss: 0.6230 - accuracy: 0.6681 - auc_4: 0.6535 - val_loss: 0.7261 - val_accuracy: 0.5070 - val_auc_4: 0.6532\n",
      "Epoch 5/50\n",
      "9950/9950 [==============================] - 9s 878us/step - loss: 0.6144 - accuracy: 0.6733 - auc_4: 0.6630 - val_loss: 0.7158 - val_accuracy: 0.5186 - val_auc_4: 0.6642\n",
      "Epoch 6/50\n",
      "9950/9950 [==============================] - 9s 923us/step - loss: 0.6074 - accuracy: 0.6830 - auc_4: 0.6716 - val_loss: 0.7182 - val_accuracy: 0.5226 - val_auc_4: 0.6725\n",
      "Epoch 7/50\n",
      "9950/9950 [==============================] - 10s 961us/step - loss: 0.6021 - accuracy: 0.6836 - auc_4: 0.6785 - val_loss: 0.7412 - val_accuracy: 0.5101 - val_auc_4: 0.6777\n",
      "Epoch 8/50\n",
      "9950/9950 [==============================] - 10s 986us/step - loss: 0.5978 - accuracy: 0.6884 - auc_4: 0.6831 - val_loss: 0.7262 - val_accuracy: 0.5256 - val_auc_4: 0.6832\n",
      "Epoch 9/50\n",
      "9950/9950 [==============================] - 10s 969us/step - loss: 0.5928 - accuracy: 0.6910 - auc_4: 0.6878 - val_loss: 0.7235 - val_accuracy: 0.5231 - val_auc_4: 0.6881\n",
      "Epoch 10/50\n",
      "9950/9950 [==============================] - 10s 969us/step - loss: 0.5897 - accuracy: 0.6951 - auc_4: 0.6917 - val_loss: 0.7281 - val_accuracy: 0.5256 - val_auc_4: 0.6921\n",
      "Epoch 11/50\n",
      "9950/9950 [==============================] - 8s 834us/step - loss: 0.5861 - accuracy: 0.6965 - auc_4: 0.6954 - val_loss: 0.7462 - val_accuracy: 0.5206 - val_auc_4: 0.6950\n",
      "Epoch 12/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5824 - accuracy: 0.7021 - auc_4: 0.6982 - val_loss: 0.7161 - val_accuracy: 0.5352 - val_auc_4: 0.6990\n",
      "Epoch 13/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5794 - accuracy: 0.7011 - auc_4: 0.7017 - val_loss: 0.7330 - val_accuracy: 0.5261 - val_auc_4: 0.7017\n",
      "Epoch 14/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5760 - accuracy: 0.7048 - auc_4: 0.7043 - val_loss: 0.7664 - val_accuracy: 0.5186 - val_auc_4: 0.7039\n",
      "Epoch 15/50\n",
      "9950/9950 [==============================] - 8s 813us/step - loss: 0.5731 - accuracy: 0.7093 - auc_4: 0.7065 - val_loss: 0.7747 - val_accuracy: 0.5176 - val_auc_4: 0.7061\n",
      "Epoch 16/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5704 - accuracy: 0.7076 - auc_4: 0.7085 - val_loss: 0.7210 - val_accuracy: 0.5407 - val_auc_4: 0.7091\n",
      "Epoch 17/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5672 - accuracy: 0.7130 - auc_4: 0.7111 - val_loss: 0.7504 - val_accuracy: 0.5246 - val_auc_4: 0.7110\n",
      "Epoch 18/50\n",
      "9950/9950 [==============================] - 8s 814us/step - loss: 0.5646 - accuracy: 0.7153 - auc_4: 0.7131 - val_loss: 0.7519 - val_accuracy: 0.5256 - val_auc_4: 0.7131\n",
      "Epoch 19/50\n",
      "9950/9950 [==============================] - 8s 815us/step - loss: 0.5629 - accuracy: 0.7141 - auc_4: 0.7151 - val_loss: 0.7533 - val_accuracy: 0.5276 - val_auc_4: 0.7150\n",
      "Epoch 20/50\n",
      "9950/9950 [==============================] - 8s 814us/step - loss: 0.5602 - accuracy: 0.7158 - auc_4: 0.7169 - val_loss: 0.7221 - val_accuracy: 0.5387 - val_auc_4: 0.7172\n",
      "Epoch 21/50\n",
      "9950/9950 [==============================] - 8s 814us/step - loss: 0.5580 - accuracy: 0.7204 - auc_4: 0.7188 - val_loss: 0.7557 - val_accuracy: 0.5296 - val_auc_4: 0.7188\n",
      "Epoch 22/50\n",
      "9950/9950 [==============================] - 8s 815us/step - loss: 0.5558 - accuracy: 0.7203 - auc_4: 0.7206 - val_loss: 0.7177 - val_accuracy: 0.5482 - val_auc_4: 0.7210\n",
      "Epoch 23/50\n",
      "9950/9950 [==============================] - 8s 813us/step - loss: 0.5531 - accuracy: 0.7228 - auc_4: 0.7224 - val_loss: 0.7175 - val_accuracy: 0.5528 - val_auc_4: 0.7230\n",
      "Epoch 24/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5515 - accuracy: 0.7234 - auc_4: 0.7243 - val_loss: 0.7296 - val_accuracy: 0.5397 - val_auc_4: 0.7246\n",
      "Epoch 25/50\n",
      "9950/9950 [==============================] - 8s 816us/step - loss: 0.5482 - accuracy: 0.7250 - auc_4: 0.7259 - val_loss: 0.7905 - val_accuracy: 0.5256 - val_auc_4: 0.7257\n",
      "Epoch 26/50\n",
      "9950/9950 [==============================] - 8s 815us/step - loss: 0.5471 - accuracy: 0.7264 - auc_4: 0.7272 - val_loss: 0.7237 - val_accuracy: 0.5508 - val_auc_4: 0.7276\n",
      "Epoch 27/50\n",
      "9950/9950 [==============================] - 9s 879us/step - loss: 0.5450 - accuracy: 0.7280 - auc_4: 0.7287 - val_loss: 0.7253 - val_accuracy: 0.5462 - val_auc_4: 0.7292\n",
      "Epoch 28/50\n",
      "9950/9950 [==============================] - 8s 835us/step - loss: 0.5438 - accuracy: 0.7288 - auc_4: 0.7304 - val_loss: 0.7260 - val_accuracy: 0.5462 - val_auc_4: 0.7306\n",
      "Epoch 29/50\n",
      "9950/9950 [==============================] - 8s 813us/step - loss: 0.5411 - accuracy: 0.7306 - auc_4: 0.7318 - val_loss: 0.7574 - val_accuracy: 0.5382 - val_auc_4: 0.7319\n",
      "Epoch 30/50\n",
      "9950/9950 [==============================] - 9s 888us/step - loss: 0.5392 - accuracy: 0.7326 - auc_4: 0.7330 - val_loss: 0.7970 - val_accuracy: 0.5317 - val_auc_4: 0.7329\n",
      "Epoch 31/50\n",
      "9950/9950 [==============================] - 9s 858us/step - loss: 0.5376 - accuracy: 0.7315 - auc_4: 0.7341 - val_loss: 0.7242 - val_accuracy: 0.5518 - val_auc_4: 0.7345\n",
      "Epoch 32/50\n",
      "9950/9950 [==============================] - 8s 809us/step - loss: 0.5352 - accuracy: 0.7331 - auc_4: 0.7355 - val_loss: 0.7164 - val_accuracy: 0.5487 - val_auc_4: 0.7359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50\n",
      "9950/9950 [==============================] - 8s 809us/step - loss: 0.5329 - accuracy: 0.7377 - auc_4: 0.7369 - val_loss: 0.7107 - val_accuracy: 0.5628 - val_auc_4: 0.7374\n",
      "Epoch 34/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5320 - accuracy: 0.7368 - auc_4: 0.7384 - val_loss: 0.7225 - val_accuracy: 0.5518 - val_auc_4: 0.7386\n",
      "Epoch 35/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5302 - accuracy: 0.7356 - auc_4: 0.7395 - val_loss: 0.7523 - val_accuracy: 0.5432 - val_auc_4: 0.7397\n",
      "Epoch 36/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5285 - accuracy: 0.7396 - auc_4: 0.7407 - val_loss: 0.7419 - val_accuracy: 0.5482 - val_auc_4: 0.7409\n",
      "Epoch 37/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5264 - accuracy: 0.7384 - auc_4: 0.7418 - val_loss: 0.7367 - val_accuracy: 0.5482 - val_auc_4: 0.7421\n",
      "Epoch 38/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5252 - accuracy: 0.7414 - auc_4: 0.7430 - val_loss: 0.7359 - val_accuracy: 0.5482 - val_auc_4: 0.7432\n",
      "Epoch 39/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.5226 - accuracy: 0.7412 - auc_4: 0.7440 - val_loss: 0.7236 - val_accuracy: 0.5568 - val_auc_4: 0.7444\n",
      "Epoch 40/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5221 - accuracy: 0.7414 - auc_4: 0.7452 - val_loss: 0.7818 - val_accuracy: 0.5387 - val_auc_4: 0.7452\n",
      "Epoch 41/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.5207 - accuracy: 0.7435 - auc_4: 0.7461 - val_loss: 0.7638 - val_accuracy: 0.5432 - val_auc_4: 0.7462\n",
      "Epoch 42/50\n",
      "9950/9950 [==============================] - 9s 861us/step - loss: 0.5195 - accuracy: 0.7439 - auc_4: 0.7470 - val_loss: 0.7613 - val_accuracy: 0.5447 - val_auc_4: 0.7471\n",
      "Epoch 43/50\n",
      "9950/9950 [==============================] - 8s 830us/step - loss: 0.5179 - accuracy: 0.7416 - auc_4: 0.7479 - val_loss: 0.7844 - val_accuracy: 0.5442 - val_auc_4: 0.7479\n",
      "Epoch 44/50\n",
      "9950/9950 [==============================] - 8s 809us/step - loss: 0.5168 - accuracy: 0.7478 - auc_4: 0.7488 - val_loss: 0.7638 - val_accuracy: 0.5462 - val_auc_4: 0.7488\n",
      "Epoch 45/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.5152 - accuracy: 0.7470 - auc_4: 0.7496 - val_loss: 0.7380 - val_accuracy: 0.5583 - val_auc_4: 0.7498\n",
      "Epoch 46/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.5133 - accuracy: 0.7464 - auc_4: 0.7505 - val_loss: 0.7559 - val_accuracy: 0.5503 - val_auc_4: 0.7507\n",
      "Epoch 47/50\n",
      "9950/9950 [==============================] - 8s 809us/step - loss: 0.5119 - accuracy: 0.7447 - auc_4: 0.7514 - val_loss: 0.7384 - val_accuracy: 0.5558 - val_auc_4: 0.7517\n",
      "Epoch 48/50\n",
      "9950/9950 [==============================] - 8s 808us/step - loss: 0.5106 - accuracy: 0.7478 - auc_4: 0.7523 - val_loss: 0.7236 - val_accuracy: 0.5593 - val_auc_4: 0.7527\n",
      "Epoch 49/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.5095 - accuracy: 0.7483 - auc_4: 0.7533 - val_loss: 0.7538 - val_accuracy: 0.5533 - val_auc_4: 0.7534\n",
      "Epoch 50/50\n",
      "9950/9950 [==============================] - 8s 809us/step - loss: 0.5086 - accuracy: 0.7478 - auc_4: 0.7541 - val_loss: 0.7654 - val_accuracy: 0.5487 - val_auc_4: 0.7542\n",
      "1990/1990 [==============================] - 1s 293us/step\n",
      "TRAIN: [ 0  1  2  3  4  5  8  9 10 11] TEST: [6 7]\n",
      "Train on 9950 samples, validate on 1990 samples\n",
      "Epoch 1/50\n",
      "9950/9950 [==============================] - 8s 847us/step - loss: 0.6808 - accuracy: 0.5609 - auc_5: 0.5600 - val_loss: 0.6855 - val_accuracy: 0.5337 - val_auc_5: 0.5768\n",
      "Epoch 2/50\n",
      "9950/9950 [==============================] - 8s 813us/step - loss: 0.6632 - accuracy: 0.6113 - auc_5: 0.6060 - val_loss: 0.6813 - val_accuracy: 0.5523 - val_auc_5: 0.6131\n",
      "Epoch 3/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.6504 - accuracy: 0.6322 - auc_5: 0.6297 - val_loss: 0.6761 - val_accuracy: 0.5960 - val_auc_5: 0.6377\n",
      "Epoch 4/50\n",
      "9950/9950 [==============================] - 8s 813us/step - loss: 0.6404 - accuracy: 0.6432 - auc_5: 0.6458 - val_loss: 0.6740 - val_accuracy: 0.6005 - val_auc_5: 0.6506\n",
      "Epoch 5/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.6335 - accuracy: 0.6545 - auc_5: 0.6564 - val_loss: 0.6754 - val_accuracy: 0.5829 - val_auc_5: 0.6585\n",
      "Epoch 6/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.6285 - accuracy: 0.6616 - auc_5: 0.6637 - val_loss: 0.6714 - val_accuracy: 0.6106 - val_auc_5: 0.6671\n",
      "Epoch 7/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.6242 - accuracy: 0.6606 - auc_5: 0.6703 - val_loss: 0.6718 - val_accuracy: 0.5960 - val_auc_5: 0.6716\n",
      "Epoch 8/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.6198 - accuracy: 0.6636 - auc_5: 0.6751 - val_loss: 0.6687 - val_accuracy: 0.6111 - val_auc_5: 0.6772\n",
      "Epoch 9/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.6162 - accuracy: 0.6665 - auc_5: 0.6794 - val_loss: 0.6678 - val_accuracy: 0.6181 - val_auc_5: 0.6816\n",
      "Epoch 10/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.6133 - accuracy: 0.6675 - auc_5: 0.6837 - val_loss: 0.6673 - val_accuracy: 0.6171 - val_auc_5: 0.6852\n",
      "Epoch 11/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.6108 - accuracy: 0.6702 - auc_5: 0.6872 - val_loss: 0.6707 - val_accuracy: 0.6261 - val_auc_5: 0.6891\n",
      "Epoch 12/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.6073 - accuracy: 0.6768 - auc_5: 0.6902 - val_loss: 0.6658 - val_accuracy: 0.6261 - val_auc_5: 0.6917\n",
      "Epoch 13/50\n",
      "9950/9950 [==============================] - 9s 855us/step - loss: 0.6045 - accuracy: 0.6759 - auc_5: 0.6933 - val_loss: 0.6650 - val_accuracy: 0.6196 - val_auc_5: 0.6943\n",
      "Epoch 14/50\n",
      "9950/9950 [==============================] - 8s 842us/step - loss: 0.6029 - accuracy: 0.6780 - auc_5: 0.6959 - val_loss: 0.6642 - val_accuracy: 0.6352 - val_auc_5: 0.6971\n",
      "Epoch 15/50\n",
      "9950/9950 [==============================] - 8s 814us/step - loss: 0.6004 - accuracy: 0.6809 - auc_5: 0.6982 - val_loss: 0.6635 - val_accuracy: 0.6296 - val_auc_5: 0.6994\n",
      "Epoch 16/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5989 - accuracy: 0.6865 - auc_5: 0.7004 - val_loss: 0.6629 - val_accuracy: 0.6357 - val_auc_5: 0.7017\n",
      "Epoch 17/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.5967 - accuracy: 0.6861 - auc_5: 0.7028 - val_loss: 0.6613 - val_accuracy: 0.6342 - val_auc_5: 0.7037\n",
      "Epoch 18/50\n",
      "9950/9950 [==============================] - 8s 815us/step - loss: 0.5944 - accuracy: 0.6876 - auc_5: 0.7049 - val_loss: 0.6678 - val_accuracy: 0.6141 - val_auc_5: 0.7052\n",
      "Epoch 19/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5928 - accuracy: 0.6883 - auc_5: 0.7065 - val_loss: 0.6610 - val_accuracy: 0.6397 - val_auc_5: 0.7074\n",
      "Epoch 20/50\n",
      "9950/9950 [==============================] - 8s 813us/step - loss: 0.5903 - accuracy: 0.6909 - auc_5: 0.7085 - val_loss: 0.6603 - val_accuracy: 0.6261 - val_auc_5: 0.7091\n",
      "Epoch 21/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5884 - accuracy: 0.6928 - auc_5: 0.7102 - val_loss: 0.6604 - val_accuracy: 0.6492 - val_auc_5: 0.7111\n",
      "Epoch 22/50\n",
      "9950/9950 [==============================] - 9s 872us/step - loss: 0.5864 - accuracy: 0.6955 - auc_5: 0.7120 - val_loss: 0.6674 - val_accuracy: 0.6161 - val_auc_5: 0.7122\n",
      "Epoch 23/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5849 - accuracy: 0.6970 - auc_5: 0.7133 - val_loss: 0.6589 - val_accuracy: 0.6442 - val_auc_5: 0.7141\n",
      "Epoch 24/50\n",
      "9950/9950 [==============================] - 8s 813us/step - loss: 0.5829 - accuracy: 0.6994 - auc_5: 0.7151 - val_loss: 0.6595 - val_accuracy: 0.6492 - val_auc_5: 0.7158\n",
      "Epoch 25/50\n",
      "9950/9950 [==============================] - 8s 848us/step - loss: 0.5817 - accuracy: 0.6984 - auc_5: 0.7165 - val_loss: 0.6584 - val_accuracy: 0.6382 - val_auc_5: 0.7171\n",
      "Epoch 26/50\n",
      "9950/9950 [==============================] - 8s 850us/step - loss: 0.5803 - accuracy: 0.7000 - auc_5: 0.7180 - val_loss: 0.6589 - val_accuracy: 0.6528 - val_auc_5: 0.7188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50\n",
      "9950/9950 [==============================] - 8s 813us/step - loss: 0.5783 - accuracy: 0.7043 - auc_5: 0.7194 - val_loss: 0.6587 - val_accuracy: 0.6407 - val_auc_5: 0.7200\n",
      "Epoch 28/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5766 - accuracy: 0.7033 - auc_5: 0.7207 - val_loss: 0.6607 - val_accuracy: 0.6327 - val_auc_5: 0.7213\n",
      "Epoch 29/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5747 - accuracy: 0.7025 - auc_5: 0.7221 - val_loss: 0.6604 - val_accuracy: 0.6548 - val_auc_5: 0.7229\n",
      "Epoch 30/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5727 - accuracy: 0.7048 - auc_5: 0.7234 - val_loss: 0.6595 - val_accuracy: 0.6628 - val_auc_5: 0.7241\n",
      "Epoch 31/50\n",
      "9950/9950 [==============================] - 8s 815us/step - loss: 0.5707 - accuracy: 0.7062 - auc_5: 0.7247 - val_loss: 0.6620 - val_accuracy: 0.6568 - val_auc_5: 0.7255\n",
      "Epoch 32/50\n",
      "9950/9950 [==============================] - 8s 817us/step - loss: 0.5701 - accuracy: 0.7087 - auc_5: 0.7260 - val_loss: 0.6555 - val_accuracy: 0.6503 - val_auc_5: 0.7265\n",
      "Epoch 33/50\n",
      "9950/9950 [==============================] - 8s 814us/step - loss: 0.5678 - accuracy: 0.7114 - auc_5: 0.7273 - val_loss: 0.6612 - val_accuracy: 0.6573 - val_auc_5: 0.7280\n",
      "Epoch 34/50\n",
      "9950/9950 [==============================] - 8s 817us/step - loss: 0.5662 - accuracy: 0.7113 - auc_5: 0.7285 - val_loss: 0.6549 - val_accuracy: 0.6628 - val_auc_5: 0.7291\n",
      "Epoch 35/50\n",
      "9950/9950 [==============================] - 8s 816us/step - loss: 0.5650 - accuracy: 0.7115 - auc_5: 0.7297 - val_loss: 0.6576 - val_accuracy: 0.6648 - val_auc_5: 0.7303\n",
      "Epoch 36/50\n",
      "9950/9950 [==============================] - 8s 815us/step - loss: 0.5635 - accuracy: 0.7096 - auc_5: 0.7309 - val_loss: 0.6584 - val_accuracy: 0.6688 - val_auc_5: 0.7315\n",
      "Epoch 37/50\n",
      "9950/9950 [==============================] - 8s 815us/step - loss: 0.5620 - accuracy: 0.7140 - auc_5: 0.7320 - val_loss: 0.6559 - val_accuracy: 0.6508 - val_auc_5: 0.7324\n",
      "Epoch 38/50\n",
      "9950/9950 [==============================] - 8s 816us/step - loss: 0.5609 - accuracy: 0.7163 - auc_5: 0.7331 - val_loss: 0.6538 - val_accuracy: 0.6633 - val_auc_5: 0.7336\n",
      "Epoch 39/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5591 - accuracy: 0.7161 - auc_5: 0.7342 - val_loss: 0.6551 - val_accuracy: 0.6583 - val_auc_5: 0.7346\n",
      "Epoch 40/50\n",
      "9950/9950 [==============================] - 8s 814us/step - loss: 0.5578 - accuracy: 0.7166 - auc_5: 0.7353 - val_loss: 0.6537 - val_accuracy: 0.6658 - val_auc_5: 0.7358\n",
      "Epoch 41/50\n",
      "9950/9950 [==============================] - 8s 813us/step - loss: 0.5569 - accuracy: 0.7186 - auc_5: 0.7363 - val_loss: 0.6565 - val_accuracy: 0.6538 - val_auc_5: 0.7367\n",
      "Epoch 42/50\n",
      "9950/9950 [==============================] - 8s 814us/step - loss: 0.5558 - accuracy: 0.7202 - auc_5: 0.7372 - val_loss: 0.6550 - val_accuracy: 0.6653 - val_auc_5: 0.7378\n",
      "Epoch 43/50\n",
      "9950/9950 [==============================] - 8s 815us/step - loss: 0.5542 - accuracy: 0.7207 - auc_5: 0.7383 - val_loss: 0.6649 - val_accuracy: 0.6322 - val_auc_5: 0.7385\n",
      "Epoch 44/50\n",
      "9950/9950 [==============================] - 8s 815us/step - loss: 0.5537 - accuracy: 0.7180 - auc_5: 0.7391 - val_loss: 0.6539 - val_accuracy: 0.6623 - val_auc_5: 0.7395\n",
      "Epoch 45/50\n",
      "9950/9950 [==============================] - 8s 816us/step - loss: 0.5516 - accuracy: 0.7214 - auc_5: 0.7401 - val_loss: 0.6527 - val_accuracy: 0.6648 - val_auc_5: 0.7405\n",
      "Epoch 46/50\n",
      "9950/9950 [==============================] - 8s 814us/step - loss: 0.5505 - accuracy: 0.7213 - auc_5: 0.7410 - val_loss: 0.6529 - val_accuracy: 0.6698 - val_auc_5: 0.7415\n",
      "Epoch 47/50\n",
      "9950/9950 [==============================] - 8s 813us/step - loss: 0.5492 - accuracy: 0.7231 - auc_5: 0.7420 - val_loss: 0.6546 - val_accuracy: 0.6653 - val_auc_5: 0.7423\n",
      "Epoch 48/50\n",
      "9950/9950 [==============================] - 8s 815us/step - loss: 0.5484 - accuracy: 0.7258 - auc_5: 0.7428 - val_loss: 0.6527 - val_accuracy: 0.6739 - val_auc_5: 0.7433\n",
      "Epoch 49/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5466 - accuracy: 0.7247 - auc_5: 0.7437 - val_loss: 0.6505 - val_accuracy: 0.6759 - val_auc_5: 0.7442\n",
      "Epoch 50/50\n",
      "9950/9950 [==============================] - 8s 815us/step - loss: 0.5461 - accuracy: 0.7237 - auc_5: 0.7447 - val_loss: 0.6540 - val_accuracy: 0.6668 - val_auc_5: 0.7450\n",
      "1990/1990 [==============================] - 1s 294us/step\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7 10 11] TEST: [8 9]\n",
      "Train on 9950 samples, validate on 1990 samples\n",
      "Epoch 1/50\n",
      "9950/9950 [==============================] - 8s 844us/step - loss: 0.6818 - accuracy: 0.5753 - auc_6: 0.5690 - val_loss: 0.6718 - val_accuracy: 0.5648 - val_auc_6: 0.5885\n",
      "Epoch 2/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.6641 - accuracy: 0.6168 - auc_6: 0.6193 - val_loss: 0.6569 - val_accuracy: 0.6146 - val_auc_6: 0.6321\n",
      "Epoch 3/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.6502 - accuracy: 0.6348 - auc_6: 0.6444 - val_loss: 0.6483 - val_accuracy: 0.6442 - val_auc_6: 0.6529\n",
      "Epoch 4/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.6398 - accuracy: 0.6460 - auc_6: 0.6588 - val_loss: 0.6430 - val_accuracy: 0.6633 - val_auc_6: 0.6665\n",
      "Epoch 5/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.6329 - accuracy: 0.6507 - auc_6: 0.6698 - val_loss: 0.6371 - val_accuracy: 0.6583 - val_auc_6: 0.6742\n",
      "Epoch 6/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.6268 - accuracy: 0.6582 - auc_6: 0.6777 - val_loss: 0.6344 - val_accuracy: 0.6533 - val_auc_6: 0.6808\n",
      "Epoch 7/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.6222 - accuracy: 0.6602 - auc_6: 0.6830 - val_loss: 0.6306 - val_accuracy: 0.6392 - val_auc_6: 0.6855\n",
      "Epoch 8/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.6173 - accuracy: 0.6676 - auc_6: 0.6884 - val_loss: 0.6254 - val_accuracy: 0.6709 - val_auc_6: 0.6910\n",
      "Epoch 9/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.6136 - accuracy: 0.6689 - auc_6: 0.6928 - val_loss: 0.6262 - val_accuracy: 0.6789 - val_auc_6: 0.6957\n",
      "Epoch 10/50\n",
      "9950/9950 [==============================] - 8s 813us/step - loss: 0.6099 - accuracy: 0.6751 - auc_6: 0.6973 - val_loss: 0.6268 - val_accuracy: 0.6844 - val_auc_6: 0.6997\n",
      "Epoch 11/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.6069 - accuracy: 0.6739 - auc_6: 0.7011 - val_loss: 0.6179 - val_accuracy: 0.6663 - val_auc_6: 0.7026\n",
      "Epoch 12/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.6045 - accuracy: 0.6777 - auc_6: 0.7042 - val_loss: 0.6192 - val_accuracy: 0.6864 - val_auc_6: 0.7061\n",
      "Epoch 13/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.6011 - accuracy: 0.6834 - auc_6: 0.7072 - val_loss: 0.6288 - val_accuracy: 0.6930 - val_auc_6: 0.7094\n",
      "Epoch 14/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.5978 - accuracy: 0.6848 - auc_6: 0.7102 - val_loss: 0.6156 - val_accuracy: 0.6854 - val_auc_6: 0.7117\n",
      "Epoch 15/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5959 - accuracy: 0.6897 - auc_6: 0.7128 - val_loss: 0.6143 - val_accuracy: 0.6874 - val_auc_6: 0.7144\n",
      "Epoch 16/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5927 - accuracy: 0.6875 - auc_6: 0.7155 - val_loss: 0.6077 - val_accuracy: 0.6719 - val_auc_6: 0.7165\n",
      "Epoch 17/50\n",
      "9950/9950 [==============================] - 8s 816us/step - loss: 0.5900 - accuracy: 0.6916 - auc_6: 0.7178 - val_loss: 0.6128 - val_accuracy: 0.6955 - val_auc_6: 0.7191\n",
      "Epoch 18/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5877 - accuracy: 0.6927 - auc_6: 0.7200 - val_loss: 0.6145 - val_accuracy: 0.7020 - val_auc_6: 0.7215\n",
      "Epoch 19/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5854 - accuracy: 0.6958 - auc_6: 0.7222 - val_loss: 0.6353 - val_accuracy: 0.6864 - val_auc_6: 0.7237\n",
      "Epoch 20/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5825 - accuracy: 0.6957 - auc_6: 0.7241 - val_loss: 0.6088 - val_accuracy: 0.6894 - val_auc_6: 0.7251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5804 - accuracy: 0.6987 - auc_6: 0.7260 - val_loss: 0.6022 - val_accuracy: 0.6764 - val_auc_6: 0.7269\n",
      "Epoch 22/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5790 - accuracy: 0.7001 - auc_6: 0.7278 - val_loss: 0.6037 - val_accuracy: 0.6804 - val_auc_6: 0.7288\n",
      "Epoch 23/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5762 - accuracy: 0.7039 - auc_6: 0.7297 - val_loss: 0.6170 - val_accuracy: 0.6995 - val_auc_6: 0.7309\n",
      "Epoch 24/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.5734 - accuracy: 0.7053 - auc_6: 0.7314 - val_loss: 0.6115 - val_accuracy: 0.7005 - val_auc_6: 0.7325\n",
      "Epoch 25/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5724 - accuracy: 0.7049 - auc_6: 0.7331 - val_loss: 0.6028 - val_accuracy: 0.6809 - val_auc_6: 0.7339\n",
      "Epoch 26/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5707 - accuracy: 0.7061 - auc_6: 0.7347 - val_loss: 0.6124 - val_accuracy: 0.7000 - val_auc_6: 0.7356\n",
      "Epoch 27/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5683 - accuracy: 0.7078 - auc_6: 0.7362 - val_loss: 0.6186 - val_accuracy: 0.6990 - val_auc_6: 0.7371\n",
      "Epoch 28/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5666 - accuracy: 0.7110 - auc_6: 0.7376 - val_loss: 0.5997 - val_accuracy: 0.6744 - val_auc_6: 0.7382\n",
      "Epoch 29/50\n",
      "9950/9950 [==============================] - 8s 816us/step - loss: 0.5650 - accuracy: 0.7113 - auc_6: 0.7391 - val_loss: 0.6064 - val_accuracy: 0.6955 - val_auc_6: 0.7398\n",
      "Epoch 30/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5632 - accuracy: 0.7111 - auc_6: 0.7404 - val_loss: 0.5995 - val_accuracy: 0.6950 - val_auc_6: 0.7412\n",
      "Epoch 31/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5619 - accuracy: 0.7142 - auc_6: 0.7419 - val_loss: 0.6020 - val_accuracy: 0.6844 - val_auc_6: 0.7424\n",
      "Epoch 32/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5599 - accuracy: 0.7139 - auc_6: 0.7432 - val_loss: 0.6067 - val_accuracy: 0.6945 - val_auc_6: 0.7438\n",
      "Epoch 33/50\n",
      "9950/9950 [==============================] - 8s 813us/step - loss: 0.5585 - accuracy: 0.7191 - auc_6: 0.7444 - val_loss: 0.6005 - val_accuracy: 0.6990 - val_auc_6: 0.7451\n",
      "Epoch 34/50\n",
      "9950/9950 [==============================] - 8s 815us/step - loss: 0.5561 - accuracy: 0.7200 - auc_6: 0.7456 - val_loss: 0.5970 - val_accuracy: 0.6779 - val_auc_6: 0.7461\n",
      "Epoch 35/50\n",
      "9950/9950 [==============================] - 8s 815us/step - loss: 0.5556 - accuracy: 0.7219 - auc_6: 0.7468 - val_loss: 0.6060 - val_accuracy: 0.6970 - val_auc_6: 0.7475\n",
      "Epoch 36/50\n",
      "9950/9950 [==============================] - 8s 816us/step - loss: 0.5532 - accuracy: 0.7220 - auc_6: 0.7479 - val_loss: 0.5995 - val_accuracy: 0.6884 - val_auc_6: 0.7486\n",
      "Epoch 37/50\n",
      "9950/9950 [==============================] - 8s 813us/step - loss: 0.5525 - accuracy: 0.7233 - auc_6: 0.7491 - val_loss: 0.6230 - val_accuracy: 0.6864 - val_auc_6: 0.7498\n",
      "Epoch 38/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5500 - accuracy: 0.7211 - auc_6: 0.7501 - val_loss: 0.6164 - val_accuracy: 0.6920 - val_auc_6: 0.7508\n",
      "Epoch 39/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5496 - accuracy: 0.7198 - auc_6: 0.7512 - val_loss: 0.6042 - val_accuracy: 0.6965 - val_auc_6: 0.7517\n",
      "Epoch 40/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5484 - accuracy: 0.7257 - auc_6: 0.7522 - val_loss: 0.5992 - val_accuracy: 0.6764 - val_auc_6: 0.7526\n",
      "Epoch 41/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5467 - accuracy: 0.7285 - auc_6: 0.7532 - val_loss: 0.6036 - val_accuracy: 0.6764 - val_auc_6: 0.7536\n",
      "Epoch 42/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5459 - accuracy: 0.7267 - auc_6: 0.7541 - val_loss: 0.5998 - val_accuracy: 0.6769 - val_auc_6: 0.7546\n",
      "Epoch 43/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5443 - accuracy: 0.7315 - auc_6: 0.7551 - val_loss: 0.5997 - val_accuracy: 0.6769 - val_auc_6: 0.7554\n",
      "Epoch 44/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5437 - accuracy: 0.7275 - auc_6: 0.7560 - val_loss: 0.5965 - val_accuracy: 0.6814 - val_auc_6: 0.7564\n",
      "Epoch 45/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5422 - accuracy: 0.7305 - auc_6: 0.7569 - val_loss: 0.5933 - val_accuracy: 0.6829 - val_auc_6: 0.7573\n",
      "Epoch 46/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5413 - accuracy: 0.7319 - auc_6: 0.7578 - val_loss: 0.6042 - val_accuracy: 0.6920 - val_auc_6: 0.7583\n",
      "Epoch 47/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5398 - accuracy: 0.7313 - auc_6: 0.7586 - val_loss: 0.6019 - val_accuracy: 0.6894 - val_auc_6: 0.7591\n",
      "Epoch 48/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5387 - accuracy: 0.7328 - auc_6: 0.7594 - val_loss: 0.6081 - val_accuracy: 0.6894 - val_auc_6: 0.7599\n",
      "Epoch 49/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5375 - accuracy: 0.7324 - auc_6: 0.7603 - val_loss: 0.6024 - val_accuracy: 0.6869 - val_auc_6: 0.7607\n",
      "Epoch 50/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5356 - accuracy: 0.7345 - auc_6: 0.7611 - val_loss: 0.5965 - val_accuracy: 0.6769 - val_auc_6: 0.7614\n",
      "1990/1990 [==============================] - 1s 292us/step\n",
      "TRAIN: [0 1 2 3 4 5 6 7 8 9] TEST: [10 11]\n",
      "Train on 9950 samples, validate on 1990 samples\n",
      "Epoch 1/50\n",
      "9950/9950 [==============================] - 8s 847us/step - loss: 0.6824 - accuracy: 0.5678 - auc_7: 0.5462 - val_loss: 0.6737 - val_accuracy: 0.6206 - val_auc_7: 0.5929\n",
      "Epoch 2/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.6612 - accuracy: 0.6134 - auc_7: 0.6178 - val_loss: 0.6621 - val_accuracy: 0.6497 - val_auc_7: 0.6345\n",
      "Epoch 3/50\n",
      "9950/9950 [==============================] - 8s 813us/step - loss: 0.6490 - accuracy: 0.6297 - auc_7: 0.6431 - val_loss: 0.6592 - val_accuracy: 0.6266 - val_auc_7: 0.6549\n",
      "Epoch 4/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.6405 - accuracy: 0.6428 - auc_7: 0.6574 - val_loss: 0.6470 - val_accuracy: 0.6548 - val_auc_7: 0.6628\n",
      "Epoch 5/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.6341 - accuracy: 0.6448 - auc_7: 0.6679 - val_loss: 0.6416 - val_accuracy: 0.6613 - val_auc_7: 0.6717\n",
      "Epoch 6/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.6287 - accuracy: 0.6563 - auc_7: 0.6753 - val_loss: 0.6480 - val_accuracy: 0.6387 - val_auc_7: 0.6802\n",
      "Epoch 7/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.6247 - accuracy: 0.6614 - auc_7: 0.6812 - val_loss: 0.6401 - val_accuracy: 0.6543 - val_auc_7: 0.6848\n",
      "Epoch 8/50\n",
      "9950/9950 [==============================] - 8s 813us/step - loss: 0.6206 - accuracy: 0.6638 - auc_7: 0.6857 - val_loss: 0.6506 - val_accuracy: 0.6276 - val_auc_7: 0.6898\n",
      "Epoch 9/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.6167 - accuracy: 0.6684 - auc_7: 0.6896 - val_loss: 0.6354 - val_accuracy: 0.6558 - val_auc_7: 0.6927\n",
      "Epoch 10/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.6138 - accuracy: 0.6687 - auc_7: 0.6934 - val_loss: 0.6396 - val_accuracy: 0.6457 - val_auc_7: 0.6964\n",
      "Epoch 11/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.6110 - accuracy: 0.6703 - auc_7: 0.6967 - val_loss: 0.6363 - val_accuracy: 0.6417 - val_auc_7: 0.6992\n",
      "Epoch 12/50\n",
      "9950/9950 [==============================] - 8s 813us/step - loss: 0.6082 - accuracy: 0.6734 - auc_7: 0.6996 - val_loss: 0.6376 - val_accuracy: 0.6407 - val_auc_7: 0.7019\n",
      "Epoch 13/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.6054 - accuracy: 0.6773 - auc_7: 0.7022 - val_loss: 0.6335 - val_accuracy: 0.6462 - val_auc_7: 0.7044\n",
      "Epoch 14/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.6025 - accuracy: 0.6812 - auc_7: 0.7049 - val_loss: 0.6532 - val_accuracy: 0.6216 - val_auc_7: 0.7068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50\n",
      "9950/9950 [==============================] - 8s 813us/step - loss: 0.5999 - accuracy: 0.6818 - auc_7: 0.7064 - val_loss: 0.6362 - val_accuracy: 0.6442 - val_auc_7: 0.7085\n",
      "Epoch 16/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5979 - accuracy: 0.6855 - auc_7: 0.7086 - val_loss: 0.6292 - val_accuracy: 0.6518 - val_auc_7: 0.7105\n",
      "Epoch 17/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5957 - accuracy: 0.6856 - auc_7: 0.7107 - val_loss: 0.6270 - val_accuracy: 0.6492 - val_auc_7: 0.7125\n",
      "Epoch 18/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5940 - accuracy: 0.6865 - auc_7: 0.7127 - val_loss: 0.6322 - val_accuracy: 0.6442 - val_auc_7: 0.7144\n",
      "Epoch 19/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5913 - accuracy: 0.6925 - auc_7: 0.7145 - val_loss: 0.6033 - val_accuracy: 0.6849 - val_auc_7: 0.7159\n",
      "Epoch 20/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5893 - accuracy: 0.6950 - auc_7: 0.7169 - val_loss: 0.6118 - val_accuracy: 0.6668 - val_auc_7: 0.7182\n",
      "Epoch 21/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5872 - accuracy: 0.6936 - auc_7: 0.7188 - val_loss: 0.6174 - val_accuracy: 0.6643 - val_auc_7: 0.7202\n",
      "Epoch 22/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5848 - accuracy: 0.6980 - auc_7: 0.7204 - val_loss: 0.6198 - val_accuracy: 0.6613 - val_auc_7: 0.7219\n",
      "Epoch 23/50\n",
      "9950/9950 [==============================] - 8s 815us/step - loss: 0.5835 - accuracy: 0.7006 - auc_7: 0.7222 - val_loss: 0.6433 - val_accuracy: 0.6347 - val_auc_7: 0.7235\n",
      "Epoch 24/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5811 - accuracy: 0.7011 - auc_7: 0.7235 - val_loss: 0.6256 - val_accuracy: 0.6548 - val_auc_7: 0.7247\n",
      "Epoch 25/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5799 - accuracy: 0.7018 - auc_7: 0.7248 - val_loss: 0.6110 - val_accuracy: 0.6653 - val_auc_7: 0.7260\n",
      "Epoch 26/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5784 - accuracy: 0.7024 - auc_7: 0.7263 - val_loss: 0.6047 - val_accuracy: 0.6744 - val_auc_7: 0.7275\n",
      "Epoch 27/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5764 - accuracy: 0.7036 - auc_7: 0.7278 - val_loss: 0.6065 - val_accuracy: 0.6709 - val_auc_7: 0.7290\n",
      "Epoch 28/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5750 - accuracy: 0.7044 - auc_7: 0.7294 - val_loss: 0.6179 - val_accuracy: 0.6643 - val_auc_7: 0.7304\n",
      "Epoch 29/50\n",
      "9950/9950 [==============================] - 8s 813us/step - loss: 0.5733 - accuracy: 0.7081 - auc_7: 0.7307 - val_loss: 0.6115 - val_accuracy: 0.6663 - val_auc_7: 0.7317\n",
      "Epoch 30/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5714 - accuracy: 0.7073 - auc_7: 0.7319 - val_loss: 0.6045 - val_accuracy: 0.6744 - val_auc_7: 0.7330\n",
      "Epoch 31/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5695 - accuracy: 0.7116 - auc_7: 0.7333 - val_loss: 0.6069 - val_accuracy: 0.6709 - val_auc_7: 0.7343\n",
      "Epoch 32/50\n",
      "9950/9950 [==============================] - 8s 813us/step - loss: 0.5675 - accuracy: 0.7142 - auc_7: 0.7347 - val_loss: 0.6003 - val_accuracy: 0.6754 - val_auc_7: 0.7356\n",
      "Epoch 33/50\n",
      "9950/9950 [==============================] - 8s 813us/step - loss: 0.5663 - accuracy: 0.7139 - auc_7: 0.7359 - val_loss: 0.6361 - val_accuracy: 0.6457 - val_auc_7: 0.7369\n",
      "Epoch 34/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5652 - accuracy: 0.7138 - auc_7: 0.7368 - val_loss: 0.6035 - val_accuracy: 0.6729 - val_auc_7: 0.7378\n",
      "Epoch 35/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5631 - accuracy: 0.7140 - auc_7: 0.7380 - val_loss: 0.5827 - val_accuracy: 0.6889 - val_auc_7: 0.7389\n",
      "Epoch 36/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.5615 - accuracy: 0.7185 - auc_7: 0.7394 - val_loss: 0.5994 - val_accuracy: 0.6714 - val_auc_7: 0.7403\n",
      "Epoch 37/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5607 - accuracy: 0.7155 - auc_7: 0.7406 - val_loss: 0.6129 - val_accuracy: 0.6648 - val_auc_7: 0.7414\n",
      "Epoch 38/50\n",
      "9950/9950 [==============================] - 8s 811us/step - loss: 0.5595 - accuracy: 0.7165 - auc_7: 0.7415 - val_loss: 0.6122 - val_accuracy: 0.6658 - val_auc_7: 0.7424\n",
      "Epoch 39/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5575 - accuracy: 0.7166 - auc_7: 0.7426 - val_loss: 0.6067 - val_accuracy: 0.6673 - val_auc_7: 0.7434\n",
      "Epoch 40/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5567 - accuracy: 0.7191 - auc_7: 0.7436 - val_loss: 0.6157 - val_accuracy: 0.6628 - val_auc_7: 0.7444\n",
      "Epoch 41/50\n",
      "9950/9950 [==============================] - 458s 46ms/step - loss: 0.5543 - accuracy: 0.7236 - auc_7: 0.7445 - val_loss: 0.5764 - val_accuracy: 0.6980 - val_auc_7: 0.7452\n",
      "Epoch 42/50\n",
      "9950/9950 [==============================] - 8s 824us/step - loss: 0.5541 - accuracy: 0.7253 - auc_7: 0.7456 - val_loss: 0.5963 - val_accuracy: 0.6759 - val_auc_7: 0.7464\n",
      "Epoch 43/50\n",
      "9950/9950 [==============================] - 8s 812us/step - loss: 0.5522 - accuracy: 0.7197 - auc_7: 0.7467 - val_loss: 0.5849 - val_accuracy: 0.6809 - val_auc_7: 0.7474\n",
      "Epoch 44/50\n",
      "9950/9950 [==============================] - 8s 810us/step - loss: 0.5509 - accuracy: 0.7214 - auc_7: 0.7477 - val_loss: 0.5818 - val_accuracy: 0.6849 - val_auc_7: 0.7484\n",
      "Epoch 45/50\n",
      "9950/9950 [==============================] - 8s 830us/step - loss: 0.5494 - accuracy: 0.7261 - auc_7: 0.7488 - val_loss: 0.5765 - val_accuracy: 0.6935 - val_auc_7: 0.7495\n",
      "Epoch 46/50\n",
      "9950/9950 [==============================] - 8s 818us/step - loss: 0.5480 - accuracy: 0.7246 - auc_7: 0.7498 - val_loss: 0.5837 - val_accuracy: 0.6899 - val_auc_7: 0.7505\n",
      "Epoch 47/50\n",
      "9950/9950 [==============================] - 8s 813us/step - loss: 0.5472 - accuracy: 0.7240 - auc_7: 0.7508 - val_loss: 0.6126 - val_accuracy: 0.6623 - val_auc_7: 0.7515\n",
      "Epoch 48/50\n",
      "9950/9950 [==============================] - 8s 813us/step - loss: 0.5459 - accuracy: 0.7260 - auc_7: 0.7516 - val_loss: 0.5882 - val_accuracy: 0.6804 - val_auc_7: 0.7523\n",
      "Epoch 49/50\n",
      "9950/9950 [==============================] - 8s 809us/step - loss: 0.5450 - accuracy: 0.7256 - auc_7: 0.7525 - val_loss: 0.6221 - val_accuracy: 0.6568 - val_auc_7: 0.7532\n",
      "Epoch 50/50\n",
      "9950/9950 [==============================] - 8s 809us/step - loss: 0.5438 - accuracy: 0.7287 - auc_7: 0.7533 - val_loss: 0.6257 - val_accuracy: 0.6523 - val_auc_7: 0.7539\n",
      "1990/1990 [==============================] - 1s 299us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.57      0.83      0.67       995\n",
      "           0       0.68      0.36      0.47       995\n",
      "\n",
      "    accuracy                           0.60      1990\n",
      "   macro avg       0.62      0.60      0.57      1990\n",
      "weighted avg       0.62      0.60      0.57      1990\n",
      "\n",
      "\n",
      "############################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.72      0.70      0.71       995\n",
      "           0       0.71      0.73      0.72       995\n",
      "\n",
      "    accuracy                           0.72      1990\n",
      "   macro avg       0.72      0.72      0.72      1990\n",
      "weighted avg       0.72      0.72      0.72      1990\n",
      "\n",
      "\n",
      "############################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.79      0.64       995\n",
      "           0       0.59      0.31      0.40       995\n",
      "\n",
      "    accuracy                           0.55      1990\n",
      "   macro avg       0.56      0.55      0.52      1990\n",
      "weighted avg       0.56      0.55      0.52      1990\n",
      "\n",
      "\n",
      "############################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.65      0.71      0.68       995\n",
      "           0       0.68      0.62      0.65       995\n",
      "\n",
      "    accuracy                           0.67      1990\n",
      "   macro avg       0.67      0.67      0.67      1990\n",
      "weighted avg       0.67      0.67      0.67      1990\n",
      "\n",
      "\n",
      "############################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.69      0.65      0.67       995\n",
      "           0       0.67      0.70      0.68       995\n",
      "\n",
      "    accuracy                           0.68      1990\n",
      "   macro avg       0.68      0.68      0.68      1990\n",
      "weighted avg       0.68      0.68      0.68      1990\n",
      "\n",
      "\n",
      "############################\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.89      0.35      0.50       995\n",
      "           0       0.59      0.96      0.73       995\n",
      "\n",
      "    accuracy                           0.65      1990\n",
      "   macro avg       0.74      0.65      0.62      1990\n",
      "weighted avg       0.74      0.65      0.62      1990\n",
      "\n",
      "\n",
      "############################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  # hinge nice\n",
    "from keras import backend as K\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "def auc(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc\n",
    "from sklearn.metrics import classification_report\n",
    "batch_size =32 \n",
    "epochs = 50\n",
    "scores_list = {}\n",
    "reports = []\n",
    "kf = KFold(n_splits=6)\n",
    "tx = X_transformed_list_np#[bads]\n",
    "ty = y_transformed_list_np#[bads]\n",
    "for train_index, test_index in kf.split(tx):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    if(test_index[0]>=0):\n",
    "        X_train, X_test = tx[train_index].reshape((tx.shape[1]*10, 28, 28, 3)), tx[test_index].reshape((tx.shape[1]*2, 28, 28, 3))\n",
    "        y_train, y_test = ty[train_index].reshape((ty.shape[1]*10, 2)), ty[test_index].reshape((ty.shape[1]*2, 2))\n",
    "        model = get_model()\n",
    "        opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\n",
    "        \n",
    "        # Let's train the model using RMSprop\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=opt,\n",
    "                      metrics=['accuracy',tf.keras.metrics.AUC()])\n",
    "\n",
    "        #x_train = x_train.astype('float32')\n",
    "        #x_test = x_test.astype('float32')\n",
    "        #x_train /= 255\n",
    "        #x_test /= 255\n",
    "\n",
    "\n",
    "        model.fit(X_train, y_train,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  validation_data = (X_test, y_test), verbose = 1, shuffle = True)\n",
    "        #scores_list[str(test_index[0])+\"_\"+str(test_index[1])] = model.evaluate(X_test, y_test)\n",
    "        scores_list[str(test_index[0])] = model.evaluate(X_test, y_test)\n",
    "        report = classification_report(y_test[:,1],   model.predict(X_test).argmax(axis = -1), target_names=['1', '0'])\n",
    "        reports.append(report)\n",
    "\n",
    "\n",
    "[]\n",
    "for i in reports:\n",
    "    print(i)\n",
    "    print(\"\\n############################\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.predict(X_test)\n",
    "goods = [0,1, 2,3, 8,9, 10,11, 12,13]\n",
    "bads = [4,5, 6,7, 14,15,16, 17,18,19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(X[202]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdsAAAHjCAYAAABrU7X0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdeZwlVX3//9fp2+v0TM/K7APDMkADCsI0orgAKnELikv9XGJMouI3StSo32hIvl81GsUETUg0fr981aCJWyVqYgxRUVRAI94B2YedYVZmX7unt9v1+6PudNet6c+ne3q6uu/A+/l4zGPq1KdP3XPvrVPn1nLOCUmSICIiIsVpmO4CiIiIPNWpsRURESmYGlsREZGCqbEVEREpmBpbERGRgqmxFRERKZgaW5k0IYR1IYQ/d+K/F0IYzKQvCiEkIYTl1fTKavp5U1Fekcn0dNx/83XY+JufhRC+mElfH0L4cSb90RDCI0WXdbqpsS1Afmeqrjs3hPBkCOG7IYS26SrbRIUQTgghfCWEsCGE0Fd9Lz8OIbzkCDbzLWBZUWUUOVL5hiCzfnm1EbloGopVN0IIl4UQbg0h7AohdIcQHgkhfC2E0HEEm3kN8P6iynisUGM7BUIIlwI/B74DvDZJkoPTXKQjEkJoAn4MrADeBJwKXAb8CJg/3u0kSXIwSZKthRRSRCZVCOES0mPWjcDzgGcC7wb2AS3j3U6SJLuSJNlXSCGPIWpsCxZCeAvwH8CnkyR5V5IkQ9X1vxdCGAwhXBhCuCOE0BNCKIcQzsvlvyCEcHMI4WAIYXcI4eshhIXV2IzqWeaLM3//8+q6GdV0awihN4Tw8mr6ZyGEL4YQ/lf17HRX9Uy83XkbZwKnAO9JkuSWJEmeSJLk10mS/FWSJN903vuLQwh7QwgfyL7nCX2QItMoc7n0JdX62BNCuD+E8Ftj5PtQtd5eVE2PWf9C6oMhhMdCCP0hhEdDCO/LxN8eQtiYSR+6fP3PmXW/H0LYWt3WoXgUQviPatkfqx6bPJcBdyZJ8rEkSe5PkuTRJEl+mCTJHyZJst14vw0hhM+FEDaGEM7KvucxXuspT41tgUIIfwJ8CXhnkiSfGOVPGoBPAe8FzgV2A3EIobGafzHp2eNG4Hzgt4GzgG8DJEnSA9wGvKj6923ABaS/PJ9ffY0LgRJwS+Z1XwfMAy4iPVN9NfAnzlvZBlSA14UQmsf53t8M/Bvwh0mSfGY8eUSOAdcAnwTOBtYA3wohzMn/UbXR+XvSuv3CJEl+lgmPVf/eBXwcuJr0h+5fA1eHEN5Wjf8EWBZCOK2afhGwHbgks41LgJ8mtePxXg38E+kZagz8YwhhlfNetwCnhBDOd/4m+55bgX+pvvZzkyS5dzz5ni7U2Bbn+cCngbclSXK98TcBeF/1bPEB4H8DJwEnV+OHLtn8XpIk9yRJcivwFuB5IYQXVP/mJqqNLemlno2kO/yhdZcA5SRJ9mded32SJH+cJMkDSZL8APgmcKn1RpIk2QxcCXwQ2BtC+EUI4dMhhNWjvqkQPgj8A3B5kiRft7Yrcgz6WJIkP0iS5GHSBnI28Ozc37SQPp/wEuA5SZLcnYuPVf8+DPx9kiTXJUnycJIk/wf4AvBnAEmSPA6so7aOfwGYGUI4o7ruYtJjQ9bnkiSJkyR5BPhzoJfaBjrv74GbgdtCCFtCCP8WQnhvCGG0W0dzSE8MFgPPS5JkvbPdpyU1tsV5AFgL/GkIYanxNwlwVya9qfr/our/ZwK/SpKkfzhDktwF7K3GIK1Q54YQZpNWnJ8AP2WkEl3C4ZXuzlx6U+Y1Ry9oWuEXA68lvYfzQuDXIYQP5f70CuATwCVJktzobVPkGDRcd5IkeZL0ik++7vwj8AzgwiRJnvC2UTVc/6oPHi0nbeSyfg6sPHR7iNo6fjHwQ9KrV5dUz3iX4dT7JEkGga2jlJ3M3/QkSXIZcCLwp8Dm6v8PhhA6c39+Q/X/FydJssva5tOZGtvibAdeQPrr8eYQwgmj/M1QkiSVTPrQJZ+GUdblHVr/K6CP9JLUoYb1p8A5IYTjgdUcXun6c+mEcewLSZIcSJLkhiRJPpokyQXAl4G/yF1a/m/Ss/G3hRDCWNsUmWZ9pGeneYcuDffm1ufrDhxed/6TtIF6qfGa46l/+Xqfr0s3AReHEM4EZgG/ZuQq1yXAhuoZ7JG+7mGSJFmXJMn1SZK8C+is5svfdvoecB7wnLG293SlxrZASZLsIN3xtwO3jHF/ZDT3Ac/JNmYhhLNJDw73VV+jH/gFcDnpfd+bqq97H+ll6Qrwy6N8K5a1QDO1B6t7SH9pvwa4Tg2u1LkHgPNCCKXc+vOBIeDhCWzza8BbgS+HEN56JBmrT+1uJL1ylPUC4PHqcxqQXsGaB/wxcHP1TPWmar4Xc/gP7EmRJMlu4ElgYS50NfAR4Psh7X0hOWpsC5YkyR7SezePkJ7hnjlGlqzPAR3A9SGEs0LaWf6fgFuTJMk+8HQT8GbggSRJtmXWvRX4ZZIk+V/nRySE8KzqU4xRtRwnhRD+P9Jft7/IP5mYJMl9pJX+5aQPYWg/k3p16PbIP4YQzgshnBxCeAPpQ1BfTZJk50Q2Wn1K/03A/w0hvOMIs38K+KMQwjtCCKtCCO8E/rBapkPb3wI8SFrHDzWsd5L+QLiMSWhsQzrYxDUhhItDCCeGEJ4RQriG9CHN7+b/PkmSa0jvN/97COEVR/v6TzU6CE6BJEkOkDY8dwA/CyGcO858W0kfnFgOlIHvA/eS3jfN+gnQSG0Fu2mUdRO1gfTHwlWkZ9H3kD4t+RXSij1a2R8kbXAvAf5plDMHkWmXJMla0if455B20bub9EGkzwLvPMptfxuIgL8LIbzrCLJ+gfSq1FXA/cCHgA8nSfKl3N/V1Pvqk8c/Z/Lq/c9J+9b/I+lVrJ+SXib+nSRJRu3KkyTJ35EOYPHtEMKrJqEMTxmh9slwERERmWw6sxURESmYGlsREZGCqbEVEREpmBpbERGRgqmxFRERKVjj0WSOouilwLWkA91/MY7jq8fIokefRUY37YN/HGF9Vl0WGd2odXnCZ7ZRFJWAzwMvA84A3hhF0Rl+LgghEEJgzZo1w8v18K+eypMvy7Hi6f49TfRfPZhIfX6qfy9FlOdY8XT7nqaiLh/NZeTzgUfiOH4sjuN+0pkr1IlZ5Nik+ixSoKO5jLyMdGShQzZy+FRTRFF0BelMMMRxTLlcBqCzs3N4uR7UU3nqqSxHYirKXE+fTT2VZRKMWZ+tugz19VnUU1mg/sozHqrLk+9oGtvRzpkPu48Tx/F1wHWH4l1dXUD6ZR5argf1VJ58WY6VUb6m4vOr5+9pourk+x2zPlt1GZ6a38tkyZanTr7rMakuT4z3/R7NZeSNpONmHrKcdL5DETn2qD6LFOhozmzLwKooik4knfz4DaSzXIjIsUf1WaRAEz6zjeN4ELgS+CHpjBBxHMf3TVbBnm6SJBn+t3r16pr0sSJb5vw/qW+qz5Mnv+9n6/OxQnV58h1VP9s4jm8AbpiksojINFJ9FimORpASEREpmBpbERGRgqmxFRERKZgaWxERkYKpsRURESnYUT2NLEfmy85j8/+WWX4h8PNMegW2051Y+/iKdYR6nFivGUmS7WYshOOOojwiU++fnbqcH/TvDOD+6nKns82ZR1mmqZIM2u89NB47ky1MNZ3ZioiIFEyNrYiISMHU2IqIiBRMja2IiEjB1NiKiIgUTI2tiIhIwdT1Z5K90ukS8N9OvqWZ5fOBOzNpb1LRx53YcgbM2ClsNGMLyHbTORPITv6yz3nFPju0Z8gMJb/5TzMWnvUK5/VEivOq5Idm7EF+acaa6KhJn8zJPM6jALQ4HfJOcTr/zMhts0bSYsf25NKzgP3V5W47m3PoAKd3T7LJ6Ra07OndLUhntiIiIgVTYysiIlIwNbYiIiIFU2MrIiJSMDW2IiIiBVNjKyIiUjB1/TlCK5yuPeB3xdnhxNZllt8ENR0L5jj55rPejC2t2WqtR9lixs5i1/DyKlbyMLcPp8/uPmAXZrvTX2Cj87k92mSGkg9cW7ti0YrhdeEz77W3KTKGucn/duN7uMOM7aTNjD1Ja016gEU8yS8AaGeGma90wO76c/x2+ygwa7cza9aBpbXpZ3XAPdXue33z7Hx2Tz2/1bDfHsn9uWPAypF14YynfrcgndmKiIgUTI2tiIhIwdTYioiIFEyNrYiISMHU2IqIiBRMja2IiEjBjqrrTxRF60jnkKgAg3Ecr56MQtWzZWPEm51YqxPLPjHfADVzg3jbLLnP6NuxipOrPzOtR5JLu7uM9/R+g/O7ruTMWtKYmyUllKBxlvNCMlFPt/q8YozD30InPs85T5mdi5Uy62Y6+dqG7ArU7PU4DE6wlIuFzDq7x53Pqa7OxEUwO5cujbLuKWwy+tleHMex14VURI4dqs8iBdBlZBERkYIdbWObAD+Kouj2KIqumIwCici0UX0WKUhIxhh+0BNF0dI4jjdHUbQQuBH4oziOb879zRXAFQBxHJ+3Zs0aADo7O1m7du2EX3uyjbc87av921jubctxxpYDG8eZr5E+J9Zvxpqxh1ZsY3B4uZW59LJ7ON1ecfaXQSfmjORIn/MOD+ZiC2fDtr0ArHlynbPR4k3WPrw63aemfby6seqzVZehvurzeMvStnqpG291al6r83W15PLNZg572TNqLKu5YseaBktmrKHi3A0cyj3x0VaCg9UnNpIJ3kWc6EEu/3KNcOhQs+buNfm/nlJTUZePqrHNiqLoo8CBOI6vcf4sCSEtR7lcpqura1JeezKMtzwXjPF5TcYDUn8N/M9MOveIUI05zvjHi5zYCWw1Y52ZxvUMXsP9fGc43dV90C7MjkE7tsmphY85T1zcm3v3V/42fO4/AAif/gM73xSYrH24WgenvbHNGkd9Hq7LUF/1ebxleWbyF278NGf849Ocp4ROydX0V/Bq/pN/A2ClM3jwin12TV+2w36SqGXvAjNGT+4Hxdmz4a70xyoD8+18nok+IJV/uQUMDxgflk3v7j8VdXnCl5GjKGqPomjWoWXgUuDeiW5PRKaP6rNIsY7maeRFwHejKDq0na/HcfyDSSnVNDvBOXt1zusA6HVi3iOeQ5luOr0EHmCkDE3JXWa+GYMPmbHZAxvM2MP928zYYz37hpeXLXgZ/7njzuH0+v32Zevn7bS7Gi3a7PQz2GKfQfBkrpvPQC88+TAAyTM/ZecDwt1/6salxlOzPievNUONPOhmPehcU9rrHDp37q3d1wfbf4ud3fcBMGOzfVrYuM0+6x3abXd3O27fXDM2u3dx7YpTnwv3/Tpdrhxv5qPhVDvW5tRlryvP/ly6g+HpzpLH/SuG4cS6uvAzIRNubOM4fgw4exLLIiLTRPVZpFjq+iMiIlIwNbYiIiIFU2MrIiJSMDW2IiIiBVNjKyIiUrDJmIjgmDQn172nNMq60Qzt8eMVZzqdbuenze7+u4eXB+adyuZdI116egbvHC0LAMngw2asddDu+jO7f6cZu7935Bn9aM5Bvr/1vuH0g/vtoaDW7bYfz3/xTnu4j7N3OsN27Mz1JRjsh52PV2Nz7HxAwofMWODTbl45hiTn51a0D6+bxQNmtnZ3BAZoPGjvXwe22YfO3Rtr05Wz+9h97yMAzNhsjwRV2mbXkaHddreg3gN2f5v9fcfVpBe+7Gy2rU1Ha+pItpj5ZjbZx4eGGc8wY8yzuyEd1i/ydMA+RNVIHrWPzeHkY6NbkM5sRURECqbGVkREpGBqbEVERAqmxlZERKRgamxFREQKpsZWRESkYGpsRURECva07Wd7Xi49I7Nu8T5MM8foZ9tvzzLHbmfS5S2lkT5/rTTQmYykNyR2H7udznT13cGeCqu/ZMcGm0a22R8CGzLphha7n2B7mx1b1m5Po3f2rA4zxuxcv71SCWbPS5fnjDH59dZ5dsyZ516ONfl9q2F43XnYU0J2Djr9u4GOvXY/26ZeuzLPIt+Hv5FZpPtiC3ZH/Cavu2iwz4sqDfZBp69U2y9+iGR4XXewP5uh5h4z1tbWbcZa2p1+tvlZAkuZdX6XeVg4RvwYoDNbERGRgqmxFRERKZgaWxERkYKpsRURESmYGlsREZGCqbEVEREp2FO6688lg/a0TDNys0s1zIcZ1VmlZj5hb7N110H3NZOhR81YQ6s9bVUyMxObOZ9k3ciUe72NdoH6nNcjedwMDVS2m7FtgyOP/Q+e0Me2zHR/2w7a3QzK++3uRPfst6cz23TAfu7/Awdyn/fQABzYnC4fsKf7A2DQLmvCZ81Y4P3+dmXqJc80QyUezK3pHV636FG7r968DUvdl2zbbfcPG+q2uw31Hajd74ZWDdB3zzYAdu50uttstWO9u+1j2f5uu/tf60BtvTuut4cnHrwdgJZw3GhZAGhq22zGmmfvsmPd+ekOR7Qlp9Skl/bD5k3VmN0zEIBGZzbEbRX7s1lYqp/p93RmKyIiUjA1tiIiIgVTYysiIlIwNbYiIiIFU2MrIiJSsDGfRo6i6MvAK4FtcRyfVV03D/gWsBJYB0RxHO8urpgiMhlUn0Wmx3i6/lwPfA74ambdh4GfxHF8dRRFH66mPzT5xTs6l+y3YwtyvXDmzIFXVNct32Hna9ztdLUB9lfWm7EnW+3phB6rjHRjmVkZ4sK9IzNrzJ9pPxdfbrYf398RnONlQ68Ty1zwCA3QnHnuPtiP2ZO0mqFfD8w2Y7O7F5ixcxYuqkl3NTZRXrgEgBct8btusGexHdvrxJ7arueYrM/2vnX+rtouZ+0dgfP3pevO3WzXnRXb/KlmSvsWmbH+Pjtvz0Btd5OmpIVFvScAUAlO18EW+4A16MzC09tvzyRUIdcNKSR0N6Xrep1jQINTloY2+7jS0G4fPJtm13b9WVCC9dXDQrMzWRCA00uJyjFyfXbMYsZxfDOQ71j1KuAr1eWvAK+e5HKJSAFUn0Wmx0R/EyyK43gLQPX/p8BsgyJPW6rPIgUrfASpKIquAK4AiOOYcrkMQGdn5/ByUZY5o440nlybntMCr6mua1pu5wuVk+0gUElWmLHBBvtyT19p5PLsovbZ/PEFLx9O95bskWy6Q7/9ejiXinFGX0pGLj11tp1M+ax/ycScTQ45o7UM2hPLdwzYu+Hi3twlwiWL6bqqeoXzoD1iFQC9Trxix8qMb7+cin24nlh1Gabis7BHbGrvqN2XV5ZO4Usd3wdg7jnOZO1n2hPAA4SKHU8Se38eSmrrwZx5i7n8jVel+YbsUc3CoF3Whoqdr8EZQSnkyjJz+fE8/6+uTWPBfg80OHWnZF/SD00z7Fju45zRAee+uBqzN5nyvyrTePfJqajLE21st0ZRtCSO4y1RFC0Btll/GMfxdcB11WTS1dUFpB/CoeWifMIZ4mzBxtr0a06G71Rvxy5fZ2+zsHu2c0cOGH98wcv5m1/dMJx+YKY95Fy52R6ScQcPmTF40g5VRu4Xl8/6F7ruff1IbMBpbbudGrPbvmf7kk32PdsPPZS7Z3vVhyh/8tMAvOieMe7Zrp3YPdsuftvfbtVk7cNJ4v2CmRLjqs9WXYYpqM+JPQzgc/bVjr36pY7v87Z9rwTg1fcdMPOt2HKi+5KlffYP6yO5Z3v5G6/iu9/4JACVHvuebfNu+z7pjF32PduWA84PioHaRvP5f3Utt/zJewEoNdh1smGGXT8a5q6yY0vOs8ty0gU16XNfDHf8OF1uPtXMBkA4zY5VnKEenzfOfXIq6vJELyN/D3hrdfmtwL9PcDsiMv1Un0UKNp6uP98ALgIWRFG0EfgIcDUQR1H0NmA98Hp7CyJSL1SfRabHmI1tHMdvNEIvmuSyTMhnN9qn7S2P2PkW5mKNS2Hhveny3Me/Z2fceZtbnkplixlraXUuIc0f+SrC2RfSfM+PhtOl4+x7oU2znf5NLXZZaLAvd8PekcVKH+zNXDq3bx1Br31fjb55ZuhGp+vChtxlsH9JBrlyIH2Y9jP9s5zCwMv7va4d9msmzj3bQLG3PopW1/X5rk47tuZ+M7TikdpLxc2X9rPiRxsAWLrW3uRxW+3ZcgAqPd6lYvvQOZA7rIbLhmi6La37vUP25eDug/Ztpr17nS41++x8Df21z3v09/ay/sH0QJeU7BuhQ+12X5xK/h5c1oA9s1lorX2+5PSBZ/Gbzb8BoDTvBfY2gSHnjlCfcxn5I85l3Y+FqZ0R6BjpoSQiInLsUmMrIiJSMDW2IiIiBVNjKyIiUjA1tiIiIgVTYysiIlKwwodrnAxfW2c/vt2/wc7X8bAdW3Lft2vSTc+7hCV33gTA/Ie/aOYb2OYP6TU4aD/63uY8ot5y3Eg3hHD5+2gpx8Pp5mX2SC/NS53uC/PtbgbMcGYEyu4VQwkccIZ2zBq0R7qi4pTFGfrugaGOmnQvQzwwlHZ1eHOlzy3O1QP2o/3vdGaRAbvbQ1LTZSjUpAPO8HeS+pg9kw632P105uanTsg4MZet5Xw48Xujx7Lm79hqB4Hefnvc1r2J3S2oUqo9h2nYnzDrZ2mfuZ4mu+9cb2Lvzwf77O49lQObzFhysLbu9vf2s/7+tOvOoHOq1ef0qutd/IAZG3BG0KvMrH0Plw2cyJrN6ReVjDEYXF+33TWo24zUdGI8zKpMt6CWXPrhAroF6cxWRESkYGpsRURECqbGVkREpGBqbEVERAqmxlZERKRgamxFREQKdkx0/XmOM7FN03Y7NnfLr8xY+5P/Vbti4DwuOLTuyZ/ZG93Wa8eAg04vmYVOb5OO/pEZMdoHEs7bNJJuC/abbGzsMGOVphlmbGPLQrswrZl+FiFAY2Y3aRo8/O+H2a9Hq92NgoZT7Fh/biaY5lZYeQYAe3rOcMoCH07sWWR6HrRjf+xMDH64qZ055Ji31Z5tCXtiGy50euk8MzeDV1svPPPBdPnEdXa+mfuW2UFgv7P/lBrt2c4HmmvrZGOlnTn709mietvs40dfk/0m+5ofM2Pds+1619fyeE26Umpg35y0D2If9ixkPTPNEAdmHTBjPbOeMGP9s2tnbuov9bKuuq5/rnN8ALrn2V3G9mHPLO/0GCPbGbEPcHqKTgqd2YqIiBRMja2IiEjB1NiKiIgUTI2tiIhIwdTYioiIFEyNrYiISMHqpuvP7i1PmrHKfQ+asRlrf2zG2n79dfsFy7+sTb//f8BPv5Qu2xNXjMnrGNPvxAYybzE5AAO/yMScGTg43Z4NpOkMO4bdcwFWZJYrCezLvCt7AiJocWb2aXSmZ5plz7LDrFxXiVIysq7Nn2VnT7PdLer92N17vuts88OZ5QuBzNdEgj07VVAXodQ/2f17mp1edac4XeryHcDaeuCMO9JlfzIZp98g0OGcizQP2Pts/0Btt7rSUBNzu5cAsL/bfiPe3tzPNjO21znq7Mx17+mvDLFpz8FqPtseZwKvnU4Xrd0Vu5w982q7Wx7o/wt+sbG67mRneiaAnofs2Nw3ORmf52/Xkth1mQnOCKQzWxERkYKpsRURESmYGlsREZGCqbEVEREpmBpbERGRgqmxFRERKdiYXX+iKPoy8EpgWxzHZ1XXfRR4ByPPzl8Vx/ENR1OQObu+bwe32F1/2PwzJ1/ZjuV7Gg2Msm4CnMkysDuiQLYjQSmXXuzMerT/UTvW48wy1OtM0LOlPZMYpHbqjJltdsaWFjvmzfrT7sz6U1p1+GucfGidPdsHAB1O3PkybvmNHXts98jyfwJXZGJv9UtTF6aqPpucffk5TrYznVi+e0/TKOtGd/YY8WebkYQLnVh+X+8g4UXDUUuJTWas0XlHgXYzdvgcS61Uqv3+DmJ3pzng9EPa6/TU6/EOcvmDY0Nm3Yx1Tkag9VtO0OkXhnMQZLX/mpNsPP1srwc+B3w1t/5v4ji+ZtJLJCJFuh7VZ5EpN+Zl5DiOb8afFlBEjhGqzyLT42hGkLoyiqLfBdYAH4jjePdofxRF0RVUr7bFcUy5nF7a7ezsHF4G4MQT7Fda1GfHXvD7dqyn247lQ6d3wi+r5XEGDzkaS5zYvMxye2cnqzOfzTOcfH3ON3jQuYLS41wNHsjk65zXSfkNme+p2fl91uiMrBKa7RheYWovkXXOmkf5ojekiefYl88A6HeuhdnzZtfOKp3TlLkut6oT/jPz0SxwNnk5zi2N+jBmfbbqMoxSnyfAGyhtnhPLjwVW6uxk7rjKMsb+w3wzMte5YdRO7b7e2lmis5wOvXayc3AZwL63M3DYpels7AVO7I9q0is6T+Zvy98GYBB7yC5vJLyBJjs25F21zb29zkWdlN9T/Z68Lx9gptdU2aPBwXFjbLhaFhh/DZ3gfj7RxvYLwMdJm6WPA58B/mC0P4zj+Drgumoy6erqAqBcLnNoGSC574v2q93h3LNd8zM75n0oa3LpX5bhudXyeOMqHoUtTuyxzPLqcpk1mc/GuS3Lw85RaK1zo+v2s+zYlsytzvIbynR9c6QsHO80jHMneM+24tw729ZVkyxf9Aa6fvbNNPGofU8NgHXOPZm1zq7v3LNdlr1nW4ZXZIrn3bP9JF1mLPGGhpsa46rPVl2Gw+vzRLzQif2OE7ssl55bLrO7WpaFh/95xljl/T0zstu5Z7s+1zB2lmeztisdHHGD09hucu7ZbuFuJ3arGdvKz2rSf1v+Nu/rei0A2517tjud36nbnA/1gDcMbO7jLr+nTNffVVc+38kHcKFzoJv/GifjO53YyPGhfHjxbM5+7tXlCTW2cRxvPbQcRdH/A5ynm0Sknqk+ixRvQl1/oijKXhG9HLh3coojIlNN9VmkeOPp+vMN4CJgQRRFG4GPABdFUXQO6WWndfjn6uOz72Y7tuceO7bLuebnPQaSv1ScjLJuAryH0A9/DH9E/m5nNu3dBpnrvMelj9mx/U7Xn+7MraxSP3RsHEnvm+XcXOlYbMeaF9mxFue+SkvumlVzI6yormvyLxIy19m9nffv/QTd9OuR5YESbMp0dfiCM8lSvZiy+mzwbs15HbmcmxCHTURVGmXd6E4fI95pRnqdJyl6czvQENBbvf/rHR+GnLv+wdlhG50jSyMHDttOY/XyaWjdaS8Bi+cAACAASURBVOarzLJj7ixkzl0m8vd6Q2adP4EXBO9g/g0n5lX0bB+mZVBzGf/kMQp05MZsbOM4fuMoq7806SURkcKpPotMD40gJSIiUjA1tiIiIgVTYysiIlIwNbYiIiIFU2MrIiJSsKMZrnGSbbNDlQ12zOuu44zyWJQ9TmyvE8uOHpiMkra43YI227HlTt+I7kyspR9OXT+Svr3jwOEZqhKvv8VsZ2i8Vmf8t/ZcrDHAguq6GU4+GG//j8M5M9PUzAzVDBw/ktyt3qljcgZldYczdSaaYbRxy0bW2UMu+h2KYD92V7Z9znlKvoYMZdbZAyT6QyQmrDBjJU4yYy3UDukUaB1e1xYeN/O1Nv23GWtyBorrc0ZsHbWH0qF13gczZtwZlpcfObHsMHqvBm7KpCe/64/ObEVERAqmxlZERKRgamxFREQKpsZWRESkYGpsRURECqbGVkREpGB10/UnPOcHZiz518vtjKf+2I71O3048o+vtzIyCYg3W7s3bccYYe+Xzczc3+XTlllOj5rjltmxZUvt5/dXzB7pLtFRauLFs5cOp09otLtD/Lr/eDO2od+ZVTrYs6vQfkptuqF1ZN0suzsE4M9A4vWz8Lr+ZCdDaaV2Yph7vX4PAtnpug/nzBl12KxYWXuonTVqJo0cqK6bw3PMfPvwZjqHTdgTlm93ehQd6KhNV1rgwInpcr/TVy84PdnaSnbGOSX7k2tsrH2PTe0tLLqg2vWnebeZb/ZMeyahefN+bcaedCb32pzr9xWaoaW6rs+e8Cg1Z4y4yZuiKHsMDLXpMPl1WWe2IiIiBVNjKyIiUjA1tiIiIgVTYysiIlIwNbYiIiIFU2MrIiJSsLrp+uPypvyYaz+eT4fTh2NmLp3pb7N3hp1tozd1D7WTwuR5PUqyPVEGqJ0DyXsIveTMejSz187ZerBkxtq7R0rTOpSwKpOeecCe9af1wA4zdusB+0N94qDzgc/Kf78rgeo0RC1LcTUt9OMWbyapbCwZ42/lMKc5Me/bcvYQBnM1JMms2+t0/dhx2EGg1rYldve4nc5hZ0+uO16lEfZUeycdcLr3HHQqutelMBmy63Ipyb+HBkohXdc6ZPeNm9lrfzaD+51Yi3186Ntem24chPnVdZu9AyfAdifmTSXlNnHZaeESip4mTme2IiIiBVNjKyIiUjA1tiIiIgVTYysiIlIwNbYiIiIFU2MrIiJSsDG7/kRRtAL4KumkHEPAdXEcXxtF0TzgW6R9MdYBURzH9jQSR2PBXWaob5HdF2f3cWaI7blZJk5uhEer6zbusvPd0WPHALYNLDFjQ868Ju2Zzg0XMJM1XDicnof9aP+CIbu/wDxnZpt5/Xa+Rf0jr9eStLGq/xnD6RP67NlHVvbaXQmWdtt9Hvp33mzGrp3xUO2KjrPgwLfS5TBGP6zeF9uxHvt7cvtoZb//oVz6GDDd9fnVTszriOPNCNRY01EOYJAF1XU92PvI5hP9mV3Wdtr7+iZndpu9uWpw6Qy4uzrdkdu9x55oh0Gn70+l3+7eVBmo7Tc50FJi8wnpusGK3dlqILHrR6XBnk7s1MceNGPLcl9T+5vg/NvT5bvH6EL3mNf3y5sxqH2fE3w8s9yXS0++8ZzZDgIfiOO4E7gAeHcURWcAHwZ+EsfxKuAn1bSI1DfVZ5FpMGZjG8fxljiO76gu7wfWAsuAVwFfqf7ZV/B/tIpIHVB9FpkeR3TPNoqilcCzgNuARXEcb4G0AuMPACMidUb1WWTqjHu4xiiKZgLfBt4Xx/G+KIrGm+8K4AqAOI4pl8sAdHZ2Di+P6RR72LSmZfaNyfnPt2+CdBysTbcc38nJ16blOd4ZteucMe4tDCTOeGzYsYbM756FnSdwZfmLw2nvS2p0BnMsOUUpNduxkInNWHYC51z9+eF00mz/PhtwYi90YkON9of6O821b6KzaRnlpR9PEw2zzXwAzLTvZbn3ec5xYm/JlOV4KH8uE9s3zv25DkykPlt1GcZfn72R9ewnE/zYYTo7oVqWFux95JwWbyeA01rtSjLg1K1KrkouWgwf/JN02XnEgiSZWIzEHs82Sc6rSS9b1s6n/vK8auwMOx+vcmL2gwqNg71mbCh3CFi+opO//kz6PXkjtgL0tTtB+9Y6YD9HQmbf6GQhZd41Eiq/xi/QBITE/RZTURQ1Ad8HfhjH8Wer6x4ELorjeEsURUuAn8Vx7A19CpCEkO5t5XKZrq6ucRUyueskM9Z3r/000+7f7DFj2++tTZ98bZlH35uWZ+NDo2SoumODHYPJeUDqyvIX+VzX24fT7gNSJecBKechjo5ldr6mE0Ze75yrP8+dH373cLr/BHvPfvIEe8e+a7ldW/rn2x/qtStqP7Py0o/Ttfl/pYlZLzXzAbDHeUDqQecBqV8527wtU5bPQdeVmdiN/gM3lmodnFjmCZik+jxcl2H89XmtE5v4A1I55TJUy9LDK818t5/4e85W4b7OV5ixTYvsepB/QOqDfwLX/FW6XMwDUvkHxDKxgftq0p/6y/P40z9Ln0oarNxplyW53Y41rDFjC7bbD0h1577gv/5Mmf/5gfR7uvsZo2TIeOwCJ3iJE2s/0wleNrxU5l108Q8jofApv0AGry6PeRk5iqIAfAlYe6hiVn0PeGt1+a3Av0+odCIyZVSfRabHeC4jX0h68eyeKIoO/RS6CrgaiKMoehvpNCyvL6aIEM5+zIx97yF75pdt7d1mbMvC2p+Kf9AB/179hfTASXafoVs3vcyMATzxpH021bjHPlFY2D9yi+yNLUv4pxP/eTh9QpN9GX2Vc6X0lHn25dnlx9mXe+YtGOn7Umlazq6lVw+nO2ZtNfO9ed/H7Ng65xKjc0luR0ftNd15pV7e3HE/AF9r77AzAlScqzZznmPHjltlx7Jnr/vKcOP4rs7UkWmtz51OLPF++ntHqvwlyBIwJ11sW2HvAzue55xKAnc+2857n32xjS25GYHesRBuqF6h3OfcvhkcsmPBubXVeNC+vd62v/ZD7V7Qxq/flp5Gzt1tV7y3f8fuMvPub95jF8bxrxfXpucNwZurkwSd5c3qA9zm9Mr5pd0zlO5nO33zGjNnr+XXQNfEzmbHa8zGNo7jW7Evcb1ocosjIkVSfRaZHhpBSkREpGBqbEVERAqmxlZERKRgamxFREQKpsZWRESkYGpsRURECjbu4Rrr1TUn2sM1bhuwhzHb1FAbe3lrK391etoPdn+b0wdzrFGLZr/EDA1um2fGNmdmAhtohM2LV47EnP53/+30UV3p5PvDDVvM2O/f+YcjiXd8gVf9+E9G0oM/tTfqTGmIN0KMMzzQprC5Jj1A/8i68KizUaDi9MMdsPsu0+/MTSjFcfZXry8283PpxpF1PzjDPsR91xtcCPjOM+1Y91lOxlxZ+4BHx3ito/KEHXp/XHs+Nfe0wGv/K1139f/ZaOZr6vHqlj1ilWdObrbDxsrIunk7/LwdT9qx1nV2rHv2GBueQjqzFRERKZgaWxERkYKpsRURESmYGlsREZGCqbEVEREpmBpbERGRgh3zXX9ubnIeQ0/eYsdWnlGTHGqZz/4X/V6a2PhcO9/mF/gFch5Rx5tGamdmeTZwaSa9y852mTOZ/bUbf2zGVm6zp8Oj99aR5cH9sCPT3ceb4dub8c6ZyP7gMju2a15tH4tB2jLrnKnwABadYMf2Ov2N/ucZdkwK48yrTvJsO/Z4Z+0Uc0tnN7L5pem6tectN/M98my7Kx5A9xltbnwqnXuvHfvAv9qxN92Umyrvbe185tC6HmdGekpOzDkILLFDc3Ldn0ptI+uOs78mABad6MTsWVbZedZ+OzjFdGYrIiJSMDW2IiIiBVNjKyIiUjA1tiIiIgVTYysiIlIwNbYiIiIFO+a7/rjCP5mh1uQjtX9KQuuMXgB6Tz1gb3P2PjsGsMDp/+JNQLE7s9wGnJ1JO0+v3+H0UvnIgRebsa9scLo1bP2zkeVZs+DiF46kKz+383mz/iywQ21OT4ILqZ2daSaNw+t63H5I8Eip2Q6ecZGbV+pLuMmOJZ35/iZNnFjtg/LCQXvH29Q7233NTYkdW+/1UyrAHc4sQz+dYcde2dRak26f20D369J1HR2to2VJPdRux4JzjDvJPn9bvTQ3rVMTrK5225nnHTsA5tihf7h4jLx1Qme2IiIiBVNjKyIiUjA1tiIiIgVTYysiIlIwNbYiIiIFG/Np5CiKVgBfBRYDQ8B1cRxfG0XRR4F3MDK8/lVxHN9QVEFF5OioLotMn/F0/RkEPhDH8R1RFM0Cbo+i6MZq7G/iOL6muOIV5xLuqUl30DO8bv1B+5n/h/qcx+WB/n5nxqBBO++SzEs25dJnOtcfznW+wWc4T+//92kXmrGFnT8cXl42q4lNF42kT+53jsHdH7djld/Ysa12N6SX7azt1zB7dgMv25uu29Xq9HkA9u1qMWPOXFFPZU/JuszDua4/vU3D685rsGf2Wdcy393sOqdu9Z1kx7b6PdJM3hxDC53YvU5Zfv9DtbP3fLoEH6que8Olc0fLAsDrv+McPO5yDkhHMitYaWTd8llOPmBR/UzANGFjNrZxHG8BtlSX90dRtBZwJkUTkXqkuiwyfY5oUIsoilYCzwJuAy4Eroyi6HeBNaS/mHc72UWkTqgui0ytkCTOMCkZURTNBH4O/GUcx9+JomgR6ZhICfBxYEkcx38wSr4rgCsA4jg+b82aNQB0dnaydu3aSXkTEzF7de2QJCtYzgY2AtA/ZF+z6B3wr3ckg851lIp9+aWpMrK8ahY8nBk1qm3o8L8/ZIYTa3O+WvsCKzQystHmuYH+3SMbak322hmHttixhh471mp/LnvacyNIlZZyoLIZgJ3Bexewv2LHB+9+3M07HpO1D69evRr8udMn1WTXZZje+ry6I3d98vjjYf36dLnVHiVq9wxnWCJg5wx7/+l2dr2B3LzrncB4PhnvaVXvrKjJiyW1E8Qvp8RG0oPNvJ6DZr55u52R8g46Ma+e5+/6zO+EneknMzTGad8B500+9Iifdzymoi6Pq7GNoqgJ+D7wwziOPztKfCXw/TiOnUHFAEhCSMtRLpfp6uoa87WL8vLkNTXpv+XTvI8PAbD+oP02Htr+QjMG0L/DuWe7y7lnm9l//+tieNlPR9JnOvvvub127BkDduxk52tf2NA3vLzsNU1s+s7IhiZ8z3aGc8/2VPvHzX+c/7qa9PNnf4xb9qZDbX6t9WR7m8BPd9k3s7Yd/7tu3vGYrH24WgenpLEtoi7D9Nbn5NKX1q74+2vhj96bLp/2MjPft1e/0t3u18+x959fHME92zIwnk9movds84NVZi0d3F6T/nRpDh+q7AHgDb+5y8z3+u/caMa4yxk7c+Ydduyc3JnB75fhH9NPpt97g8DPF9uxS3/bzzseU1GXx+z6E0VRAL4ErM1WziiKst/x5cC9R1dMESmS6rLI9BnPPdsLgbcA90RRdGd13VXAG6MoOof00tM64J2FlFBEJovqssg0Gc/TyLcy+mnxMd0P74bwnZr0x8p/yg1d6br/nZxq5tu43JmCB+hptzuVzJm7woyd0D3yES9shfdkitDpXCo+s9+OnVKxYyTORkPmXk7bLE7OXsduPN/O1xLbsTbneZumW8zQ8TNX1aSbG+Zy/MzXA3Bci3MtD9g2c6wroU8vT9W6HH70g5p0ed8+uqrrklV/Yea7rNfff5qcW5Pnb7dje3K3bxbPgg9XDxsl51pxh3M0XuDcZFiY2A9uLByqnWpsScNMPlRdd9yZ9hvcd6r9bEpHt3P7Zu2Tdmz+xtr0bODl6WKzMysYwKX2ofOYoRGkRERECqbGVkREpGBqbEVERAqmxlZERKRgamxFREQKpsZWRESkYEc0NvLTxV+Eq83YfyVfdvNW5m82Y23z7Y973uDIs++zS4289PTB4fSSvmYz3yJnlCicoRxJnNmLGjKxNuCszJCJjfZMIdjFdMeHPOiMr7Mulz4BWNeSDhnzuTBlIxzKMSp83u6qllzrj553mdP159z8DDYZO3Pde+bNhDdUB2Xrd7r+NDpHY28ynTnOOdM8arvwNFDivOZ0XanZH+7UNHvQji11ZgsazTnpf0+HqqwzWxERkYKpsRURESmYGlsREZGCqbEVEREpmBpbERGRgqmxFRERKdi4Jo+fRFP6YiLHkGOt84PqssjoJjZ5fAGFCECIouj2bHq6/9VTeVSWY6M8k1yWY83T5Xt5SpVHZZmSsoxKl5FFREQKpsZWRESkYNPZ2F43ja89mnoqj8piq6fy1FNZpls9fRb1VBaor/KoLKMrvCxT/YCUiIjI044uI4uIiBRsWmb9iaLopcC1QAn4YhzH9jQ7xZdlHbAfqACDcRyvnuLX/zLwSmBbHMdnVdfNA74FrCSd+CaK43j3NJXlo8A7gO3VP7sqjuMbpqAsK4CvAotJ5y+6Lo7ja6fjs3HK8lGm4bOpJ/VUl6vlWcc01ed6qstOeT6K6vO01OcpP7ONoqgEfB54GXAG8MYois6Y6nLkXBzH8TlT3dBWXQ+8NLfuw8BP4jheBfykmp6usgD8TfXzOWcKG5NB4ANxHHcCFwDvru4n0/HZWGWB6fls6kKd1mWYvvp8PfVTl63ygOrztNTn6biMfD7wSBzHj8Vx3A98E3jVNJSjLsRxfDOwK7f6VcBXqstfAV49jWWZFnEcb4nj+I7q8n5gLbCMafhsnLI83akuZ9RTXXbKMy1Un6ensV0GbMikNzK9B64E+FEURbdHUXTFNJYja1Ecx1sg3TGAhdNcniujKLo7iqIvR1HkzB5fjCiKVgLPAm5jmj+bXFlgmj+baVZvdRnqrz7XW10G1WerLFDgZzMdje1oI2xM5yPRF8ZxfC7ppbB3R1H0gmksSz36AnAycA6wBfjMVL54FEUzgW8D74vjeN9UvvY4yjKtn00dqLe6DKrPY1F9tstS6GczHY3tRmBFJr0c2DwN5QAgjuPN1f+3Ad8lvTQ23bZGUbQEoPr/tukqSBzHW+M4rsRxPAT8P6bw84miqIm0MnwtjuPvVFdPy2czWlmm87OpE3VVl6Eu63Pd1GVQffbKUvRnMx2NbRlYFUXRiVEUNQNvAL43DeUgiqL2KIpmHVoGLgXunY6y5HwPeGt1+a3Av09XQQ5VhKrLmaLPJ4qiAHwJWBvH8WczoSn/bKyyTNdnU0fqpi5D3dbnuqnLoPrslaXoz2ZaBrWIoujlwN+Sdhf4chzHfznlhUjLcRLpr19Iu0F9farLEkXRN4CLgAXAVuAjwL8BMXA8sB54fRzHhT/oYJTlItLLKgnpo/nvPHSPpeCyPA+4BbiH9PF8gKtI761M6WfjlOWNTMNnU0/qpS5XyzKt9bme6rJTnotQfZ6W+qwRpERERAqmEaREREQKpsZWRESkYGpsRURECqbGVkREpGBqbEVERAqmxlZERKRgamxFREQKpsZWRESkYGpsRURECqbGVkREpGBqbEVERAqmxlZERKRgamxFREQKpsZWRESkYGpsRURECqbGVkREpGBqbEVERAqmxlZERKRgamxFREQKpsZWRESkYGpsRURECqbGVkREpGBqbEVERArWeDSZoyh6KXAtUAK+GMfx1WNkSY7m9USewsJ0F+AI67PqssjoRq3LEz6zjaKoBHweeBlwBvDGKIrOGLMUIRBCYM2aNcPL9fCvnsqTL8ux4un+PU30Xz2YSH1+qn8vRZTnWPF0+56moi4fzWXk84FH4jh+LI7jfuCbwKuOYnsiMn1Un0UKdDSN7TJgQya9sbpORI49qs8iBTqae7ajnTMfdh8niqIrgCsA4jimXC4D0NnZObxcD+qpPPVUliMxFWWup8+mnsoyCcasz1Zdhvr6LOqpLFB/5RkP1eXJdzSN7UZgRSa9HNic/6M4jq8Drqsmk66uLiD9Mg8t14N6Kk++LElybDyLMhWfXz1/TxNVJ9/vmPXZqsvw1PxeJku2PHXyXY9JdXlivO/3aBrbMrAqiqITgU3AG4A3HcX2RGT6qD6LFGjC92zjOB4ErgR+CKxNV8X3TVbBnm6SJBn+t3r16pr0sSJb5vw/qW+qz5Mnv+9n6/OxQnV58h1VP9s4jm8AbpiksojINFJ9FimORpASEREpmBpbERGRgqmxFRERKZgaWxERkYKpsRURESnYUT2NLEfmr5zH5v85s/wy4L8y6ZOdbT7TibWPr1hTwusycCwN0C4Cx87gFLA/l54B9FSXdzv5+s1IkqwzYyGsHFepno50ZisiIlIwNbYiIiIFU2MrIiJSMDW2IiIiBVNjKyIiUjA1tiIiIgVT159JdqbTJeAXTr5TM8sXAXdn0vucfLuc2IlO7DT6zFippkvAfGBnJp3vSpDVY4f6e81Q8tjNZiyc9ALn9USK83+cunyTk+/UXHohsK26vNjJ5x+Mdzixx53Y1lz6ecCt6eLgATvb3iE7trvFDCW/sj+ZcMEl9jafBnRmKyIiUjA1tiIiIgVTYysiIlIwNbYiIiIFU2MrIiJSMDW2IiIiBVPXnyM1xmwfG52Y8zA92Yfwu4E7Mukn3dezH9/fyBYz9qTTleBM9gwvz+O57OI3w+lFvU53gd1O15+tdlcjNgyYoeSav6ldsXzF8LrwwT+2tykyhheOUZfXOrFBJ5bvqvdCYE11eYGTbzb3mLFZ/Q+YsY69G8zY3L25er78HNj4SwDCXqe+7nE+m10z7Nj2eWYo+V9fr12x5MThdeHjb7K3+RShM1sREZGCqbEVEREpmBpbERGRgqmxFRERKZgaWxERkYKpsRURESnYUXX9iaJoHek0MBVgMI7j1ZNRqHrWPEZ8rhOb7cRmZpZLuXSbWx67Q1GjEwvYj/bnIzXpEOzCNJTsWGOTHWtyttmc20VDAzR7n4hM1NOtPh83Rtyrrx1ObFYuXcqsa3fytTkdipqH7O5xDUP9ZmwoF2sgGV5XSuxtOocHSCpO0BHyx4cwyrqnrsnoZ3txHMfe/E8icuxQfRYpgC4ji4iIFOxoG9sE+FEURbdHUXTFZBRIRKaN6rNIQUIyxpBlniiKlsZxvDmKooXAjcAfxXF8c+5vrgCuAIjj+Lw1a9KByzo7O1m71hsQbWqNtzxhtX8by7kzOe7YcmqHffTz2fdPmrDvybQ494faMtss0U6F7pFtDjn3ayrOvjToxPqdWG8uPW8O7EqHk1yz4Qk73xSYrH14dbpPOTeup8ZY9dmqy1Bf9Xm8ZZk7Rl1unWAsX1/bYbgGefftGrCHT2wYOmjHKvY921IlV8+bF0H/VgBCxRlA1rstO+jcZx103uFA7omX4zpgezq45ZotjzkvWLypqMtH1dhmRVH0UeBAHMfXOH+WhOoDNuVyma6urkl57ckw3vI0j/F5LXVii53YoszyNcAHjVjeksNGYh2xwhkb+SR2mrFO9g4vz+e57OSXw+nFfd2jZUntybeMGdudhzE2OiPNPpw7IESvgfg7AIT3/A873xSYrH24WgenvbHNGkd9Hq7LUF/1ebxled0YdfkMJ3aKE8sfA7qAcnXZHjkYZmXGIM+b0Ws3BDP2rre3mRsbueH49zK0/loASvvsBjwzPPrhdjmPee1wHjvbekJt+ooXw3U/BiD8xf/nvGDxpqIuT/gychRF7VEUzTq0DFwK3DvR7YnI9FF9FinW0TyNvAj4bhRFh7bz9TiOfzAppZpuzi9e+4JNaq8T805d+gZGzuD6S4H1mUuy+yuPmvn2VOwZP3ZWtpqxHYO77Fj//uHlF80/h1t23jWcPqPH/jV85l7n09nhXJfa6lzO2pr7PTjYB1vTzyN53Z/a+YDwr59y41LjKVmfFzl1eayDn3MtBrv2QH5+rwECm6t9aQ7sucXM1brTPrNt3G3P+tO4zz4GtOyrvYJ11nFv4977bwWgfZ99JWrBXvtotWS3c2a7c6Ed25G70tb3HHjkVwAkv/1FOx8Q/uPtbvxYMOHGNo7jx4CzJ7EsIjJNVJ9FiqWuPyIiIgVTYysiIlIwNbYiIiIFU2MrIiJSMDW2IiIiBZuMiQiOTQdyXQJaM+u2O/n2OzEOH/CoJqs3uErlnuHlwaWr2L754eH0gaEHzXy7h+zH/rcn2+xYxe61vq1/ZOCKCzoO8qutI90td/Ta73D3fntwiufssrtglHY5v/l25cbjGeyDXevS5Z3+7D/J2faIg+Gu69y8cgwZrXtPdd1yJ5s3WAz4s/54s38dXF/bhWdo0ekc3Jp23enZUB4tCwCV7Wvs2B6768+A0/Wnsr+2k9LKZ3dzy323AVByjmXt9lg5LNpjH8jO2mN/qqfszs1v0dcNj1Xfc0+f/YJAcuHfmrHwi/e5eeuFzmxFREQKpsZWRESkYGpsRURECqbGVkREpGBqbEVERAqmxlZERKRgamxFREQK9vTtZ5ufC69pZF2LM0/e8gP+Zmc5P19aZ9ixxuaRnnvNSWDl0Eg6SeyNloI9PV1TsKe1qzj5ehnptzgUoLd5JL3P+X22s2LvTtv77HyLZzWZMWbn+tKWSjC7I12eM8vOBzDg9ZSUp4rTcunWzLrfcfKdN8Z2J9rPtr9pcU26PTRxQXXdDux9dutAyYzt6LXr8v5eu49qftbLgSHYUl3X48whmDgzYs7G7k+/o2RvtLm59pizKMDW6rrjxzrvmzPHjx8DdGYrIiJSMDW2IiIiBVNjKyIiUjA1tiIiIgVTYysiIlIwNbYiIiIFe2p3/bnJntaN+3Pp80fWte20szX1+C/Z0GBnTmY9YcYGZz8+8nfHLWZw3UgBB1seHi0LAD0NdmwwbDJjpWBPsfdEcnBk+5WDlPeMTP+3btDuE7DhYDBj27rt7j0Xd9t9ok7pznWVGOqH7o3pcvc8Mx8A++zuTclZHzVj4V47JtNj1mjT6FWtzqVnZNY919lmvstQXpMzX+bgRrv+HNhcWycbvqBclwAAIABJREFUZ3aw4OF0Xd+6zWa+xx9/xIw9lpnmMu8huyg83l2b/oOD8IM70+XNzrFs70E7Fpx8y7vtaT1v6qkt6Cf6u/nzx9IpB/+H8/0CvKDSbsaS3/2aGQtffbO73amkM1sREZGCqbEVEREpmBpbERGRgqmxFRERKZgaWxERkYKpsRURESnYmF1/oij6MvBKYFscx2dV180DvgWsBNYBURzHu4sr5gQ5j8QvyD2+3jgAC7amy6t32fmOO+hMlQE0l+wpg0p93WYsycyk0VZJeMbekXT/HPtr2tvaasZ2lux83SW7m85gGIklAQZbRtJ7muyZSTY7rzcraTNjS/vsmVBO2Zvr3tPYCAuq63YvMvOlltmhvqVj5H1qOlbr8+uc2O/m0scBb68un380L+p183O6uXX0d9SuSBpYWl23yNknQ+l4M7a98oAZe7h/nxnbmZsQaHBoZN1e71Bm95ojsd86G5zpkL6Rmy3o/Q0J35iRrts9Rks0+zj7+HH28cv9zHViPGe21wMvza37MPCTOI5XAT+ppkWk/l2P6rPIlBuzsY3j+GYgf673KuAr1eWvAK+e5HKJSAFUn0Wmx0RHkFoUx/EWgDiOt0RRtND6wyiKrgCuqP4t5XI6YkhnZ+fwcmFOtkONucskp86GG6u/92fYcyPTlNiXUQGCe5noODOWNJ45vLxkVgdXXfySkVipf7QsAFQa7ImjB4Odb8iZADp7DenE1pP459O+Ppz2fp01OteXWit2zpmDzlb7crvogmXw9k+ky73OpPMAg048sWPl/ePbL6dkH54a46rPVl2G4j+LBU5sfi49g7Enhh+XDifWao9oxAm5sanaW+H8dF1D/wozW2f/i8zYsoGrzNhrnVHdunOhU4/v5Id/l35Pg96gTc5lZLx8RxDrPLmT8rdvA6CDmU5GWN7ifBkz7NtQ5VfVT10ufLjGOI6vA66rJpOuri4AyuUyh5YL8x37m8/fs73xpfCSH6TL/j1be8cGaC45QyTOsmPJgpFh3K66+CV88qc3Dqf756w38+1tfdyM7Wy0h4fsLjnDSoaRsdr++bSv8zsPvmk43TZkf6aL+u3d6bT99j2X522zK8srHs/ds337J+CLf54uPzzGPdttE7tn2/Xz8Q3xNln7cDLGUHX1wqrLUHx9/n3nM8rfsz0PuL26fNHRvKh9KxQ22c9fsPXR2vT5p8GvHwRgaPMaM9va9TeasR9v+JEZu3WnXdDygdr0D/+uzG+9J/2etjnDUWL/Tgf7972fr7/2R3X527fR9dpnA/DSxgucjHD1ykvN2NnnXmzGuj7xQne7w2WZgro80aeRt0ZRtASg+r89IKaI1DvVZ5GCTbSx/R7w1uryW4F/n5ziiMg0UH0WKdh4uv58g/RqzIIoijYCHwGuBuIoit4GrAdeX2QhXR91LsHdY4cW5HroNL4AFtyRLs/baV+and37qBkDKDVusYOznzRDg4u2Di83XPBcmu8duX+QLLbzNc+3L01XZtiXn7c2bzVjuxpHLpH1Dh3kgf13D6f7ncf+2yp2N6T7B+xLxesH7dl79g/WXuu6NBngR4PpJfc3DIzxW9F5TfrMxwxILv65GQs/Hd9lqXpVz/X5cucS3LOcfPlZf9pGWTeqh+06AMBjzgn+Bifv1lydPPN4uK16MNpu39ppedK+f9W0wb5UvMs55GzLHecGe2DboUOLdznYu1vmxdx7vblgL/BIuu4HM/z7pfe3LzZjf3lwlf2SN9nfU7hkrK6Dk2vMxjaO4zcaIftuvojUJdVnkemhEaREREQKpsZWRESkYGpsRURECqbGVkREpGBqbEVERApW+AhSk+LFTveeO+1QyXm0ffnOu2rSzW9fxfJbHwZgxc5fmPk6etfZGwWS5u1mrDLP7krQv3QkX+mNb2POmpuH0wdX7rBfb4X9aHvHQnvaksTpFXMw04NnaAgOZgfLcUZAPIA9JM2DXqzBfn8bckNOnkM//xDSLk37E7urEcBrE7u7wLyK04epd44ZSl6XKc/cUk06/Ksz5YkA8Cqne8/pTr6zndhoA/0NryvfZGe8e6OzVeBhZ9qfJ+zZvdiS259fdwnceC8A3Ts2mNkqO+w+PLOcXkitRzIA2SCHj4w9nQ71Bjrgz6a2/snfmLG39NgdvW51quS/ZPbFk3Lp1wfn+DBBOrMVEREpmBpbERGRgqmxFRERKZgaWxERkYKpsRURESmYGlsREZGCHRtdf5yn7Ofvt2MX9jxsxp6/++6adMfAcn5rc7ruGXseMfO19TpTbAD9LXZh+wd2m7He0sisHjMGK5y7dSR9sM3ugrC/zX5kfoU9XztznNjPMz/BmoAlmfQWr3eLtzd5vXQa5puhW3pru+8caG7klqXpuqHe452NwroOezaQi3qfYcYu5JlmbEa2V1ATsFi/V4/E+U7sXCfmTuu9+YHa9IITYEd1dp0tDxz+94c86UwAD7DF6ee2xdnZt+T61Q2UhtcN7lxgZmvau8KMLUzs486z2GPG8hOfNQKHSmB3uJsGY1Wjk50/eMZcM/R/u+zZvR7LLD8TuG6MIhwtHSlEREQKpsZWRESkYGpsRURECqbGVkREpGBqbEVERAqmxlZERKRgddT15z47VC6boVU8ZMae6UwJdC6125zRfz7nbvxSNd9do2UBoOQ8Zg+wf9COHXB6GvRkJiBp7u7l+F8/OJzufmyUDIe2eYYda91nx3r77djjmR4IzRU4PtO9yusN4Xbv8fa04HRC6MilG/qgI+3S9YsZ9mP9AL8YsKd9+mRLuxl7vrPN8zJdpt7fAJ/NpD/4eXv6lWvePfmziNSrP+teZ8aOHzxoxk7Y/rgZa3vkV/YL3vmz2nT0Mfj2J9Pl2+zZYrjD6f8GsHaJGRoYda6h1FZq94OFvb1seyjtgrQDu/vfTp4wY/ud445XJZeN8reH1tVV15+hMeJPPmrHeu3jNU5XxRszy/ty6XZndqruCc4IpDNbERGRgqmxFRERKZgaWxERkYKpsRURESmYGlsREZGCqbEVEREp2Jhdf6Io+jLwSmBbHMdnVdd9FHgHsL36Z1fFcXzD0RSkg5+asfOGX+Zwz8HuLnA+D5qxU6mdDaSVHk6tdhVa4E0zdBS8B8YbcsvZjgWNzjP6rZud2Gw71jTLjiWZF59VgYsyvRV67B4z3DPDjrn9E7zYotpZf2hqGlk3sNzJCPSeaMcq9oxAtzibvKVlZPnNrfC3p2eC9uRFdWMq6vMzt/w/M3ZaX8mMrdzqzKj1qDN7zyO5WN/74ZEfpMvr7GMH60+wY0CC3fVnC/PM2HZqK8lcmtlKOkPVHuxZf/bTYsYS7L56c5xjYL5nYFtmndOZhgNObFqc6cTO9qYim5gx5oOakPH0s70e+Bzw1dz6v4nj+JpJL5GIFOl6VJ9FptyYl5HjOL4Z2DUFZRGRgqk+i0yPoxlB6sooin4XWAN8II5je2gUEal3qs8iBZpoY/sF4ONAUv3/M8AfjPaHURRdAVwBEMcx5erQi52dncPLAKXqPY3RzMAeA3Em9pB87fQ626yNNXaexsLyofvGFTPfWJxboTi3O2tesaWzkxMyn409cBgM2bd5GHKGTxxwRqp7YWabC+d0cuXlI2V5i3N75KB9O27ij+KtrC1oZ/tJlFd/M0080/u0gSQ/1mM2NsHyZN5H52woX5aJebXJGXK0DoyrPlt1GWrr80nL7Hudrc7n3rJ8wA6eZddlLs3FFp0KH/hRutzjjJ/aM8a9viH7IYRFzjiA83M7e2vnQk4vXwlAxRmXsOLcl61gD3M54BwD87mWdXbyier39EEz19ijJ06GfBvgmuvEjvOHbR1XWYBx19AJ1uWQOGNAHhJF0Urg+4ceqBhvbBRJqI4rWS6X6erqGg508Dkz00QfkLrAeUDqWbkHpBaWf8q2rosBWH4UD0h5pwPeQwfZG/InlMs8kflsnMMMfac4r9dpx5483Y7dk9nmlZeX+dx3R8pyg/NM0j1ehfB+aXh21O5W5dXfpGvNG9LExhf6eXt/y45Vfnti5cn8EClfBl3fy8S8B6RebT8eV62DUzZ48iTV5+G6DLX1+VuP/JmZyXtA6lTnAak27wGpe3KxD/wIPnNpuny784DUnWM8INV9thnb4Jwc5B+QOr18JQ90pce3PfSY+faz3ozt424zttk5BuZzfaJc5s+r39N/mLmm5gGpfBvgeq0Te9d77Ngl146vLMA4SwLO2MheXZ7Q+UYURdmfrpcD905kOyIy/VSfRYo3nq4/3wAuAhZEUbQR+AhwURRF55BedloHvPNoC/IMfmHGOp0z25VsMGMLWWfG2nKXXhqo0DaOM1rvLBNwfrf6vxazl3uGcttxL2rvtEOtT9qx+c4V1lWZq7MtA7Bq60j6+c4Z6k6n689mb4KVVqfvz9zc6XJjaWRdMkZfm+Q4O+ZcXXSvoWV/njYBSzNpeyKYujEV9fmkLZ8xY0sP2Iects3/f3v3HmR3Wd9x/H32fs/9Tm7EEJagXEwYNFQDIuUiBTrlqaCW3iC1orUyOApTZeww2lqgODpSCo5QRX1ACgz1QuUeOoaFVAkYQpkQgdxIyCbZZJPd7O7pH+dkz4X9fndJ9re/E/i8ZhjO8/vu8ztPfnue8+zv8n0ep4c4K1+9pZv39hVSfl4v/+Eie51lsYCNzqXijdiXLreXxRZQzyZyaWh7sS+V9zqXf3qdD2wdXWZsRtnaPrUwmND0frMWPO7EEjFvmPgxXsy5/VBBhh1sY4yXDLH59gTaIiIJU38WSYdmkBIREUmYBlsREZGEabAVERFJmAZbERGRhGmwFRERSdjhTNc4qqawxoy1OlNFNLDRjHmzBJTPuTJQtM1LCBguOegNJ+btt/jB/n5KM3q8iZlq7af+qXEa2+ykDE0rSvWp7SstH+tM2nS6kxZ0l5MWlG10Un9aytIvqjKFbbXDrfbh5Bt52QL2hDylsaqyt7An+XlXObrfTpBr2elU9DqPk8b2ltiBom3OnBad7ocANmBPwfaqs+rPTkpTznqp4bX8tj4nr8w786lyvj1qnQM3oSz1p5rCZEwLnPfzfhVrnZhrqC/kg9vsOUJyvPlHjnK+zCqIzmxFREQSpsFWREQkYRpsRUREEqbBVkREJGEabEVERBKmwVZERCRhFZP6c5+zqtc/zf2gGavZbeei7Nu93Yzt6C/NQZhAhh35w7HbWWGj00kHgLeuZVnSHudwVzUW1vbpq6phZ+PkwXLTJDsXpXqKnW9SO83+dzTZ63vTPK5oH9Uws6jc6OQh1ffZKTx1+2easbubF5qxPU1la5NkmqFmSX6n9nqjOcfZIS9Nx0v9KU616i8rnz9mS9JWtEnL7VjWS/15wYmNd2LlaWXNFBYndbK/tqw7y9kpbNlir3m8g1PNWA+lnWuAavaR21etk/rT6CTctLDIjDU4eTMDZW1pZDyL+SMAZvI/Zr0p2N+dzoJhrHLW1+bksvIE4OL86zOcegDBCzoLdx8qZ83aQ6UzWxERkYRpsBUREUmYBlsREZGEabAVERFJmAZbERGRhGmwFRERSVjFpP546mevN2Pdu+08jW2de8xY167StJijqrNsaMttO7DXbsvWfv+Q7cBeiWagwa7b1FRI/emtyvBaU2E/E+qyZr1sdb8Za8COtfbZ+6zvLbyuGYCJReUWe0EX6vfY+xxo7DVju2u7zdhPM2WrnQz0w778tgZnySOAXvs92eGsNLTJ2WfxCjOnABv8JkgZZ2UoN+Z0u86yj3lrFrry217utZeberbHXrkH4EVazNhO7M9PVdnyNvvJ8EJ+27ihKuRNc9Ypq3EOQCv1Tqz0oNZQxeT8tonYaZOtTuqPtwrZHif4QvmvoopC2pazmhjgpnAdIcOYzmxFRESSpsFWREQkYRpsRUREEqbBVkREJGEabEVERBKmwVZERCRhwz4zHUKYDdwJTAcGgFtjjDeHECYCPwHmkUuACDHGziQaOXPuFjPWbWeN0L3DjnWWtfRAHWyel3u9zc4Y4pk+Lz8BNmanmrGqKnvFoMkNhdVAzq+p49HJswbLc5rt9KYFLfYB6G+wc5jqau0cntZMIWUmk6miLlP4N48fsNN7xh2wUxDq99qx03q2mbEZ3a+UlKce1cOVm3Pbvt1wrFkPgC7nF7nF+T1ucPa5sej1x4Df+k2oNKn3Z+8bx1lQ64CdUcdLZacMi4u2rcza/ePujJ3aA/AcR5mx/djLZpUvUPRZ4L78P3yucwAWOe/X66QFNTopfpPLVhmqool6cqtmTXSSeGY4uTbjM78zY+fbTeHr5fuB/PpD8MBwp332AmbAi8NUrgwjObPtA66KMbYDpwKfCSEcB3wJeDjGuBB4OF8Wkcqm/iySgmEH2xjj5hjj6vzrLmAtMAu4ALgj/2N3ABcm1UgRGR3qzyLpeFv3bEMI84CTgFXAtBjjZsh1YMC+fioiFUf9WWTsjHieqxBCC/BT4PMxxt0hhJHWuwK4AiDGSEdHBwDt7e2Dr4dztDOr2sCAHXNuZZAtuwcw/uh2Lrwr154+Z58h60zzBxxwD6n9t01NUWj2gnnceM8dg+W6KrtB9dV2rMGJ1dXYsZqawn3ZmqmLmPy5JwbLVc50bM019n2llmq7YlXGrnd1dem93mnNc7h66c0AXFblTX4H9DsfnANOPWeWx+JY+wzouLYotmJkn+dKcCj92erL8Pb6s2muHaqeZMcWf7i03Di9ncVfzLVlrvNMx7k909zm7Ouz39T72in/pC9qh4fzh8Z+csGPNTLFjDU4cx3Wc3zpz7ZPYnHHpfl2ehcv7AM3Hft5jyrnH3FD2S3yo2a2c8NXcgfmH4bpyv50jf60myPRDpR8eg/3szyETDZrP/ByUAihFngQ+GWM8cb8tnXA8hjj5hDCDOCxGOOiYXaVzeS/WDs6Oli6dOmIGhk/Yce8B6QyzgNSPWWPflx4Vwf3XZprj/+AlN9BN2anm7GRPiB14z138IU/uWyw7D4g1WYfgAXj7Qek5ky0O8y0SYURZfLnnmD7tz40WG6aZH9eeibaPe3VcXZvaqyx/4D5t3ELSspXL72Zb3b8HQDfbjjHrAdA15/asS3O73GDs8+iB6Q6roWl1xfFbrP/aPDk++ChVT4Eo9SfB/syjLw/u1836+zQgVV2bHVZbPEXO3jhn3NtWbnarnf3S593GgPP7fgLM7af95mx8gekHu6Aj+QPjfP3BN7BPq7kybxSx7LWjM3juZLy4o5LeWHpXQBM5H/NegPYB+73zgNSrQvMEF8/rbR8w1c6uOpruQPzwB/a9YDcw4iWFmeA4AfD7DinAyj59DonAB6vLw97GTmEkAFuB9Ye7Jh5DwAHR4TLgPsPqXUiMmbUn0XSMZLLyMuATwFrQgi/yW+7BvgGEEMIfwW8ClycTBMh/NCOffdb9l8gXTvtM63tu0uvafS01fDyWZMBeHH/TLPeQ85ftAD7OcYODky2Y1WF3Iavtk3mwY+uGCy31NrPvR/XYJ/ZLm6yz2yPcWKzGwtnvWfXTeHR+ZcPlmfU2mfZZ2x/yoxNe/k5M8ZE+8z21ONPKSk3D9Rw6v7c5b1H+trsfQK/2znBDr7pVLQXPCk9e13RAbeN7OpMBUm1P3snDD3OWe9q5/bFqrJvsXkt8OtludcPtdqXGFfNd/ojwBY7vQc7W43Oss9Wfy105nfVucuu94pzlW4bs8xYpxN7o25ZSXlepp6n6z4NwOxxdt7aBV03m7Fj+u0zW6cpfPTM0nJbW2HbUx9+688Xe9PN0nrZiT1ohzLnF153dMAIr7QeqmEH2xjjSuxLXB8Z3eaISJLUn0XSoRmkREREEqbBVkREJGEabEVERBKmwVZERCRhGmxFREQSpsFWREQkYSOerrFSPdhuJ+dt77LzU1/rLp3/69KmKn5wYm7bpn4nWYw5foOy77FjfbPt2IGmwuuGJlh80mBxT9aeHO7pjD0T1NPsM2Mrttu5cl9aVTTrylmX84nHVhbK658x6zmzuIGX0rjYnjvxiZmlyYen9Q/wxI7ctk2ZLmenwFZnKrD1zhRvR8aKXe84Nzix55wc3KfL0rTPyMC38tvWj7eXraN7mGn+BpzkTm/W1qaych1wsOs70xLucvJsH3GWmJvq5Pze1Lu+dEP2aK48uG3bLXZFfuzEHM4he7JsxslzagrbOsun3XpbNjmx5w9nx6NKZ7YiIiIJ02ArIiKSMA22IiIiCdNgKyIikjANtiIiIgnTYCsiIpKwIz71578+ascm7F5mxjoPHFtSPtA2jk3nnJsrDLzX3mn2BL9BA+127MAkO1acNtMGnHlyodxZ/sMFx7/8ihm75YWHzNiy3/3K3ukrawqve/bBS0Xl3XY1Wg8x1nasGVrXuqSkvL+qeXDbzoETnZ0CdU5qh/dn5iNjto67FLnGOexOhh/rG5tLyj0tVaz/QH7brFPtiluO9xv0ZqMd2+nUK19Grw04+D3lLO04y0k5u3ONHTvDzpzjrakvM4q2ef8Ih7c21B/bofVnlZZ7irbZyY0jMcUOZb58WHseTTqzFRERSZgGWxERkYRpsBUREUmYBlsREZGEabAVERFJmAZbERGRhB3xqT+ezrbHzdgJ2Q+UlBup5oSJEwD4Ld4SFM6yHSOKG3qLXtdQuriQk8Hy/NT5ZuyqhSvM2M/nzDRjE35btERP80T44CWF8qtP2o3ped2ONTnHpXqhGXpv2UpJjdm6wW2v9dn/BoD1PU7w20rvOZKsdX5dJ28pXWanKVPFybW5baubnOV52oY51+h3YtVOrHzVn1pyGTcAU+1qG4+zYyvOtWNPvWTHpm4o+35oq4cz89t6nFXIpk+zY6dstWOn2aElZeXmom3b7WoAbPCCmdXD1K4MOrMVERFJmAZbERGRhGmwFRERSZgGWxERkYRpsBUREUnYsE8jhxBmA3cC08nNF31rjPHmEMJ1wOXAtvyPXhNj/FlSDRWRw6O+LJKekaT+9AFXxRhXhxBagWdDCP+dj90UY/yX5JqXnKvLHjafTt/gttVsNOs9jrNyD7DGWd6mt9d+nH5cd93g6+qWKsZ1F9bBmN/dO1QVANr3dpmxRX32Ej33z7cf+58y5W8GXy9rm8JTZxbK571+ulmPtb+0Y52/t2Pb7WN26qulF1+aewvbftPrX5hZ72QivUu9I/vy3z9Z2j+mnZEd3Hb/1j1mvXt2ucvlgF01t2SNpXwJmx7g5fxrJxPphPKUoSKntNmxHzkLG80555SS8vIp8Ninc9suotuu2P2qHZvwgBOzQ+eVlccVbXPeDYBNTsz+dqwsww62McbNwOb8664QwlpgVtINE5HRpb4skp63NalFCGEecBKwClgGXBlC+DPgGXJ/MTsrr4pIpVBfFhlbmWzWWZW5SAihBXgcuD7GeG8IYRq5iT+ywD8CM2KMfzlEvSuAKwBijO9/5plnAGhvb2ft2rWj8o84FPOXTC4pj2MGu3J/9NNNg1mvC2dBaWCfUzc7YF9Dqh4oTJGzqBrWFc1eUz9g/44a+u1pbrxY3YC9XHNNfyHW0trInq59g+XxvfuHqpKz31lZvt+52FNdZ4berC+deWrcpHHsejO3Ove2bIu9T2DPgXo7+MYzbt2RGK3P8JIlSwDGbEqr0e7LkG5/nn906XRO49oWsWv3OgB29tmzwXX2TzZjAPTbtzcY2dcmAO3TYO3BSZec33Kjc1ek2Yk1OrNZ1ZWdTrW2Qlf+ztME7FtQDDizRFU7i84326GdZV9/LbSzh9xnZtsQP1/M+WYhe/hdeUz68ogG2xBCLfAg8MsY441DxOcBD8YYjx9mV9lMJteOjo4Oli5dOux7J+UH2b8uKZ/NtfyC6wFYjT194OO8z93vGo41YyO9Z/toSxWn7ym6Z7vHuWe727lnu9v+iM7tsm9ITdlTuCG1bPlinnrshcHyea+vM+sd8j3b8XPN0A8Xls5Td/alZ/OLu34BwC29H7T3Cax83Z7Kku8c/tg2Wp/hfB8ck8E2ib4M6fbn/7i79A+yc854lJ8/knu24P6tF5r17tn15/6O9yy3Y2/jnm3HF2DpwSOdwD3bxc5MqHPK7qEuPx0eezT3+iIesyt232THvHu2p9ihB6eUlk+jg5XkPjPftasB8Csn1jsKPWcs+vKwqT8hhAxwO7C2uHOGEGYU/dhFwPOH10wRSZL6skh6RnLPdhnwKWBNCOE3+W3XAJeEEE4kd0FlA2DPei8ilUB9WSQlI3kaeSVDnxYf0Xl4n8zcVlLu6FjBJ5fmtu3N/sis92umu/td76z601dn35scX1c4xHOBW8YXyjMb7furcxvs2wCznMtSNc3OtZe9Ra9r4bzi8562BXa9+Z81Q9lu+6OW2WbfselpKb2ama1ppWfShwDY3uusWgLwZa3sU+yd2pc/dfGuknJHR//gthe/aX9eFx842t2vdyuw1+7KTCuLTayCT+YvAy92+uTJzmJjJzsZh5OdlYSYsbm03DiJi977Zu71LOc5iiZn+R6cZyG424yUP/ZeV7RtuJuZo3GpOG2aQUpERCRhGmxFREQSpsFWREQkYRpsRUREEqbBVkREJGEabEVERBL2tuZGfrdozlxixrJZb/0JOMNJ/Xk7f9t8vDhDo96ZA82LTZ5ix7r32bG9RVMy1jfAgvcUxewpINln//v+b7+d87Byrh27t+zt/qAB7l2YS/l5cdk7IB9AEnXs1V8xY9mvXevWXWfPvMoOZyaoqrIuObUGrsyn7kxwvh5mOrNHtngZh0c5sfI8JKpg4cFtfU5Fb8kjO/YKM83YPWVr9/wtcE/+9c/fBV1ZZ7YiIiIJ02ArIiKSMA22IiIiCdNgKyIikjANtiIiIgnTYCsiIpKwES0eP4rG9M1EjiBHWvKD+rLI0A5t8fgEGpEBMiGEZ4vLaf9XSe1RW46M9oxyW44075bmY5txAAADgUlEQVTfyzuqPWrLmLRlSLqMLCIikjANtiIiIglLc7C9NcX3HkoltUdtsVVSeyqpLWmrpGNRSW2BymqP2jK0xNsy1g9IiYiIvOvoMrKIiEjCUln1J4RwNnAzUA3cFmP8RhrtyLdlA9AF9AN9McYlY/z+3wM+BrwRYzw+v20i8BNgHrABCDHGzpTach1wObAt/2PXxBh/NgZtmQ3cCUwHBoBbY4w3p3FsnLZcRwrHppJUUl/Ot2cDKfXnSurLTnuuQ/05lf485me2IYRq4DvAOcBxwCUhhOPGuh1lTo8xnjjWA23e94Gzy7Z9CXg4xrgQeDhfTqstADflj8+JYziY9AFXxRjbgVOBz+Q/J2kcG6stkM6xqQgV2pchvf78fSqnL1vtAfXnVPpzGpeRTwFejjGujzH2Aj8GLkihHRUhxvgEsKNs8wXAHfnXdwAXptiWVMQYN8cYV+dfdwFrgVmkcGyctrzbqS8XqaS+7LQnFerP6Qy2s4DXisqvk+4XVxZ4KITwbAjhihTbUWxajHEz5D4YwNSU23NlCOG5EML3QggTxvrNQwjzgJOAVaR8bMraAikfm5RVWl+GyuvPldaXQf3ZagskeGzSGGyHmmEjzUeil8UYTyZ3KewzIYQPpdiWSvRdYAFwIrAZuGEs3zyE0AL8FPh8jHH3WL73CNqS6rGpAJXWl0H9eTjqz3ZbEj02aQy2rwOzi8pHAZtSaAcAMcZN+f+/AfwnuUtjadsaQpgBkP//G2k1JMa4NcbYH2McAP6dMTw+IYRacp3hhzHGe/ObUzk2Q7UlzWNTISqqL0NF9ueK6cug/uy1Jeljk8Zg2wEsDCHMDyHUAR8HHkihHYQQmkMIrQdfA2cBz6fRljIPAJflX18G3J9WQw52hLyLGKPjE0LIALcDa2OMNxaFxvzYWG1J69hUkIrpy1Cx/bli+jKoP3ttSfrYpDKpRQjhXOBfyaULfC/GeP2YNyLXjqPJ/fULuTSou8a6LSGEHwHLgcnAVuCrwH1ABOYArwIXxxgTf9DBaMtycpdVsuQezV9x8B5Lwm05DXgSWEPu8XyAa8jdWxnTY+O05RJSODaVpFL6cr4tqfbnSurLTnuWo/6cSn/WDFIiIiIJ0wxSIiIiCdNgKyIikjANtiIiIgnTYCsiIpIwDbYiIiIJ02ArIiKSMA22IiIiCdNgKyIikrD/B4sXw94D+tbfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x576 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Two subplots, the axes array is 1-d\n",
    "f, axarr = plt.subplots(2,2, figsize = (8,8))\n",
    "axarr[0][0].set_title('Known Skill')\n",
    "axarr[0][0].imshow(X[0])\n",
    "axarr[1][0].imshow(X[1])\n",
    "\n",
    "axarr[0][1].set_title('Unknown Skill')\n",
    "axarr[0][1].imshow(X[1550])\n",
    "axarr[1][1].imshow(X[1551])\n",
    "\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.01,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (7880, 28, 28, 3)\n",
      "7880 train samples\n",
      "80 test samples\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "Y_test = keras.utils.to_categorical(Y_test, num_classes)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "input_shape = (img_rows, img_cols, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 995, 28, 28, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ty.shape\n",
    "tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11343 samples, validate on 597 samples\n",
      "Epoch 1/50\n",
      "11343/11343 [==============================] - 9s 819us/step - loss: 0.6695 - accuracy: 0.5941 - val_loss: 0.6517 - val_accuracy: 0.6415\n",
      "Epoch 2/50\n",
      "11343/11343 [==============================] - 9s 774us/step - loss: 0.6462 - accuracy: 0.6364 - val_loss: 0.6365 - val_accuracy: 0.6566\n",
      "Epoch 3/50\n",
      "11343/11343 [==============================] - 9s 777us/step - loss: 0.6351 - accuracy: 0.6497 - val_loss: 0.6265 - val_accuracy: 0.6499\n",
      "Epoch 4/50\n",
      "11343/11343 [==============================] - 9s 775us/step - loss: 0.6278 - accuracy: 0.6574 - val_loss: 0.6254 - val_accuracy: 0.6583\n",
      "Epoch 5/50\n",
      "11343/11343 [==============================] - 9s 787us/step - loss: 0.6227 - accuracy: 0.6606 - val_loss: 0.6163 - val_accuracy: 0.6683\n",
      "Epoch 6/50\n",
      "11343/11343 [==============================] - 9s 777us/step - loss: 0.6184 - accuracy: 0.6654 - val_loss: 0.6076 - val_accuracy: 0.6784\n",
      "Epoch 7/50\n",
      "11343/11343 [==============================] - 9s 776us/step - loss: 0.6132 - accuracy: 0.6699 - val_loss: 0.6043 - val_accuracy: 0.6817\n",
      "Epoch 8/50\n",
      "11343/11343 [==============================] - 9s 777us/step - loss: 0.6105 - accuracy: 0.6705 - val_loss: 0.6214 - val_accuracy: 0.6549\n",
      "Epoch 9/50\n",
      "11343/11343 [==============================] - 9s 784us/step - loss: 0.6074 - accuracy: 0.6780 - val_loss: 0.5936 - val_accuracy: 0.7152\n",
      "Epoch 10/50\n",
      "11343/11343 [==============================] - 9s 775us/step - loss: 0.6042 - accuracy: 0.6787 - val_loss: 0.5917 - val_accuracy: 0.7018\n",
      "Epoch 11/50\n",
      "11343/11343 [==============================] - 9s 781us/step - loss: 0.6014 - accuracy: 0.6774 - val_loss: 0.5977 - val_accuracy: 0.6817\n",
      "Epoch 12/50\n",
      "11343/11343 [==============================] - 9s 779us/step - loss: 0.5988 - accuracy: 0.6868 - val_loss: 0.5848 - val_accuracy: 0.7169\n",
      "Epoch 13/50\n",
      "11343/11343 [==============================] - 9s 778us/step - loss: 0.5962 - accuracy: 0.6837 - val_loss: 0.5851 - val_accuracy: 0.7002\n",
      "Epoch 14/50\n",
      "11343/11343 [==============================] - 10s 848us/step - loss: 0.5932 - accuracy: 0.6904 - val_loss: 0.5789 - val_accuracy: 0.7286\n",
      "Epoch 15/50\n",
      "11343/11343 [==============================] - 9s 828us/step - loss: 0.5908 - accuracy: 0.6916 - val_loss: 0.5748 - val_accuracy: 0.7270\n",
      "Epoch 16/50\n",
      "11343/11343 [==============================] - 10s 843us/step - loss: 0.5885 - accuracy: 0.6941 - val_loss: 0.5836 - val_accuracy: 0.6935\n",
      "Epoch 17/50\n",
      "11343/11343 [==============================] - 10s 860us/step - loss: 0.5862 - accuracy: 0.6956 - val_loss: 0.5707 - val_accuracy: 0.7136\n",
      "Epoch 18/50\n",
      "11343/11343 [==============================] - 9s 825us/step - loss: 0.5840 - accuracy: 0.6977 - val_loss: 0.5686 - val_accuracy: 0.7337\n",
      "Epoch 19/50\n",
      "11343/11343 [==============================] - 9s 797us/step - loss: 0.5820 - accuracy: 0.6991 - val_loss: 0.5715 - val_accuracy: 0.7119\n",
      "Epoch 20/50\n",
      "11343/11343 [==============================] - 10s 901us/step - loss: 0.5796 - accuracy: 0.6993 - val_loss: 0.5643 - val_accuracy: 0.7320\n",
      "Epoch 21/50\n",
      "11343/11343 [==============================] - 9s 793us/step - loss: 0.5775 - accuracy: 0.7013 - val_loss: 0.5768 - val_accuracy: 0.7035\n",
      "Epoch 22/50\n",
      "11343/11343 [==============================] - 9s 778us/step - loss: 0.5758 - accuracy: 0.7040 - val_loss: 0.5594 - val_accuracy: 0.7370\n",
      "Epoch 23/50\n",
      "11343/11343 [==============================] - 9s 777us/step - loss: 0.5738 - accuracy: 0.7058 - val_loss: 0.5586 - val_accuracy: 0.7219\n",
      "Epoch 24/50\n",
      "11343/11343 [==============================] - 9s 776us/step - loss: 0.5724 - accuracy: 0.7058 - val_loss: 0.5565 - val_accuracy: 0.7236\n",
      "Epoch 25/50\n",
      "11343/11343 [==============================] - 9s 778us/step - loss: 0.5696 - accuracy: 0.7090 - val_loss: 0.5537 - val_accuracy: 0.7303\n",
      "Epoch 26/50\n",
      "11343/11343 [==============================] - 9s 775us/step - loss: 0.5684 - accuracy: 0.7103 - val_loss: 0.5538 - val_accuracy: 0.7353\n",
      "Epoch 27/50\n",
      "11343/11343 [==============================] - 9s 782us/step - loss: 0.5668 - accuracy: 0.7098 - val_loss: 0.5517 - val_accuracy: 0.7286\n",
      "Epoch 28/50\n",
      "11343/11343 [==============================] - 9s 780us/step - loss: 0.5652 - accuracy: 0.7116 - val_loss: 0.5506 - val_accuracy: 0.7236\n",
      "Epoch 29/50\n",
      "11343/11343 [==============================] - 9s 778us/step - loss: 0.5632 - accuracy: 0.7118 - val_loss: 0.5529 - val_accuracy: 0.7219\n",
      "Epoch 30/50\n",
      "11343/11343 [==============================] - 9s 778us/step - loss: 0.5613 - accuracy: 0.7137 - val_loss: 0.5498 - val_accuracy: 0.7270\n",
      "Epoch 31/50\n",
      "11343/11343 [==============================] - 9s 775us/step - loss: 0.5594 - accuracy: 0.7167 - val_loss: 0.5456 - val_accuracy: 0.7236\n",
      "Epoch 32/50\n",
      "11343/11343 [==============================] - 9s 776us/step - loss: 0.5584 - accuracy: 0.7172 - val_loss: 0.5430 - val_accuracy: 0.7303\n",
      "Epoch 33/50\n",
      "11343/11343 [==============================] - 9s 777us/step - loss: 0.5570 - accuracy: 0.7208 - val_loss: 0.5491 - val_accuracy: 0.7320\n",
      "Epoch 34/50\n",
      "11343/11343 [==============================] - 9s 805us/step - loss: 0.5548 - accuracy: 0.7212 - val_loss: 0.5499 - val_accuracy: 0.7320\n",
      "Epoch 35/50\n",
      "11343/11343 [==============================] - 11s 947us/step - loss: 0.5534 - accuracy: 0.7174 - val_loss: 0.5526 - val_accuracy: 0.7253\n",
      "Epoch 36/50\n",
      "11343/11343 [==============================] - 11s 953us/step - loss: 0.5515 - accuracy: 0.7222 - val_loss: 0.5398 - val_accuracy: 0.7303\n",
      "Epoch 37/50\n",
      "11343/11343 [==============================] - 10s 883us/step - loss: 0.5512 - accuracy: 0.7216 - val_loss: 0.5422 - val_accuracy: 0.7353\n",
      "Epoch 38/50\n",
      "11343/11343 [==============================] - 11s 942us/step - loss: 0.5498 - accuracy: 0.7225 - val_loss: 0.5355 - val_accuracy: 0.7370\n",
      "Epoch 39/50\n",
      "11343/11343 [==============================] - 10s 910us/step - loss: 0.5481 - accuracy: 0.7241 - val_loss: 0.5372 - val_accuracy: 0.7454\n",
      "Epoch 40/50\n",
      "11343/11343 [==============================] - 10s 910us/step - loss: 0.5468 - accuracy: 0.7261 - val_loss: 0.5339 - val_accuracy: 0.7303\n",
      "Epoch 41/50\n",
      "11343/11343 [==============================] - 10s 910us/step - loss: 0.5456 - accuracy: 0.7247 - val_loss: 0.5331 - val_accuracy: 0.7286\n",
      "Epoch 42/50\n",
      "11343/11343 [==============================] - 11s 985us/step - loss: 0.5447 - accuracy: 0.7278 - val_loss: 0.5317 - val_accuracy: 0.7387\n",
      "Epoch 43/50\n",
      "11343/11343 [==============================] - 10s 905us/step - loss: 0.5429 - accuracy: 0.7252 - val_loss: 0.5413 - val_accuracy: 0.7370\n",
      "Epoch 44/50\n",
      "11343/11343 [==============================] - 10s 886us/step - loss: 0.5421 - accuracy: 0.7285 - val_loss: 0.5317 - val_accuracy: 0.7320\n",
      "Epoch 45/50\n",
      "11343/11343 [==============================] - 10s 842us/step - loss: 0.5410 - accuracy: 0.7261 - val_loss: 0.5286 - val_accuracy: 0.7454\n",
      "Epoch 46/50\n",
      "11343/11343 [==============================] - 10s 920us/step - loss: 0.5400 - accuracy: 0.7268 - val_loss: 0.5265 - val_accuracy: 0.7454\n",
      "Epoch 47/50\n",
      "11343/11343 [==============================] - 10s 925us/step - loss: 0.5391 - accuracy: 0.7285 - val_loss: 0.5316 - val_accuracy: 0.7353\n",
      "Epoch 48/50\n",
      "11343/11343 [==============================] - 10s 869us/step - loss: 0.5371 - accuracy: 0.7316 - val_loss: 0.5266 - val_accuracy: 0.7320\n",
      "Epoch 49/50\n",
      "11343/11343 [==============================] - 9s 805us/step - loss: 0.5367 - accuracy: 0.7317 - val_loss: 0.5265 - val_accuracy: 0.7471\n",
      "Epoch 50/50\n",
      "11343/11343 [==============================] - 9s 774us/step - loss: 0.5358 - accuracy: 0.7306 - val_loss: 0.5305 - val_accuracy: 0.7387\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [597, 1990]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-9362003092b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m#scores_list[str(test_index[0])+\"_\"+str(test_index[1])] = model.evaluate(X_test, y_test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m#scores_list[str(test_index[0])] = model.evaluate(X_test, y_test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'0'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict)\u001b[0m\n\u001b[0;32m   1850\u001b[0m     \"\"\"\n\u001b[0;32m   1851\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1852\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1853\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1854\u001b[0m     \u001b[0mlabels_given\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \"\"\"\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 205\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [597, 1990]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "epochs = 50\n",
    "X = tx[:].reshape((tx.shape[1]*tx.shape[0], 28, 28, 3))\n",
    "y = ty[:].reshape((ty.shape[1]*ty.shape[0], 2))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.05,shuffle=True)\n",
    "\n",
    "model = get_model()\n",
    "opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#x_train = x_train.astype('float32')\n",
    "#x_test = x_test.astype('float32')\n",
    "#x_train /= 255\n",
    "#x_test /= 255\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data = (x_test, y_test), verbose = 1, shuffle = True)\n",
    "#scores_list[str(test_index[0])+\"_\"+str(test_index[1])] = model.evaluate(X_test, y_test)\n",
    "#scores_list[str(test_index[0])] = model.evaluate(X_test, y_test)\n",
    "report = classification_report(y_test[:,1],   model.predict(X_test).argmax(axis = -1), target_names=['1', '0'])\n",
    "print(report)\n",
    "\n",
    "model.save('80_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.65      0.71       298\n",
      "           0       0.70      0.83      0.76       299\n",
      "\n",
      "    accuracy                           0.74       597\n",
      "   macro avg       0.75      0.74      0.74       597\n",
      "weighted avg       0.75      0.74      0.74       597\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test[:,1],   model.predict(x_test).argmax(axis = -1), target_names=['1', '0'])\n",
    "print(report)\n",
    "\n",
    "model.save('80_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('path_to_my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "24/24 [==============================] - 0s 250us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7046559453010559, 0.625]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label true 0 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 1\n",
      "\n",
      "[1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 0\n",
      "\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 1\n",
      "\n",
      "[0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 0\n",
      "\n",
      "[1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 0\n",
      "\n",
      "[1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 0\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 0\n",
      "\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 0\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 0\n",
      "\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 0\n",
      "\n",
      "[0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 0\n",
      "\n",
      "[1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 0\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 1\n",
      "\n",
      "[0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 0\n",
      "\n",
      "[0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 0\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 1\n",
      "\n",
      "[0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 1\n",
      "\n",
      "[0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 0\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 0\n",
      "\n",
      "[0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 0\n",
      "\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 0\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 0\n",
      "\n",
      "[0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 0\n",
      "\n",
      "[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 0\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 0\n",
      "\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 0\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 0\n",
      "\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 0\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 0\n",
      "\n",
      "[0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 0\n",
      "\n",
      "[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 0\n",
      "\n",
      "[0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 0\n",
      "\n",
      "[1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 0\n",
      "\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 0\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 0\n",
      "\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "\n",
      "Label true 0 \n",
      "Label pred 0\n",
      "\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "\n",
      "Accuracy on 0 = 0.775\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "\n",
      "Label true 1 \n",
      "Label pred 1\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "\n",
      "\n",
      "\n",
      "Accuracy on 1 = 1.0\n"
     ]
    }
   ],
   "source": [
    "from test_pipeline_3 import  TestPipelineEEG\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import keras\n",
    "import numpy as np\n",
    "model = keras.models.load_model(\"80_model.h5\")\n",
    "              #'data/data_train_new_appr_27_label1.csv',\n",
    "              #'data/data_train_new_appr_28_label0.csv'\n",
    "predictor = TestPipelineEEG(image_size = 28, frame_duration = 0.78, overlap = 0.0, model_path = '80_model.h5', normalize = True)\n",
    "data = pd.read_csv('data/data_train_new_appr_28_label0.csv')\n",
    "data.columns = range(14)\n",
    "overlap = 0.8\n",
    "accum_window = 500\n",
    "acc_lst = []\n",
    "j = 0\n",
    "while((j+accum_window) <= data.shape[0]):\n",
    "    lst = []\n",
    "    i = 0\n",
    "    temp = data.iloc[j:j+accum_window]\n",
    "    while (i+100) <= temp.shape[0]:\n",
    "        lst.append(predictor.evaluate(temp.iloc[i:i+100], threshold = 0.5))\n",
    "        i = i + 100 - int(100*overlap)\n",
    "    label = max(set(lst), key=lst.count)\n",
    "    \n",
    "    print(\"Label true {} \\nLabel pred {}\\n\".format(0, label))\n",
    "    print(lst)\n",
    "    print(\"\\n\\n\")\n",
    "    acc_lst.append(label)\n",
    "    j+=accum_window\n",
    "print(\"Accuracy on 0 = {}\".format((1-np.sum(acc_lst)/len(acc_lst))))\n",
    "\n",
    "j = 0\n",
    "data = pd.read_csv('data/data_train_new_appr_27_label1.csv')\n",
    "data.columns = range(14)\n",
    "acc_lst = []\n",
    "while((j+accum_window) <= data.shape[0]):\n",
    "    lst = []\n",
    "    i = 0\n",
    "    temp = data.iloc[j:j+accum_window]\n",
    "    while (i+100) <= temp.shape[0]:\n",
    "        lst.append(predictor.evaluate(temp.iloc[i:i+100], threshold = 0.5))\n",
    "        i = i + 100 - int(100*overlap)\n",
    "    label = max(set(lst), key=lst.count)\n",
    "    print(\"Label true {} \\nLabel pred {}\\n\".format(1, label))\n",
    "    print(lst)\n",
    "    print(\"\\n\\n\")\n",
    "    acc_lst.append(label)\n",
    "    j+=accum_window\n",
    "print(\"Accuracy on 1 = {}\".format((np.sum(acc_lst)/len(acc_lst))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
