{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\Diplom_New\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\Diplom_New\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\Diplom_New\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\Diplom_New\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\Diplom_New\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\Diplom_New\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\Diplom_New\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\Diplom_New\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\Diplom_New\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\Diplom_New\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\Diplom_New\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\user\\Anaconda3\\envs\\Diplom_New\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "from os import listdir\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import shuffle\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy\n",
    "from sklearn import metrics\n",
    "#from statsmodels.stats.weightstats import _tconfint_generic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Convolution1D, Dense, Dropout, Input, merge, GlobalMaxPooling1D, MaxPooling1D, Flatten, LSTM\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import RMSprop\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Embedding, LSTM, LeakyReLU\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auroc(y_true, y_pred):\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References\n",
    "# https://github.com/ShubhamVerma1/EEG-Signal-Classification/blob/master/Code.ipynb - попробовано (NN, LSTM)\n",
    "# https://github.com/SuperBruceJia/EEG-Motor-Imagery-Classification-CNNs-TensorFlow - не пробовал (CNN)\n",
    "# https://github.com/kiselev1189/EEGClassificationMCNN/blob/master/NewDatasetConvnet.ipynb - не пробовал, яндекс, не оч код\n",
    "# https://github.com/Cerebro409/EEG-Classification-Using-Recurrent-Neural-Network/blob/master/eeg_lstm-v2.ipynb - не пробовал"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(280000, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>sample_num</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4233.462</td>\n",
       "      <td>4149.744</td>\n",
       "      <td>4195.000</td>\n",
       "      <td>4192.179</td>\n",
       "      <td>4136.282</td>\n",
       "      <td>4193.333</td>\n",
       "      <td>4199.872</td>\n",
       "      <td>4195.769</td>\n",
       "      <td>4184.231</td>\n",
       "      <td>4253.846</td>\n",
       "      <td>4195.000</td>\n",
       "      <td>4185.769</td>\n",
       "      <td>4188.462</td>\n",
       "      <td>4171.282</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4231.795</td>\n",
       "      <td>4141.410</td>\n",
       "      <td>4194.103</td>\n",
       "      <td>4208.974</td>\n",
       "      <td>4135.385</td>\n",
       "      <td>4189.615</td>\n",
       "      <td>4190.769</td>\n",
       "      <td>4184.103</td>\n",
       "      <td>4177.436</td>\n",
       "      <td>4245.385</td>\n",
       "      <td>4189.359</td>\n",
       "      <td>4178.205</td>\n",
       "      <td>4178.718</td>\n",
       "      <td>4167.436</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4226.282</td>\n",
       "      <td>4138.974</td>\n",
       "      <td>4191.923</td>\n",
       "      <td>4198.077</td>\n",
       "      <td>4148.333</td>\n",
       "      <td>4202.692</td>\n",
       "      <td>4180.769</td>\n",
       "      <td>4167.308</td>\n",
       "      <td>4164.487</td>\n",
       "      <td>4234.872</td>\n",
       "      <td>4178.974</td>\n",
       "      <td>4163.333</td>\n",
       "      <td>4163.462</td>\n",
       "      <td>4157.949</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4235.641</td>\n",
       "      <td>4167.692</td>\n",
       "      <td>4198.846</td>\n",
       "      <td>4199.231</td>\n",
       "      <td>4165.256</td>\n",
       "      <td>4218.077</td>\n",
       "      <td>4186.282</td>\n",
       "      <td>4176.026</td>\n",
       "      <td>4172.436</td>\n",
       "      <td>4233.974</td>\n",
       "      <td>4191.667</td>\n",
       "      <td>4179.487</td>\n",
       "      <td>4176.667</td>\n",
       "      <td>4174.103</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4247.436</td>\n",
       "      <td>4188.974</td>\n",
       "      <td>4207.051</td>\n",
       "      <td>4223.718</td>\n",
       "      <td>4162.436</td>\n",
       "      <td>4208.077</td>\n",
       "      <td>4190.641</td>\n",
       "      <td>4183.205</td>\n",
       "      <td>4175.128</td>\n",
       "      <td>4235.513</td>\n",
       "      <td>4201.923</td>\n",
       "      <td>4194.487</td>\n",
       "      <td>4189.359</td>\n",
       "      <td>4192.436</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  4233.462  4149.744  4195.000  4192.179  4136.282  4193.333  4199.872   \n",
       "1  4231.795  4141.410  4194.103  4208.974  4135.385  4189.615  4190.769   \n",
       "2  4226.282  4138.974  4191.923  4198.077  4148.333  4202.692  4180.769   \n",
       "3  4235.641  4167.692  4198.846  4199.231  4165.256  4218.077  4186.282   \n",
       "4  4247.436  4188.974  4207.051  4223.718  4162.436  4208.077  4190.641   \n",
       "\n",
       "          7         8         9        10        11        12        13  \\\n",
       "0  4195.769  4184.231  4253.846  4195.000  4185.769  4188.462  4171.282   \n",
       "1  4184.103  4177.436  4245.385  4189.359  4178.205  4178.718  4167.436   \n",
       "2  4167.308  4164.487  4234.872  4178.974  4163.333  4163.462  4157.949   \n",
       "3  4176.026  4172.436  4233.974  4191.667  4179.487  4176.667  4174.103   \n",
       "4  4183.205  4175.128  4235.513  4201.923  4194.487  4189.359  4192.436   \n",
       "\n",
       "   sample_num  label  \n",
       "0           0      1  \n",
       "1           0      1  \n",
       "2           0      1  \n",
       "3           0      1  \n",
       "4           0      1  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data = pd.read_csv('C:\\\\Users\\\\user\\\\cortex-v2-example\\\\python\\\\New_data_full_15\\\\full_data_shuffle.csv',\n",
    "#                  index_col = False)\n",
    "#data = pd.read_csv('C:\\\\Users\\\\user\\\\cortex-v2-example\\\\python\\\\New_data_full_16\\\\full_data_shuffle_21_04_1167.csv',\n",
    "#                  index_col = False)\n",
    "#data = pd.read_csv('C:\\\\Users\\\\user\\\\cortex-v2-example\\\\python\\\\data_22_04_new_approach\\\\data_train_new_appr.csv',\n",
    "#                  index_col = False)\n",
    "data = pd.read_csv('C:\\\\Users\\\\user\\\\cortex-v2-example\\\\python\\\\df_all.csv')\n",
    "#data.rename(columns = {'0':'sample_num','15':'gesture'},inplace = True)\n",
    "data = data.drop(columns = ['Unnamed: 0'])\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(280000, 16)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = data.replace(['сжать руку','разжать руку'], [1,0])\n",
    "df = data.copy()\n",
    "#print(df['gesture'].unique())\n",
    "print(df['label'].unique())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_coorelation_matrix(df):\n",
    "    fig = plt.figure(figsize = (20,12))\n",
    "    df = pd.DataFrame(df)\n",
    "    corr = df.corr()\n",
    "    ax = sns.heatmap(corr, \n",
    "                     vmin=-1, \n",
    "                     vmax=1, \n",
    "                     center=0,\n",
    "                     cmap=sns.diverging_palette(20, 220, n=200),\n",
    "                     square=True)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(),\n",
    "                       rotation=45,\n",
    "                       horizontalalignment='right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_sensor(data,name,color):\n",
    "    color_list=[\"navy\",\"darkmagenta\",\"red\",\"black\"]\n",
    "    fig, ax = plt.subplots(2,7, figsize=(28,16))\n",
    "    sns.set(style=\"white\")\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    x=0\n",
    "    for i in range(2):\n",
    "        for j in range(7):\n",
    "            plt.suptitle(name)\n",
    "            sns.distplot(data.iloc[:,x],kde=False,ax=ax[i][j],color=color_list[color],bins=20);\n",
    "            x+=1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x864 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_coorelation_matrix(df.drop(columns = ['gesture','sample_num']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2016x1152 with 14 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_sensor(df[df['gesture']==1].drop(columns=['sample_num', 'gesture']),'Gesture: 1',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2016x1152 with 14 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_sensor(df[df['gesture']==0].drop(columns=['sample_num', 'gesture']),'Gesture: 0',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Adding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_num</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>gesture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4314.359</td>\n",
       "      <td>4218.590</td>\n",
       "      <td>4211.923</td>\n",
       "      <td>4207.051</td>\n",
       "      <td>4199.872</td>\n",
       "      <td>4209.103</td>\n",
       "      <td>4202.821</td>\n",
       "      <td>4203.462</td>\n",
       "      <td>4191.795</td>\n",
       "      <td>4202.564</td>\n",
       "      <td>4212.949</td>\n",
       "      <td>4226.923</td>\n",
       "      <td>4216.154</td>\n",
       "      <td>4217.564</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>4296.667</td>\n",
       "      <td>4204.231</td>\n",
       "      <td>4195.897</td>\n",
       "      <td>4194.487</td>\n",
       "      <td>4187.436</td>\n",
       "      <td>4195.513</td>\n",
       "      <td>4201.026</td>\n",
       "      <td>4196.026</td>\n",
       "      <td>4183.077</td>\n",
       "      <td>4183.077</td>\n",
       "      <td>4194.872</td>\n",
       "      <td>4211.538</td>\n",
       "      <td>4199.103</td>\n",
       "      <td>4202.821</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>4305.769</td>\n",
       "      <td>4209.359</td>\n",
       "      <td>4199.103</td>\n",
       "      <td>4195.513</td>\n",
       "      <td>4208.205</td>\n",
       "      <td>4194.103</td>\n",
       "      <td>4186.923</td>\n",
       "      <td>4190.897</td>\n",
       "      <td>4180.769</td>\n",
       "      <td>4181.923</td>\n",
       "      <td>4197.692</td>\n",
       "      <td>4214.872</td>\n",
       "      <td>4197.692</td>\n",
       "      <td>4207.051</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4329.872</td>\n",
       "      <td>4226.282</td>\n",
       "      <td>4219.615</td>\n",
       "      <td>4208.333</td>\n",
       "      <td>4225.769</td>\n",
       "      <td>4218.590</td>\n",
       "      <td>4187.436</td>\n",
       "      <td>4199.103</td>\n",
       "      <td>4191.667</td>\n",
       "      <td>4204.103</td>\n",
       "      <td>4215.256</td>\n",
       "      <td>4235.769</td>\n",
       "      <td>4214.615</td>\n",
       "      <td>4224.744</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4321.282</td>\n",
       "      <td>4218.077</td>\n",
       "      <td>4216.282</td>\n",
       "      <td>4202.821</td>\n",
       "      <td>4201.923</td>\n",
       "      <td>4213.205</td>\n",
       "      <td>4196.795</td>\n",
       "      <td>4205.769</td>\n",
       "      <td>4201.026</td>\n",
       "      <td>4202.564</td>\n",
       "      <td>4206.538</td>\n",
       "      <td>4233.462</td>\n",
       "      <td>4209.615</td>\n",
       "      <td>4217.179</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_num         1         2         3         4         5         6  \\\n",
       "0           0  4314.359  4218.590  4211.923  4207.051  4199.872  4209.103   \n",
       "1           0  4296.667  4204.231  4195.897  4194.487  4187.436  4195.513   \n",
       "2           0  4305.769  4209.359  4199.103  4195.513  4208.205  4194.103   \n",
       "3           0  4329.872  4226.282  4219.615  4208.333  4225.769  4218.590   \n",
       "4           0  4321.282  4218.077  4216.282  4202.821  4201.923  4213.205   \n",
       "\n",
       "          7         8         9        10        11        12        13  \\\n",
       "0  4202.821  4203.462  4191.795  4202.564  4212.949  4226.923  4216.154   \n",
       "1  4201.026  4196.026  4183.077  4183.077  4194.872  4211.538  4199.103   \n",
       "2  4186.923  4190.897  4180.769  4181.923  4197.692  4214.872  4197.692   \n",
       "3  4187.436  4199.103  4191.667  4204.103  4215.256  4235.769  4214.615   \n",
       "4  4196.795  4205.769  4201.026  4202.564  4206.538  4233.462  4209.615   \n",
       "\n",
       "         14  gesture  \n",
       "0  4217.564        0  \n",
       "1  4202.821        0  \n",
       "2  4207.051        0  \n",
       "3  4224.744        0  \n",
       "4  4217.179        0  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\Diplom_New\\lib\\site-packages\\pandas\\core\\generic.py:3936: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">1</th>\n",
       "      <th colspan=\"4\" halign=\"left\">2</th>\n",
       "      <th colspan=\"2\" halign=\"left\">3</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">12</th>\n",
       "      <th colspan=\"4\" halign=\"left\">13</th>\n",
       "      <th colspan=\"4\" halign=\"left\">14</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>skew</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>skew</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>...</th>\n",
       "      <th>std</th>\n",
       "      <th>skew</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>skew</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>skew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>431040.006</td>\n",
       "      <td>4310.40006</td>\n",
       "      <td>13.148857</td>\n",
       "      <td>-0.145418</td>\n",
       "      <td>422275.388</td>\n",
       "      <td>4222.75388</td>\n",
       "      <td>11.259058</td>\n",
       "      <td>0.925693</td>\n",
       "      <td>421663.469</td>\n",
       "      <td>4216.63469</td>\n",
       "      <td>...</td>\n",
       "      <td>12.560249</td>\n",
       "      <td>-0.394037</td>\n",
       "      <td>421497.945</td>\n",
       "      <td>4214.97945</td>\n",
       "      <td>18.317692</td>\n",
       "      <td>-1.282354</td>\n",
       "      <td>421749.749</td>\n",
       "      <td>4217.49749</td>\n",
       "      <td>14.284647</td>\n",
       "      <td>-0.424926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>429456.024</td>\n",
       "      <td>4294.56024</td>\n",
       "      <td>20.624025</td>\n",
       "      <td>0.469093</td>\n",
       "      <td>424592.688</td>\n",
       "      <td>4245.92688</td>\n",
       "      <td>15.556198</td>\n",
       "      <td>0.706419</td>\n",
       "      <td>421201.922</td>\n",
       "      <td>4212.01922</td>\n",
       "      <td>...</td>\n",
       "      <td>15.671253</td>\n",
       "      <td>0.117687</td>\n",
       "      <td>418919.740</td>\n",
       "      <td>4189.19740</td>\n",
       "      <td>19.874206</td>\n",
       "      <td>0.355128</td>\n",
       "      <td>419281.282</td>\n",
       "      <td>4192.81282</td>\n",
       "      <td>18.798343</td>\n",
       "      <td>0.556029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>426603.842</td>\n",
       "      <td>4266.03842</td>\n",
       "      <td>13.255429</td>\n",
       "      <td>0.121591</td>\n",
       "      <td>421986.799</td>\n",
       "      <td>4219.86799</td>\n",
       "      <td>14.373552</td>\n",
       "      <td>-0.228927</td>\n",
       "      <td>420904.873</td>\n",
       "      <td>4209.04873</td>\n",
       "      <td>...</td>\n",
       "      <td>14.487198</td>\n",
       "      <td>0.348861</td>\n",
       "      <td>421396.541</td>\n",
       "      <td>4213.96541</td>\n",
       "      <td>21.457575</td>\n",
       "      <td>0.321921</td>\n",
       "      <td>420208.338</td>\n",
       "      <td>4202.08338</td>\n",
       "      <td>15.503169</td>\n",
       "      <td>0.564626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>426186.414</td>\n",
       "      <td>4261.86414</td>\n",
       "      <td>17.008175</td>\n",
       "      <td>0.388280</td>\n",
       "      <td>423933.204</td>\n",
       "      <td>4239.33204</td>\n",
       "      <td>18.832860</td>\n",
       "      <td>0.195715</td>\n",
       "      <td>420690.127</td>\n",
       "      <td>4206.90127</td>\n",
       "      <td>...</td>\n",
       "      <td>11.373059</td>\n",
       "      <td>-0.527574</td>\n",
       "      <td>421828.719</td>\n",
       "      <td>4218.28719</td>\n",
       "      <td>13.905983</td>\n",
       "      <td>0.240404</td>\n",
       "      <td>421317.305</td>\n",
       "      <td>4213.17305</td>\n",
       "      <td>15.766822</td>\n",
       "      <td>0.064520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>424016.925</td>\n",
       "      <td>4240.16925</td>\n",
       "      <td>13.054049</td>\n",
       "      <td>-0.733485</td>\n",
       "      <td>421938.592</td>\n",
       "      <td>4219.38592</td>\n",
       "      <td>12.650932</td>\n",
       "      <td>0.754463</td>\n",
       "      <td>420399.870</td>\n",
       "      <td>4203.99870</td>\n",
       "      <td>...</td>\n",
       "      <td>12.822393</td>\n",
       "      <td>0.128829</td>\n",
       "      <td>421265.512</td>\n",
       "      <td>4212.65512</td>\n",
       "      <td>13.563664</td>\n",
       "      <td>-0.194463</td>\n",
       "      <td>420496.539</td>\n",
       "      <td>4204.96539</td>\n",
       "      <td>12.917164</td>\n",
       "      <td>-0.299879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            1                                            2              \\\n",
       "          sum        mean        std      skew         sum        mean   \n",
       "0  431040.006  4310.40006  13.148857 -0.145418  422275.388  4222.75388   \n",
       "1  429456.024  4294.56024  20.624025  0.469093  424592.688  4245.92688   \n",
       "2  426603.842  4266.03842  13.255429  0.121591  421986.799  4219.86799   \n",
       "3  426186.414  4261.86414  17.008175  0.388280  423933.204  4239.33204   \n",
       "4  424016.925  4240.16925  13.054049 -0.733485  421938.592  4219.38592   \n",
       "\n",
       "                                 3              ...         12            \\\n",
       "         std      skew         sum        mean  ...        std      skew   \n",
       "0  11.259058  0.925693  421663.469  4216.63469  ...  12.560249 -0.394037   \n",
       "1  15.556198  0.706419  421201.922  4212.01922  ...  15.671253  0.117687   \n",
       "2  14.373552 -0.228927  420904.873  4209.04873  ...  14.487198  0.348861   \n",
       "3  18.832860  0.195715  420690.127  4206.90127  ...  11.373059 -0.527574   \n",
       "4  12.650932  0.754463  420399.870  4203.99870  ...  12.822393  0.128829   \n",
       "\n",
       "           13                                           14              \\\n",
       "          sum        mean        std      skew         sum        mean   \n",
       "0  421497.945  4214.97945  18.317692 -1.282354  421749.749  4217.49749   \n",
       "1  418919.740  4189.19740  19.874206  0.355128  419281.282  4192.81282   \n",
       "2  421396.541  4213.96541  21.457575  0.321921  420208.338  4202.08338   \n",
       "3  421828.719  4218.28719  13.905983  0.240404  421317.305  4213.17305   \n",
       "4  421265.512  4212.65512  13.563664 -0.194463  420496.539  4204.96539   \n",
       "\n",
       "                        \n",
       "         std      skew  \n",
       "0  14.284647 -0.424926  \n",
       "1  18.798343  0.556029  \n",
       "2  15.503169  0.564626  \n",
       "3  15.766822  0.064520  \n",
       "4  12.917164 -0.299879  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X = df.groupby(['sample_num']).agg(['min','max','sum','mean','std','skew']).reset_index().drop(columns=['gesture', 'sample_num'])\n",
    "X = df.groupby(['sample_num']).agg(['sum','mean','std','skew']).reset_index().drop(columns=['gesture', 'sample_num'])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_kurt = df.groupby(['sample_num']).apply(pd.DataFrame.kurt).drop(columns =['sample_num', 'gesture'])\n",
    "X_kurt = X_kurt.reset_index().drop(columns = ['sample_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(1, sum)</th>\n",
       "      <th>(1, mean)</th>\n",
       "      <th>(1, std)</th>\n",
       "      <th>(1, skew)</th>\n",
       "      <th>(2, sum)</th>\n",
       "      <th>(2, mean)</th>\n",
       "      <th>(2, std)</th>\n",
       "      <th>(2, skew)</th>\n",
       "      <th>(3, sum)</th>\n",
       "      <th>(3, mean)</th>\n",
       "      <th>...</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>431040.006</td>\n",
       "      <td>4310.40006</td>\n",
       "      <td>13.148857</td>\n",
       "      <td>-0.145418</td>\n",
       "      <td>422275.388</td>\n",
       "      <td>4222.75388</td>\n",
       "      <td>11.259058</td>\n",
       "      <td>0.925693</td>\n",
       "      <td>421663.469</td>\n",
       "      <td>4216.63469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158282</td>\n",
       "      <td>0.480231</td>\n",
       "      <td>-0.155663</td>\n",
       "      <td>-0.113750</td>\n",
       "      <td>1.105309</td>\n",
       "      <td>2.591978</td>\n",
       "      <td>0.922843</td>\n",
       "      <td>0.422739</td>\n",
       "      <td>2.142318</td>\n",
       "      <td>0.843541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>429456.024</td>\n",
       "      <td>4294.56024</td>\n",
       "      <td>20.624025</td>\n",
       "      <td>0.469093</td>\n",
       "      <td>424592.688</td>\n",
       "      <td>4245.92688</td>\n",
       "      <td>15.556198</td>\n",
       "      <td>0.706419</td>\n",
       "      <td>421201.922</td>\n",
       "      <td>4212.01922</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.305032</td>\n",
       "      <td>0.173096</td>\n",
       "      <td>0.474177</td>\n",
       "      <td>-0.706754</td>\n",
       "      <td>-0.440211</td>\n",
       "      <td>-0.215350</td>\n",
       "      <td>-0.536716</td>\n",
       "      <td>-0.273022</td>\n",
       "      <td>0.757713</td>\n",
       "      <td>0.590539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>426603.842</td>\n",
       "      <td>4266.03842</td>\n",
       "      <td>13.255429</td>\n",
       "      <td>0.121591</td>\n",
       "      <td>421986.799</td>\n",
       "      <td>4219.86799</td>\n",
       "      <td>14.373552</td>\n",
       "      <td>-0.228927</td>\n",
       "      <td>420904.873</td>\n",
       "      <td>4209.04873</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.072038</td>\n",
       "      <td>-0.634111</td>\n",
       "      <td>-0.080198</td>\n",
       "      <td>-0.572210</td>\n",
       "      <td>-0.463263</td>\n",
       "      <td>-0.931559</td>\n",
       "      <td>-0.561619</td>\n",
       "      <td>-0.680034</td>\n",
       "      <td>-0.955036</td>\n",
       "      <td>-0.233890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>426186.414</td>\n",
       "      <td>4261.86414</td>\n",
       "      <td>17.008175</td>\n",
       "      <td>0.388280</td>\n",
       "      <td>423933.204</td>\n",
       "      <td>4239.33204</td>\n",
       "      <td>18.832860</td>\n",
       "      <td>0.195715</td>\n",
       "      <td>420690.127</td>\n",
       "      <td>4206.90127</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.480065</td>\n",
       "      <td>2.135504</td>\n",
       "      <td>0.494244</td>\n",
       "      <td>0.415464</td>\n",
       "      <td>0.729864</td>\n",
       "      <td>-0.701557</td>\n",
       "      <td>0.107515</td>\n",
       "      <td>1.236307</td>\n",
       "      <td>0.522749</td>\n",
       "      <td>0.475724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>424016.925</td>\n",
       "      <td>4240.16925</td>\n",
       "      <td>13.054049</td>\n",
       "      <td>-0.733485</td>\n",
       "      <td>421938.592</td>\n",
       "      <td>4219.38592</td>\n",
       "      <td>12.650932</td>\n",
       "      <td>0.754463</td>\n",
       "      <td>420399.870</td>\n",
       "      <td>4203.99870</td>\n",
       "      <td>...</td>\n",
       "      <td>0.344715</td>\n",
       "      <td>1.977961</td>\n",
       "      <td>1.062138</td>\n",
       "      <td>-0.528115</td>\n",
       "      <td>-0.351181</td>\n",
       "      <td>-0.488883</td>\n",
       "      <td>-0.142272</td>\n",
       "      <td>-0.271946</td>\n",
       "      <td>-0.570188</td>\n",
       "      <td>0.276129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     (1, sum)   (1, mean)   (1, std)  (1, skew)    (2, sum)   (2, mean)  \\\n",
       "0  431040.006  4310.40006  13.148857  -0.145418  422275.388  4222.75388   \n",
       "1  429456.024  4294.56024  20.624025   0.469093  424592.688  4245.92688   \n",
       "2  426603.842  4266.03842  13.255429   0.121591  421986.799  4219.86799   \n",
       "3  426186.414  4261.86414  17.008175   0.388280  423933.204  4239.33204   \n",
       "4  424016.925  4240.16925  13.054049  -0.733485  421938.592  4219.38592   \n",
       "\n",
       "    (2, std)  (2, skew)    (3, sum)   (3, mean)  ...         5         6  \\\n",
       "0  11.259058   0.925693  421663.469  4216.63469  ...  0.158282  0.480231   \n",
       "1  15.556198   0.706419  421201.922  4212.01922  ... -0.305032  0.173096   \n",
       "2  14.373552  -0.228927  420904.873  4209.04873  ... -0.072038 -0.634111   \n",
       "3  18.832860   0.195715  420690.127  4206.90127  ... -0.480065  2.135504   \n",
       "4  12.650932   0.754463  420399.870  4203.99870  ...  0.344715  1.977961   \n",
       "\n",
       "          7         8         9        10        11        12        13  \\\n",
       "0 -0.155663 -0.113750  1.105309  2.591978  0.922843  0.422739  2.142318   \n",
       "1  0.474177 -0.706754 -0.440211 -0.215350 -0.536716 -0.273022  0.757713   \n",
       "2 -0.080198 -0.572210 -0.463263 -0.931559 -0.561619 -0.680034 -0.955036   \n",
       "3  0.494244  0.415464  0.729864 -0.701557  0.107515  1.236307  0.522749   \n",
       "4  1.062138 -0.528115 -0.351181 -0.488883 -0.142272 -0.271946 -0.570188   \n",
       "\n",
       "         14  \n",
       "0  0.843541  \n",
       "1  0.590539  \n",
       "2 -0.233890  \n",
       "3  0.475724  \n",
       "4  0.276129  \n",
       "\n",
       "[5 rows x 70 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.concat([X, X_kurt], axis = 1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: min, dtype: int64"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df.groupby(['sample_num']).agg(['min','max','sum','mean','std']).reset_index()['gesture']['min']\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = shuffle(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "899 100\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(X) * 0.9)\n",
    "test_size = len(X) - train_size\n",
    "train, test = X.iloc[0:train_size], X.iloc[train_size:len(X)]\n",
    "print(train_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(899, 70) (100, 70)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train)\n",
    "X_test = scaler.transform(test)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(scaler, open('pred_dir\\\\scaler_new.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(899,) (100,)\n"
     ]
    }
   ],
   "source": [
    "y_train = y.iloc[0:train_size]\n",
    "y_test = y.iloc[train_size:len(y)]\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy after 20 experiments 0.5791666666666667\n",
      "[0.51666667 0.525      0.625      0.50833333 0.58333333 0.60833333\n",
      " 0.68333333 0.6        0.54166667 0.6        0.61666667 0.54166667\n",
      " 0.63333333 0.59166667 0.575      0.46666667 0.59166667 0.55\n",
      " 0.65       0.575     ]\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "lr_accur_list = cross_val_score(clf, X_train, y_train, cv=20, scoring = 'accuracy')\n",
    "print('Mean Accuracy after 20 experiments', lr_accur_list.mean())\n",
    "print(lr_accur_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_mean_std = lr_accur_list.std(ddof=1)/sqrt(len(lr_accur_list))\n",
    "#print('logistic regression model mean accuracy 95%% confidence interval', _tconfint_generic(lr_accur_list.mean(), \n",
    "#                                                                       lr_mean_std,\n",
    "#                                                                       len(lr_accur_list) - 1,\n",
    "#                                                                       0.05, 'two-sided'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for test:  0.5225\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "print('Accuracy score for test: ',\n",
    "      accuracy_score(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for test:  0.375\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score for test: ',\n",
    "      accuracy_score(y_test[:200], clf.predict(X_test[:200])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for test:  0.67\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score for test: ',\n",
    "      accuracy_score(y_test[200:400], clf.predict(X_test[200:400])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy after 20 experiments 0.5783333333333334\n",
      "[0.575      0.59166667 0.575      0.49166667 0.59166667 0.63333333\n",
      " 0.64166667 0.575      0.575      0.58333333 0.54166667 0.625\n",
      " 0.58333333 0.56666667 0.63333333 0.525      0.55833333 0.55833333\n",
      " 0.56666667 0.575     ]\n"
     ]
    }
   ],
   "source": [
    "tree_clf = DecisionTreeClassifier()\n",
    "tree_accur_list = cross_val_score(tree_clf, X_train, y_train, cv=20, scoring = 'accuracy')\n",
    "print('Mean Accuracy after 20 experiments', tree_accur_list.mean())\n",
    "print(tree_accur_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tree_mean_std = tree_accur_list.std(ddof=1)/sqrt(len(tree_accur_list))\n",
    "#print('decision tree model mean accuracy 95%% confidence interval', _tconfint_generic(tree_accur_list.mean(), \n",
    "#                                                                       tree_mean_std,\n",
    "#                                                                       len(tree_accur_list) - 1,\n",
    "#                                                                       0.05, 'two-sided'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for test:  0.4975\n"
     ]
    }
   ],
   "source": [
    "tree_clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "print('Accuracy score for test: ',\n",
    "      accuracy_score(y_test, tree_clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for test:  0.385\n"
     ]
    }
   ],
   "source": [
    "tree_clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "print('Accuracy score for test: ',\n",
    "      accuracy_score(y_test[:200], tree_clf.predict(X_test[:200])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for test:  0.58\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score for test: ',\n",
    "      accuracy_score(y_test[200:400], tree_clf.predict(X_test[200:400])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy after 20 experiments 0.6758333333333334\n",
      "[0.65833333 0.675      0.675      0.64166667 0.70833333 0.675\n",
      " 0.7        0.7        0.65       0.68333333 0.70833333 0.675\n",
      " 0.69166667 0.69166667 0.68333333 0.61666667 0.69166667 0.66666667\n",
      " 0.675      0.65      ]\n"
     ]
    }
   ],
   "source": [
    "forest_clf = RandomForestClassifier()\n",
    "forest_accur_list = cross_val_score(forest_clf, X_train, y_train, cv=20, scoring = 'accuracy')\n",
    "print('Mean Accuracy after 20 experiments', forest_accur_list.mean())\n",
    "print(forest_accur_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forest_mean_std = forest_accur_list.std(ddof=1)/sqrt(len(forest_accur_list))\n",
    "#print('random forest model mean accuarcy 95%% confidence interval', _tconfint_generic(forest_accur_list.mean(), \n",
    "#                                                                       forest_mean_std,\n",
    "#                                                                       len(forest_accur_list) - 1,\n",
    "#                                                                       0.05, 'two-sided'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for test:  0.5375\n"
     ]
    }
   ],
   "source": [
    "forest_clf = RandomForestClassifier().fit(X_train, y_train)\n",
    "print('Accuracy score for test: ',\n",
    "      accuracy_score(y_test, forest_clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for test:  0.505\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score for test: ',\n",
    "      accuracy_score(y_test[:200], forest_clf.predict(X_test[:200])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for test:  0.57\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score for test: ',\n",
    "      accuracy_score(y_test[200:], forest_clf.predict(X_test[200:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting classifier (best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy after 20 experiments 0.6770833333333334\n",
      "[0.63333333 0.65833333 0.725      0.65833333 0.61666667 0.65\n",
      " 0.7        0.74166667 0.69166667 0.69166667 0.66666667 0.69166667\n",
      " 0.66666667 0.73333333 0.69166667 0.69166667 0.61666667 0.68333333\n",
      " 0.625      0.70833333]\n"
     ]
    }
   ],
   "source": [
    "boosting_clf = GradientBoostingClassifier()\n",
    "boosting_accur_list = cross_val_score(boosting_clf, X_train, y_train, cv=20, scoring = 'accuracy')\n",
    "print('Mean Accuracy after 20 experiments', boosting_accur_list.mean())\n",
    "print(boosting_accur_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boosting_mean_std = boosting_accur_list.std(ddof=1)/sqrt(len(boosting_accur_list))\n",
    "#print('random forest model mean accuracy 95%% confidence interval', _tconfint_generic(boosting_accur_list.mean(), \n",
    "#                                                                       boosting_mean_std,\n",
    "#                                                                       len(boosting_accur_list) - 1,\n",
    "#                                                                       0.05, 'two-sided'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for test:  0.5325\n"
     ]
    }
   ],
   "source": [
    "boosting_clf = GradientBoostingClassifier().fit(X_train, y_train)\n",
    "print('Accuracy score for test: ',\n",
    "      accuracy_score(y_test, boosting_clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for test:  0.465\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score for test: ',\n",
    "      accuracy_score(y_test[:200], boosting_clf.predict(X_test[:200])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for test:  0.6\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score for test: ',\n",
    "      accuracy_score(y_test[200:], boosting_clf.predict(X_test[200:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Neural Network (github, yandex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/kiselev1189/EEGClassificationMCNN/blob/master/NewDatasetConvnet.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62000, 15) (62000,)\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(columns=['gesture'])\n",
    "y = df['gesture']\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55800 6200\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(X) * 0.9)\n",
    "test_size = len(X) - train_size\n",
    "train, test = X.iloc[0:train_size], X.iloc[train_size:len(X)]\n",
    "print(train_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55800, 15) (6200, 15)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train)\n",
    "X_test = scaler.transform(test)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55800,) (6200,)\n"
     ]
    }
   ],
   "source": [
    "y_train = y.iloc[0:train_size]\n",
    "y_test = y.iloc[train_size:len(y)]\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\Diplom_New\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(50, 5, activation=\"tanh\", padding=\"same\")`\n",
      "  \n",
      "C:\\Users\\user\\Anaconda3\\envs\\Diplom_New\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"dr...)`\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\user\\Anaconda3\\envs\\Diplom_New\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "def get_base_model(input_len, fsize):\n",
    "    '''Base network to be shared (eq. to feature extraction).\n",
    "    '''\n",
    "    input_seq = Input(shape=(input_len, 24))\n",
    "    nb_filters = 50\n",
    "    convolved = Convolution1D(nb_filters, 5, border_mode=\"same\", activation=\"tanh\")(input_seq)\n",
    "    pooled = GlobalMaxPooling1D()(convolved)\n",
    "    compressed = Dense(50, activation=\"linear\")(pooled)\n",
    "    compressed = Dropout(0.3)(compressed)\n",
    "    compressed = Dense(50, activation=\"relu\")(compressed)\n",
    "    compressed = Dropout(0.3)(compressed)\n",
    "    model = Model(input=input_seq, output=compressed)            \n",
    "    return model\n",
    "\n",
    "slice_len = 100\n",
    "\n",
    "input1125_seq = Input(shape=(slice_len, 24))\n",
    "\n",
    "base_network1125 = get_base_model(slice_len, 10)\n",
    "\n",
    "embedding_1125 = base_network1125(input1125_seq)\n",
    "out = Dense(2, activation='softmax')(embedding_1125)\n",
    "    \n",
    "model = Model(input=input1125_seq, output=out)\n",
    "    \n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generate_slice(test=False):\n",
    "    if test:\n",
    "        experiment_data = random.choice(list(test.values()))\n",
    "    else:\n",
    "        experiment_data = random.choice(list(train.values()))\n",
    "    \n",
    "    X = experiment_data.drop(columns=['gesture','sample_num'])\n",
    "    y = experiment_data[\"gesture\"]\n",
    "    \n",
    "    while True:\n",
    "        slice_start = np.random.choice(len(X) - slice_len)\n",
    "        slice_end = slice_start + slice_len\n",
    "        slice_x = X[slice_start:slice_end]\n",
    "        #slice_x = normalize(slice_x)\n",
    "        slice_y = y[slice_start:slice_end]\n",
    "        \n",
    "        if len(set(slice_y)) == 1:\n",
    "            return slice_x, to_onehot(slice_y[-1])\n",
    "\n",
    "\n",
    "def data_generator(batch_size, test=False):\n",
    "    while True:\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        \n",
    "        for i in range(0, batch_size):\n",
    "            x, y = generate_slice(test=test)\n",
    "            batch_x.append(x)\n",
    "            batch_y.append(y)\n",
    "            \n",
    "        y = np.array(batch_y)\n",
    "        x = np.array([i for i in batch_x])\n",
    "        yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_15 to have 3 dimensions, but got array with shape (55800, 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-6aab21d8140c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#                    callbacks=[earlyStopping, checkpointer], verbose=2, nb_val_samples=15000,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#                    validation_data=data_generator(batch_size=25, test=True))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\Diplom_New\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Diplom_New\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Diplom_New\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    133\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    136\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_15 to have 3 dimensions, but got array with shape (55800, 15)"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "nb_epoch = 100000\n",
    "earlyStopping = EarlyStopping(monitor='categorical_accuracy', patience=10, verbose=0, mode='auto')\n",
    "checkpointer = ModelCheckpoint(\"convlstm_alldata.h5\", monitor='categorical_accuracy', verbose=0,\n",
    "                               save_best_only=True, mode='auto', period=1)\n",
    "\n",
    "samples_per_epoch = 15000\n",
    "nb_epoch = 1\n",
    "\n",
    "#model.fit_generator(data_generator(batch_size=25), samples_per_epoch, nb_epoch, \n",
    "#                    callbacks=[earlyStopping, checkpointer], verbose=2, nb_val_samples=15000,\n",
    "#                    validation_data=data_generator(batch_size=25, test=True))\n",
    "model.fit(x=X_train, y=y_train, batch_size=25, epochs=10, validation_data=X_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network (Udacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>sample_num</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4233.462</td>\n",
       "      <td>4149.744</td>\n",
       "      <td>4195.000</td>\n",
       "      <td>4192.179</td>\n",
       "      <td>4136.282</td>\n",
       "      <td>4193.333</td>\n",
       "      <td>4199.872</td>\n",
       "      <td>4195.769</td>\n",
       "      <td>4184.231</td>\n",
       "      <td>4253.846</td>\n",
       "      <td>4195.000</td>\n",
       "      <td>4185.769</td>\n",
       "      <td>4188.462</td>\n",
       "      <td>4171.282</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4231.795</td>\n",
       "      <td>4141.410</td>\n",
       "      <td>4194.103</td>\n",
       "      <td>4208.974</td>\n",
       "      <td>4135.385</td>\n",
       "      <td>4189.615</td>\n",
       "      <td>4190.769</td>\n",
       "      <td>4184.103</td>\n",
       "      <td>4177.436</td>\n",
       "      <td>4245.385</td>\n",
       "      <td>4189.359</td>\n",
       "      <td>4178.205</td>\n",
       "      <td>4178.718</td>\n",
       "      <td>4167.436</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4226.282</td>\n",
       "      <td>4138.974</td>\n",
       "      <td>4191.923</td>\n",
       "      <td>4198.077</td>\n",
       "      <td>4148.333</td>\n",
       "      <td>4202.692</td>\n",
       "      <td>4180.769</td>\n",
       "      <td>4167.308</td>\n",
       "      <td>4164.487</td>\n",
       "      <td>4234.872</td>\n",
       "      <td>4178.974</td>\n",
       "      <td>4163.333</td>\n",
       "      <td>4163.462</td>\n",
       "      <td>4157.949</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4235.641</td>\n",
       "      <td>4167.692</td>\n",
       "      <td>4198.846</td>\n",
       "      <td>4199.231</td>\n",
       "      <td>4165.256</td>\n",
       "      <td>4218.077</td>\n",
       "      <td>4186.282</td>\n",
       "      <td>4176.026</td>\n",
       "      <td>4172.436</td>\n",
       "      <td>4233.974</td>\n",
       "      <td>4191.667</td>\n",
       "      <td>4179.487</td>\n",
       "      <td>4176.667</td>\n",
       "      <td>4174.103</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4247.436</td>\n",
       "      <td>4188.974</td>\n",
       "      <td>4207.051</td>\n",
       "      <td>4223.718</td>\n",
       "      <td>4162.436</td>\n",
       "      <td>4208.077</td>\n",
       "      <td>4190.641</td>\n",
       "      <td>4183.205</td>\n",
       "      <td>4175.128</td>\n",
       "      <td>4235.513</td>\n",
       "      <td>4201.923</td>\n",
       "      <td>4194.487</td>\n",
       "      <td>4189.359</td>\n",
       "      <td>4192.436</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  4233.462  4149.744  4195.000  4192.179  4136.282  4193.333  4199.872   \n",
       "1  4231.795  4141.410  4194.103  4208.974  4135.385  4189.615  4190.769   \n",
       "2  4226.282  4138.974  4191.923  4198.077  4148.333  4202.692  4180.769   \n",
       "3  4235.641  4167.692  4198.846  4199.231  4165.256  4218.077  4186.282   \n",
       "4  4247.436  4188.974  4207.051  4223.718  4162.436  4208.077  4190.641   \n",
       "\n",
       "          7         8         9        10        11        12        13  \\\n",
       "0  4195.769  4184.231  4253.846  4195.000  4185.769  4188.462  4171.282   \n",
       "1  4184.103  4177.436  4245.385  4189.359  4178.205  4178.718  4167.436   \n",
       "2  4167.308  4164.487  4234.872  4178.974  4163.333  4163.462  4157.949   \n",
       "3  4176.026  4172.436  4233.974  4191.667  4179.487  4176.667  4174.103   \n",
       "4  4183.205  4175.128  4235.513  4201.923  4194.487  4189.359  4192.436   \n",
       "\n",
       "   sample_num  label  \n",
       "0           0      1  \n",
       "1           0      1  \n",
       "2           0      1  \n",
       "3           0      1  \n",
       "4           0      1  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appr 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2800, 56)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\Diplom_New\\lib\\site-packages\\pandas\\core\\generic.py:3936: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">0</th>\n",
       "      <th colspan=\"4\" halign=\"left\">1</th>\n",
       "      <th colspan=\"2\" halign=\"left\">2</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">11</th>\n",
       "      <th colspan=\"4\" halign=\"left\">12</th>\n",
       "      <th colspan=\"4\" halign=\"left\">13</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>skew</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>skew</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>...</th>\n",
       "      <th>std</th>\n",
       "      <th>skew</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>skew</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>skew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>422535.386</td>\n",
       "      <td>4225.35386</td>\n",
       "      <td>22.943132</td>\n",
       "      <td>0.623300</td>\n",
       "      <td>416386.148</td>\n",
       "      <td>4163.86148</td>\n",
       "      <td>23.646534</td>\n",
       "      <td>1.653921</td>\n",
       "      <td>420429.743</td>\n",
       "      <td>4204.29743</td>\n",
       "      <td>...</td>\n",
       "      <td>12.986948</td>\n",
       "      <td>0.292322</td>\n",
       "      <td>418407.820</td>\n",
       "      <td>4184.07820</td>\n",
       "      <td>11.716692</td>\n",
       "      <td>-0.322284</td>\n",
       "      <td>420480.388</td>\n",
       "      <td>4204.80388</td>\n",
       "      <td>15.128111</td>\n",
       "      <td>-0.529249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>418849.104</td>\n",
       "      <td>4188.49104</td>\n",
       "      <td>13.141999</td>\n",
       "      <td>1.685818</td>\n",
       "      <td>417558.720</td>\n",
       "      <td>4175.58720</td>\n",
       "      <td>17.344086</td>\n",
       "      <td>1.499177</td>\n",
       "      <td>421727.564</td>\n",
       "      <td>4217.27564</td>\n",
       "      <td>...</td>\n",
       "      <td>11.527668</td>\n",
       "      <td>0.203475</td>\n",
       "      <td>420292.567</td>\n",
       "      <td>4202.92567</td>\n",
       "      <td>14.340586</td>\n",
       "      <td>-0.456426</td>\n",
       "      <td>420625.897</td>\n",
       "      <td>4206.25897</td>\n",
       "      <td>10.324266</td>\n",
       "      <td>0.318849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>418708.458</td>\n",
       "      <td>4187.08458</td>\n",
       "      <td>10.480366</td>\n",
       "      <td>1.143462</td>\n",
       "      <td>421533.199</td>\n",
       "      <td>4215.33199</td>\n",
       "      <td>22.858908</td>\n",
       "      <td>1.664043</td>\n",
       "      <td>424451.155</td>\n",
       "      <td>4244.51155</td>\n",
       "      <td>...</td>\n",
       "      <td>12.861367</td>\n",
       "      <td>-0.178935</td>\n",
       "      <td>421826.160</td>\n",
       "      <td>4218.26160</td>\n",
       "      <td>13.122579</td>\n",
       "      <td>0.219388</td>\n",
       "      <td>421802.691</td>\n",
       "      <td>4218.02691</td>\n",
       "      <td>11.011654</td>\n",
       "      <td>0.010080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>419694.230</td>\n",
       "      <td>4196.94230</td>\n",
       "      <td>12.200193</td>\n",
       "      <td>0.852128</td>\n",
       "      <td>420814.359</td>\n",
       "      <td>4208.14359</td>\n",
       "      <td>29.285325</td>\n",
       "      <td>1.635677</td>\n",
       "      <td>420965.128</td>\n",
       "      <td>4209.65128</td>\n",
       "      <td>...</td>\n",
       "      <td>13.132305</td>\n",
       "      <td>0.416484</td>\n",
       "      <td>421877.049</td>\n",
       "      <td>4218.77049</td>\n",
       "      <td>12.508268</td>\n",
       "      <td>-0.361629</td>\n",
       "      <td>421668.079</td>\n",
       "      <td>4216.68079</td>\n",
       "      <td>13.883749</td>\n",
       "      <td>0.490287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>420775.134</td>\n",
       "      <td>4207.75134</td>\n",
       "      <td>17.454689</td>\n",
       "      <td>0.925080</td>\n",
       "      <td>420168.850</td>\n",
       "      <td>4201.68850</td>\n",
       "      <td>34.312835</td>\n",
       "      <td>0.816006</td>\n",
       "      <td>420311.407</td>\n",
       "      <td>4203.11407</td>\n",
       "      <td>...</td>\n",
       "      <td>12.525939</td>\n",
       "      <td>0.735080</td>\n",
       "      <td>422885.895</td>\n",
       "      <td>4228.85895</td>\n",
       "      <td>14.110537</td>\n",
       "      <td>0.158259</td>\n",
       "      <td>422612.820</td>\n",
       "      <td>4226.12820</td>\n",
       "      <td>16.676755</td>\n",
       "      <td>0.749659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0                                            1              \\\n",
       "          sum        mean        std      skew         sum        mean   \n",
       "0  422535.386  4225.35386  22.943132  0.623300  416386.148  4163.86148   \n",
       "1  418849.104  4188.49104  13.141999  1.685818  417558.720  4175.58720   \n",
       "2  418708.458  4187.08458  10.480366  1.143462  421533.199  4215.33199   \n",
       "3  419694.230  4196.94230  12.200193  0.852128  420814.359  4208.14359   \n",
       "4  420775.134  4207.75134  17.454689  0.925080  420168.850  4201.68850   \n",
       "\n",
       "                                 2              ...         11            \\\n",
       "         std      skew         sum        mean  ...        std      skew   \n",
       "0  23.646534  1.653921  420429.743  4204.29743  ...  12.986948  0.292322   \n",
       "1  17.344086  1.499177  421727.564  4217.27564  ...  11.527668  0.203475   \n",
       "2  22.858908  1.664043  424451.155  4244.51155  ...  12.861367 -0.178935   \n",
       "3  29.285325  1.635677  420965.128  4209.65128  ...  13.132305  0.416484   \n",
       "4  34.312835  0.816006  420311.407  4203.11407  ...  12.525939  0.735080   \n",
       "\n",
       "           12                                           13              \\\n",
       "          sum        mean        std      skew         sum        mean   \n",
       "0  418407.820  4184.07820  11.716692 -0.322284  420480.388  4204.80388   \n",
       "1  420292.567  4202.92567  14.340586 -0.456426  420625.897  4206.25897   \n",
       "2  421826.160  4218.26160  13.122579  0.219388  421802.691  4218.02691   \n",
       "3  421877.049  4218.77049  12.508268 -0.361629  421668.079  4216.68079   \n",
       "4  422885.895  4228.85895  14.110537  0.158259  422612.820  4226.12820   \n",
       "\n",
       "                        \n",
       "         std      skew  \n",
       "0  15.128111 -0.529249  \n",
       "1  10.324266  0.318849  \n",
       "2  11.011654  0.010080  \n",
       "3  13.883749  0.490287  \n",
       "4  16.676755  0.749659  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X = df.groupby(['sample_num']).agg(['min','max','sum','mean','std','skew']).reset_index().drop(columns=['gesture', 'sample_num'])\n",
    "X = df.groupby(['sample_num']).agg(['sum','mean','std','skew']).reset_index().drop(columns=['label', 'sample_num'])\n",
    "print(X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2800, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.121036</td>\n",
       "      <td>2.321952</td>\n",
       "      <td>-0.449738</td>\n",
       "      <td>-0.208740</td>\n",
       "      <td>-0.227091</td>\n",
       "      <td>0.275026</td>\n",
       "      <td>-0.767402</td>\n",
       "      <td>-0.462860</td>\n",
       "      <td>-0.670035</td>\n",
       "      <td>-0.840695</td>\n",
       "      <td>-0.353871</td>\n",
       "      <td>-0.179143</td>\n",
       "      <td>-0.068469</td>\n",
       "      <td>0.495747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.223837</td>\n",
       "      <td>2.953703</td>\n",
       "      <td>-1.221506</td>\n",
       "      <td>4.035890</td>\n",
       "      <td>-0.368210</td>\n",
       "      <td>-0.002820</td>\n",
       "      <td>-0.108777</td>\n",
       "      <td>0.341003</td>\n",
       "      <td>0.693150</td>\n",
       "      <td>-0.470444</td>\n",
       "      <td>-0.553766</td>\n",
       "      <td>-0.540069</td>\n",
       "      <td>0.111163</td>\n",
       "      <td>-0.207836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.358433</td>\n",
       "      <td>2.536829</td>\n",
       "      <td>-1.009442</td>\n",
       "      <td>1.128803</td>\n",
       "      <td>-0.330908</td>\n",
       "      <td>1.005082</td>\n",
       "      <td>0.304864</td>\n",
       "      <td>1.431213</td>\n",
       "      <td>0.666686</td>\n",
       "      <td>0.131994</td>\n",
       "      <td>0.009822</td>\n",
       "      <td>-0.552262</td>\n",
       "      <td>-0.134105</td>\n",
       "      <td>0.334541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.134144</td>\n",
       "      <td>1.895580</td>\n",
       "      <td>-0.945230</td>\n",
       "      <td>1.142032</td>\n",
       "      <td>-0.253959</td>\n",
       "      <td>-0.512105</td>\n",
       "      <td>-0.366236</td>\n",
       "      <td>-0.418871</td>\n",
       "      <td>-0.521497</td>\n",
       "      <td>-0.649155</td>\n",
       "      <td>-0.303190</td>\n",
       "      <td>-0.067776</td>\n",
       "      <td>-0.160889</td>\n",
       "      <td>-0.365442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.103128</td>\n",
       "      <td>-0.360645</td>\n",
       "      <td>1.008207</td>\n",
       "      <td>0.113093</td>\n",
       "      <td>-0.321950</td>\n",
       "      <td>0.160976</td>\n",
       "      <td>-0.945195</td>\n",
       "      <td>0.057205</td>\n",
       "      <td>-0.353816</td>\n",
       "      <td>-0.898919</td>\n",
       "      <td>-0.475137</td>\n",
       "      <td>0.009528</td>\n",
       "      <td>-0.450036</td>\n",
       "      <td>-0.176413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.121036  2.321952 -0.449738 -0.208740 -0.227091  0.275026 -0.767402   \n",
       "1  3.223837  2.953703 -1.221506  4.035890 -0.368210 -0.002820 -0.108777   \n",
       "2  1.358433  2.536829 -1.009442  1.128803 -0.330908  1.005082  0.304864   \n",
       "3 -0.134144  1.895580 -0.945230  1.142032 -0.253959 -0.512105 -0.366236   \n",
       "4 -0.103128 -0.360645  1.008207  0.113093 -0.321950  0.160976 -0.945195   \n",
       "\n",
       "          7         8         9        10        11        12        13  \n",
       "0 -0.462860 -0.670035 -0.840695 -0.353871 -0.179143 -0.068469  0.495747  \n",
       "1  0.341003  0.693150 -0.470444 -0.553766 -0.540069  0.111163 -0.207836  \n",
       "2  1.431213  0.666686  0.131994  0.009822 -0.552262 -0.134105  0.334541  \n",
       "3 -0.418871 -0.521497 -0.649155 -0.303190 -0.067776 -0.160889 -0.365442  \n",
       "4  0.057205 -0.353816 -0.898919 -0.475137  0.009528 -0.450036 -0.176413  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_kurt = df.groupby(['sample_num']).apply(pd.DataFrame.kurt).drop(columns =['sample_num', 'label'])\n",
    "X_kurt = X_kurt.reset_index().drop(columns = ['sample_num'])\n",
    "print(X_kurt.shape)\n",
    "X_kurt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(0, sum)</th>\n",
       "      <th>(0, mean)</th>\n",
       "      <th>(0, std)</th>\n",
       "      <th>(0, skew)</th>\n",
       "      <th>(1, sum)</th>\n",
       "      <th>(1, mean)</th>\n",
       "      <th>(1, std)</th>\n",
       "      <th>(1, skew)</th>\n",
       "      <th>(2, sum)</th>\n",
       "      <th>(2, mean)</th>\n",
       "      <th>...</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>422535.386</td>\n",
       "      <td>4225.35386</td>\n",
       "      <td>22.943132</td>\n",
       "      <td>0.623300</td>\n",
       "      <td>416386.148</td>\n",
       "      <td>4163.86148</td>\n",
       "      <td>23.646534</td>\n",
       "      <td>1.653921</td>\n",
       "      <td>420429.743</td>\n",
       "      <td>4204.29743</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227091</td>\n",
       "      <td>0.275026</td>\n",
       "      <td>-0.767402</td>\n",
       "      <td>-0.462860</td>\n",
       "      <td>-0.670035</td>\n",
       "      <td>-0.840695</td>\n",
       "      <td>-0.353871</td>\n",
       "      <td>-0.179143</td>\n",
       "      <td>-0.068469</td>\n",
       "      <td>0.495747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>418849.104</td>\n",
       "      <td>4188.49104</td>\n",
       "      <td>13.141999</td>\n",
       "      <td>1.685818</td>\n",
       "      <td>417558.720</td>\n",
       "      <td>4175.58720</td>\n",
       "      <td>17.344086</td>\n",
       "      <td>1.499177</td>\n",
       "      <td>421727.564</td>\n",
       "      <td>4217.27564</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.368210</td>\n",
       "      <td>-0.002820</td>\n",
       "      <td>-0.108777</td>\n",
       "      <td>0.341003</td>\n",
       "      <td>0.693150</td>\n",
       "      <td>-0.470444</td>\n",
       "      <td>-0.553766</td>\n",
       "      <td>-0.540069</td>\n",
       "      <td>0.111163</td>\n",
       "      <td>-0.207836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>418708.458</td>\n",
       "      <td>4187.08458</td>\n",
       "      <td>10.480366</td>\n",
       "      <td>1.143462</td>\n",
       "      <td>421533.199</td>\n",
       "      <td>4215.33199</td>\n",
       "      <td>22.858908</td>\n",
       "      <td>1.664043</td>\n",
       "      <td>424451.155</td>\n",
       "      <td>4244.51155</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.330908</td>\n",
       "      <td>1.005082</td>\n",
       "      <td>0.304864</td>\n",
       "      <td>1.431213</td>\n",
       "      <td>0.666686</td>\n",
       "      <td>0.131994</td>\n",
       "      <td>0.009822</td>\n",
       "      <td>-0.552262</td>\n",
       "      <td>-0.134105</td>\n",
       "      <td>0.334541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>419694.230</td>\n",
       "      <td>4196.94230</td>\n",
       "      <td>12.200193</td>\n",
       "      <td>0.852128</td>\n",
       "      <td>420814.359</td>\n",
       "      <td>4208.14359</td>\n",
       "      <td>29.285325</td>\n",
       "      <td>1.635677</td>\n",
       "      <td>420965.128</td>\n",
       "      <td>4209.65128</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.253959</td>\n",
       "      <td>-0.512105</td>\n",
       "      <td>-0.366236</td>\n",
       "      <td>-0.418871</td>\n",
       "      <td>-0.521497</td>\n",
       "      <td>-0.649155</td>\n",
       "      <td>-0.303190</td>\n",
       "      <td>-0.067776</td>\n",
       "      <td>-0.160889</td>\n",
       "      <td>-0.365442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>420775.134</td>\n",
       "      <td>4207.75134</td>\n",
       "      <td>17.454689</td>\n",
       "      <td>0.925080</td>\n",
       "      <td>420168.850</td>\n",
       "      <td>4201.68850</td>\n",
       "      <td>34.312835</td>\n",
       "      <td>0.816006</td>\n",
       "      <td>420311.407</td>\n",
       "      <td>4203.11407</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.321950</td>\n",
       "      <td>0.160976</td>\n",
       "      <td>-0.945195</td>\n",
       "      <td>0.057205</td>\n",
       "      <td>-0.353816</td>\n",
       "      <td>-0.898919</td>\n",
       "      <td>-0.475137</td>\n",
       "      <td>0.009528</td>\n",
       "      <td>-0.450036</td>\n",
       "      <td>-0.176413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     (0, sum)   (0, mean)   (0, std)  (0, skew)    (1, sum)   (1, mean)  \\\n",
       "0  422535.386  4225.35386  22.943132   0.623300  416386.148  4163.86148   \n",
       "1  418849.104  4188.49104  13.141999   1.685818  417558.720  4175.58720   \n",
       "2  418708.458  4187.08458  10.480366   1.143462  421533.199  4215.33199   \n",
       "3  419694.230  4196.94230  12.200193   0.852128  420814.359  4208.14359   \n",
       "4  420775.134  4207.75134  17.454689   0.925080  420168.850  4201.68850   \n",
       "\n",
       "    (1, std)  (1, skew)    (2, sum)   (2, mean)  ...         4         5  \\\n",
       "0  23.646534   1.653921  420429.743  4204.29743  ... -0.227091  0.275026   \n",
       "1  17.344086   1.499177  421727.564  4217.27564  ... -0.368210 -0.002820   \n",
       "2  22.858908   1.664043  424451.155  4244.51155  ... -0.330908  1.005082   \n",
       "3  29.285325   1.635677  420965.128  4209.65128  ... -0.253959 -0.512105   \n",
       "4  34.312835   0.816006  420311.407  4203.11407  ... -0.321950  0.160976   \n",
       "\n",
       "          6         7         8         9        10        11        12  \\\n",
       "0 -0.767402 -0.462860 -0.670035 -0.840695 -0.353871 -0.179143 -0.068469   \n",
       "1 -0.108777  0.341003  0.693150 -0.470444 -0.553766 -0.540069  0.111163   \n",
       "2  0.304864  1.431213  0.666686  0.131994  0.009822 -0.552262 -0.134105   \n",
       "3 -0.366236 -0.418871 -0.521497 -0.649155 -0.303190 -0.067776 -0.160889   \n",
       "4 -0.945195  0.057205 -0.353816 -0.898919 -0.475137  0.009528 -0.450036   \n",
       "\n",
       "         13  \n",
       "0  0.495747  \n",
       "1 -0.207836  \n",
       "2  0.334541  \n",
       "3 -0.365442  \n",
       "4 -0.176413  \n",
       "\n",
       "[5 rows x 70 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.concat([X, X_kurt], axis = 1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data.groupby(['sample_num']).agg(['min','max','sum','mean','std']).reset_index()['label']['min']\n",
    "y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 70) (400, 70)\n"
     ]
    }
   ],
   "source": [
    "X_train = X.iloc[:2400,:]\n",
    "X_test = X.iloc[2400:,:]\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400,) (400,)\n"
     ]
    }
   ],
   "source": [
    "y_train = y.iloc[:2400]\n",
    "y_test = y.iloc[2400:]\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X,y = shuffle(X, y, random_state=0)\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2520 280\n"
     ]
    }
   ],
   "source": [
    "#train_size = int(len(X) * 0.9)\n",
    "#test_size = len(X) - train_size\n",
    "#train, test = X.iloc[0:train_size], X.iloc[train_size:len(X)]\n",
    "#print(train_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 70) (400, 70)\n"
     ]
    }
   ],
   "source": [
    "#scaler = StandardScaler()\n",
    "#X_train = scaler.fit_transform(train)\n",
    "#X_test = scaler.transform(test)\n",
    "#print(X_train.shape, X_test.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(scaler, open('pred_dir\\\\scaler_new.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2520,) (280,)\n"
     ]
    }
   ],
   "source": [
    "y_train = y.iloc[0:train_size]\n",
    "y_test = y.iloc[train_size:len(y)]\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 512)               36352     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 36,865\n",
      "Trainable params: 36,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation = 'relu', input_shape = (X_train.shape[1], )))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 512)               36352     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 36,865\n",
      "Trainable params: 36,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2280 samples, validate on 120 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 0.6940 - accuracy: 0.5623 - val_loss: 0.6292 - val_accuracy: 0.6750\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.62917, saving model to MLP.weights.best.hdf5\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.6473 - accuracy: 0.6167 - val_loss: 0.6094 - val_accuracy: 0.7000\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.62917 to 0.60940, saving model to MLP.weights.best.hdf5\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.6255 - accuracy: 0.6461 - val_loss: 0.6364 - val_accuracy: 0.5833\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.60940\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.6069 - accuracy: 0.6789 - val_loss: 0.6316 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.60940\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.5903 - accuracy: 0.6789 - val_loss: 0.6247 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.60940\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.5686 - accuracy: 0.7057 - val_loss: 0.6156 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.60940\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.5548 - accuracy: 0.7259 - val_loss: 0.6114 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.60940\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.5388 - accuracy: 0.7364 - val_loss: 0.6312 - val_accuracy: 0.6250\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.60940\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.5239 - accuracy: 0.7496 - val_loss: 0.5954 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.60940 to 0.59545, saving model to MLP.weights.best.hdf5\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.5206 - accuracy: 0.7531 - val_loss: 0.6356 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.59545\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.4976 - accuracy: 0.7759 - val_loss: 0.6782 - val_accuracy: 0.6167\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.59545\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.4943 - accuracy: 0.7711 - val_loss: 0.6110 - val_accuracy: 0.6417\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.59545\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.4690 - accuracy: 0.7917 - val_loss: 0.5902 - val_accuracy: 0.6750\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.59545 to 0.59021, saving model to MLP.weights.best.hdf5\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.4651 - accuracy: 0.7877 - val_loss: 0.6162 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.59021\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.4419 - accuracy: 0.8105 - val_loss: 0.6450 - val_accuracy: 0.6167\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.59021\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.4273 - accuracy: 0.8184 - val_loss: 0.6082 - val_accuracy: 0.6250\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.59021\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.4123 - accuracy: 0.8285 - val_loss: 0.6004 - val_accuracy: 0.6583\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.59021\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.4072 - accuracy: 0.8316 - val_loss: 0.6173 - val_accuracy: 0.6583\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.59021\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.3868 - accuracy: 0.8482 - val_loss: 0.7119 - val_accuracy: 0.5833\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.59021\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.3764 - accuracy: 0.8539 - val_loss: 0.6211 - val_accuracy: 0.6417\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.59021\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.3638 - accuracy: 0.8526 - val_loss: 0.6623 - val_accuracy: 0.6250\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.59021\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.3585 - accuracy: 0.8592 - val_loss: 0.6055 - val_accuracy: 0.6167\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.59021\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.3420 - accuracy: 0.8785 - val_loss: 0.6097 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.59021\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.3319 - accuracy: 0.8816 - val_loss: 0.6245 - val_accuracy: 0.6583\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.59021\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.3132 - accuracy: 0.8939 - val_loss: 0.6224 - val_accuracy: 0.6833\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.59021\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.3105 - accuracy: 0.8851 - val_loss: 0.6497 - val_accuracy: 0.6417\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.59021\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.2944 - accuracy: 0.9053 - val_loss: 0.6004 - val_accuracy: 0.6750\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.59021\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.2935 - accuracy: 0.8947 - val_loss: 0.7027 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.59021\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.2783 - accuracy: 0.9175 - val_loss: 0.6464 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.59021\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.2744 - accuracy: 0.9053 - val_loss: 0.6684 - val_accuracy: 0.6833\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.59021\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.2607 - accuracy: 0.9158 - val_loss: 0.7405 - val_accuracy: 0.6250\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.59021\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.2483 - accuracy: 0.9241 - val_loss: 0.6379 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.59021\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.2638 - accuracy: 0.9127 - val_loss: 0.6665 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.59021\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.2876 - accuracy: 0.9215 - val_loss: 0.7608 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.59021\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.2386 - accuracy: 0.9294 - val_loss: 0.6612 - val_accuracy: 0.6750\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.59021\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.2189 - accuracy: 0.9425 - val_loss: 0.6930 - val_accuracy: 0.6333\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.59021\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.2169 - accuracy: 0.9395 - val_loss: 0.6859 - val_accuracy: 0.6583\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.59021\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.2062 - accuracy: 0.9421 - val_loss: 0.6366 - val_accuracy: 0.6833\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.59021\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.1976 - accuracy: 0.9487 - val_loss: 0.7593 - val_accuracy: 0.6417\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.59021\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.1891 - accuracy: 0.9557 - val_loss: 0.6803 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.59021\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.1798 - accuracy: 0.9583 - val_loss: 0.6728 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.59021\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.1742 - accuracy: 0.9627 - val_loss: 0.7281 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.59021\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.1704 - accuracy: 0.9605 - val_loss: 0.6721 - val_accuracy: 0.6833\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.59021\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.1636 - accuracy: 0.9662 - val_loss: 0.7027 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.59021\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.1526 - accuracy: 0.9671 - val_loss: 0.6846 - val_accuracy: 0.7167\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.59021\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.1525 - accuracy: 0.9732 - val_loss: 0.7262 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.59021\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.1454 - accuracy: 0.9746 - val_loss: 0.7232 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.59021\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.1380 - accuracy: 0.9772 - val_loss: 0.7119 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.59021\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.1298 - accuracy: 0.9820 - val_loss: 0.7377 - val_accuracy: 0.6583\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.59021\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.1283 - accuracy: 0.9798 - val_loss: 0.7701 - val_accuracy: 0.6750\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.59021\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.1255 - accuracy: 0.9820 - val_loss: 0.7674 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.59021\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.1190 - accuracy: 0.9838 - val_loss: 0.8217 - val_accuracy: 0.6583\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.59021\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.1148 - accuracy: 0.9811 - val_loss: 0.7535 - val_accuracy: 0.6583\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.59021\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.1141 - accuracy: 0.9855 - val_loss: 0.9424 - val_accuracy: 0.6583\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.59021\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.2190 - accuracy: 0.9487 - val_loss: 0.8974 - val_accuracy: 0.6583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00055: val_loss did not improve from 0.59021\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.1488 - accuracy: 0.9632 - val_loss: 0.9127 - val_accuracy: 0.6333\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.59021\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.1322 - accuracy: 0.9724 - val_loss: 0.8160 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.59021\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.1040 - accuracy: 0.9846 - val_loss: 0.8295 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.59021\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.1483 - accuracy: 0.9768 - val_loss: 0.9471 - val_accuracy: 0.6833\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.59021\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.2175 - accuracy: 0.9504 - val_loss: 0.9389 - val_accuracy: 0.6833\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.59021\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.0995 - accuracy: 0.9873 - val_loss: 0.8586 - val_accuracy: 0.6750\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.59021\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.0893 - accuracy: 0.9939 - val_loss: 0.8102 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.59021\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.0839 - accuracy: 0.9939 - val_loss: 0.8219 - val_accuracy: 0.6750\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.59021\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.0821 - accuracy: 0.9939 - val_loss: 0.8501 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.59021\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.0773 - accuracy: 0.9934 - val_loss: 0.8630 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.59021\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.0754 - accuracy: 0.9961 - val_loss: 0.8690 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.59021\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.0729 - accuracy: 0.9947 - val_loss: 0.8379 - val_accuracy: 0.6417\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.59021\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.0709 - accuracy: 0.9943 - val_loss: 0.8525 - val_accuracy: 0.7000\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.59021\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.0680 - accuracy: 0.9965 - val_loss: 0.9097 - val_accuracy: 0.6583\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.59021\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.0650 - accuracy: 0.9969 - val_loss: 0.8779 - val_accuracy: 0.6750\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.59021\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.0627 - accuracy: 0.9978 - val_loss: 0.8200 - val_accuracy: 0.6833\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.59021\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.0612 - accuracy: 0.9974 - val_loss: 0.8976 - val_accuracy: 0.6583\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.59021\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.0580 - accuracy: 0.9987 - val_loss: 0.8995 - val_accuracy: 0.6417\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.59021\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.0568 - accuracy: 0.9991 - val_loss: 0.9413 - val_accuracy: 0.6333\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.59021\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.0538 - accuracy: 0.9991 - val_loss: 0.8913 - val_accuracy: 0.6583\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.59021\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.0522 - accuracy: 0.9991 - val_loss: 0.9150 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.59021\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.0510 - accuracy: 0.9991 - val_loss: 0.9145 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.59021\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.0494 - accuracy: 0.9991 - val_loss: 0.9222 - val_accuracy: 0.6750\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.59021\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.0476 - accuracy: 0.9991 - val_loss: 0.9548 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.59021\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.0448 - accuracy: 1.0000 - val_loss: 0.9321 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.59021\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.0436 - accuracy: 1.0000 - val_loss: 0.9883 - val_accuracy: 0.6583\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.59021\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.0426 - accuracy: 1.0000 - val_loss: 0.9127 - val_accuracy: 0.6583\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.59021\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.0940 - accuracy: 0.9846 - val_loss: 1.0672 - val_accuracy: 0.6833\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.59021\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.1443 - accuracy: 0.9689 - val_loss: 1.0101 - val_accuracy: 0.6917\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.59021\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.0831 - accuracy: 0.9873 - val_loss: 1.0659 - val_accuracy: 0.6417\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.59021\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.0701 - accuracy: 0.9921 - val_loss: 0.9768 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.59021\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.0478 - accuracy: 0.9978 - val_loss: 0.9148 - val_accuracy: 0.7083\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.59021\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.0409 - accuracy: 0.9996 - val_loss: 0.9439 - val_accuracy: 0.7167\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.59021\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.0383 - accuracy: 1.0000 - val_loss: 0.9775 - val_accuracy: 0.6750\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.59021\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.0370 - accuracy: 0.9996 - val_loss: 0.9927 - val_accuracy: 0.7000\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.59021\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.0347 - accuracy: 1.0000 - val_loss: 0.9747 - val_accuracy: 0.6917\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.59021\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.0329 - accuracy: 1.0000 - val_loss: 0.9922 - val_accuracy: 0.7167\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.59021\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.0329 - accuracy: 1.0000 - val_loss: 0.9676 - val_accuracy: 0.6917\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.59021\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.0321 - accuracy: 1.0000 - val_loss: 0.9853 - val_accuracy: 0.7000\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.59021\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.0297 - accuracy: 1.0000 - val_loss: 1.0030 - val_accuracy: 0.6917\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.59021\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.0293 - accuracy: 1.0000 - val_loss: 1.0084 - val_accuracy: 0.6917\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.59021\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.0283 - accuracy: 1.0000 - val_loss: 0.9792 - val_accuracy: 0.7167\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.59021\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.0275 - accuracy: 1.0000 - val_loss: 1.0184 - val_accuracy: 0.6750\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.59021\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.0263 - accuracy: 1.0000 - val_loss: 1.0466 - val_accuracy: 0.6750\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.59021\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.0264 - accuracy: 1.0000 - val_loss: 1.0386 - val_accuracy: 0.7083\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.59021\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath = 'MLP.weights.best.hdf5', verbose = 1, save_best_only = True)\n",
    "hist = model.fit(X_train, y_train, epochs = 100, batch_size=32, validation_split = 0.05, callbacks = [checkpointer], verbose = 2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 0s 38us/step\n",
      "Accuracy:  0.5625\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 45us/step\n",
      "Accuracy:  0.46000000834465027\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test[:200, :], y_test[:200], verbose=1)\n",
    "print(\"Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 45us/step\n",
      "Accuracy:  0.6650000214576721\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test[200:400, :], y_test[200:400], verbose=1)\n",
    "print(\"Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280/280 [==============================] - 0s 39us/step\n",
      "Accuracy:  0.6321428418159485\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict2 = [1 if a>0.5 else 0 for a in model.predict(X_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6321428571428571\n",
      "Precision = 0.6293706293706294\n",
      "Recall = 0.6428571428571429\n",
      "F1 Score = 0.6360424028268551\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = {}\\nPrecision = {}\\nRecall = {}\\nF1 Score = {}\".format(metrics.accuracy_score(y_test, predict2), \n",
    "                                                                         metrics.precision_score(y_test, predict2),\n",
    "                                                                         metrics.recall_score(y_test, predict2),\n",
    "                                                                         metrics.f1_score(y_test, predict2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 1000)              71000     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 1001      \n",
      "=================================================================\n",
      "Total params: 2,074,001\n",
      "Trainable params: 2,074,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(1000, activation = 'relu', input_shape = (X_train.shape[1], )))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(1000, activation = 'relu'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(1000, activation = 'relu'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(1, activation = 'sigmoid'))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 1000)              71000     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 1001      \n",
      "=================================================================\n",
      "Total params: 2,074,001\n",
      "Trainable params: 2,074,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2280 samples, validate on 120 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 0.7139 - accuracy: 0.5390 - val_loss: 0.6580 - val_accuracy: 0.7000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65801, saving model to MLP_new.weights.best.hdf5\n",
      "Epoch 2/100\n",
      " - 3s - loss: 0.6551 - accuracy: 0.6127 - val_loss: 0.6358 - val_accuracy: 0.5833\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.65801 to 0.63577, saving model to MLP_new.weights.best.hdf5\n",
      "Epoch 3/100\n",
      " - 3s - loss: 0.6472 - accuracy: 0.6241 - val_loss: 0.6505 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.63577\n",
      "Epoch 4/100\n",
      " - 3s - loss: 0.6126 - accuracy: 0.6592 - val_loss: 0.7113 - val_accuracy: 0.6917\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.63577\n",
      "Epoch 5/100\n",
      " - 3s - loss: 0.5848 - accuracy: 0.6886 - val_loss: 0.6834 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.63577\n",
      "Epoch 6/100\n",
      " - 3s - loss: 0.5695 - accuracy: 0.7031 - val_loss: 0.7013 - val_accuracy: 0.5833\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.63577\n",
      "Epoch 7/100\n",
      " - 3s - loss: 0.5107 - accuracy: 0.7399 - val_loss: 0.6514 - val_accuracy: 0.6333\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.63577\n",
      "Epoch 8/100\n",
      " - 4s - loss: 0.4827 - accuracy: 0.7614 - val_loss: 0.6984 - val_accuracy: 0.5833\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.63577\n",
      "Epoch 9/100\n",
      " - 4s - loss: 0.4592 - accuracy: 0.7719 - val_loss: 0.6595 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.63577\n",
      "Epoch 10/100\n",
      " - 3s - loss: 0.4419 - accuracy: 0.7943 - val_loss: 0.7568 - val_accuracy: 0.6250\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.63577\n",
      "Epoch 11/100\n",
      " - 4s - loss: 0.4186 - accuracy: 0.8110 - val_loss: 0.6102 - val_accuracy: 0.6833\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.63577 to 0.61021, saving model to MLP_new.weights.best.hdf5\n",
      "Epoch 12/100\n",
      " - 4s - loss: 0.3535 - accuracy: 0.8346 - val_loss: 0.8249 - val_accuracy: 0.6333\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.61021\n",
      "Epoch 13/100\n",
      " - 4s - loss: 0.3339 - accuracy: 0.8623 - val_loss: 0.7598 - val_accuracy: 0.6833\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.61021\n",
      "Epoch 14/100\n",
      " - 4s - loss: 0.2731 - accuracy: 0.8781 - val_loss: 0.9021 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.61021\n",
      "Epoch 15/100\n",
      " - 4s - loss: 0.2578 - accuracy: 0.9000 - val_loss: 0.8868 - val_accuracy: 0.6417\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.61021\n",
      "Epoch 16/100\n",
      " - 4s - loss: 0.2056 - accuracy: 0.9075 - val_loss: 0.8503 - val_accuracy: 0.6833\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.61021\n",
      "Epoch 17/100\n",
      " - 3s - loss: 0.2090 - accuracy: 0.9158 - val_loss: 0.8957 - val_accuracy: 0.6583\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.61021\n",
      "Epoch 18/100\n",
      " - 4s - loss: 0.1629 - accuracy: 0.9373 - val_loss: 1.1554 - val_accuracy: 0.6083\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.61021\n",
      "Epoch 19/100\n",
      " - 4s - loss: 0.1284 - accuracy: 0.9504 - val_loss: 1.1687 - val_accuracy: 0.6167\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.61021\n",
      "Epoch 20/100\n",
      " - 4s - loss: 0.1237 - accuracy: 0.9544 - val_loss: 1.5817 - val_accuracy: 0.5917\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.61021\n",
      "Epoch 21/100\n",
      " - 4s - loss: 0.1520 - accuracy: 0.9434 - val_loss: 1.1228 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.61021\n",
      "Epoch 22/100\n",
      " - 4s - loss: 0.1438 - accuracy: 0.9518 - val_loss: 1.1642 - val_accuracy: 0.6583\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.61021\n",
      "Epoch 23/100\n",
      " - 3s - loss: 0.1516 - accuracy: 0.9482 - val_loss: 1.3461 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.61021\n",
      "Epoch 24/100\n",
      " - 3s - loss: 0.1044 - accuracy: 0.9610 - val_loss: 1.4179 - val_accuracy: 0.6333\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.61021\n",
      "Epoch 25/100\n",
      " - 4s - loss: 0.0849 - accuracy: 0.9763 - val_loss: 1.6137 - val_accuracy: 0.6083\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.61021\n",
      "Epoch 26/100\n",
      " - 3s - loss: 0.0657 - accuracy: 0.9807 - val_loss: 1.5094 - val_accuracy: 0.6750\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.61021\n",
      "Epoch 27/100\n",
      " - 4s - loss: 0.0849 - accuracy: 0.9728 - val_loss: 1.1150 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.61021\n",
      "Epoch 28/100\n",
      " - 4s - loss: 0.0541 - accuracy: 0.9803 - val_loss: 1.5652 - val_accuracy: 0.6333\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.61021\n",
      "Epoch 29/100\n",
      " - 4s - loss: 0.0625 - accuracy: 0.9785 - val_loss: 1.3005 - val_accuracy: 0.6417\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.61021\n",
      "Epoch 30/100\n",
      " - 4s - loss: 0.0811 - accuracy: 0.9746 - val_loss: 1.3814 - val_accuracy: 0.5833\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.61021\n",
      "Epoch 31/100\n",
      " - 3s - loss: 0.0483 - accuracy: 0.9833 - val_loss: 1.4228 - val_accuracy: 0.6750\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.61021\n",
      "Epoch 32/100\n",
      " - 5s - loss: 0.0521 - accuracy: 0.9838 - val_loss: 1.5623 - val_accuracy: 0.6083\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.61021\n",
      "Epoch 33/100\n",
      " - 4s - loss: 0.0503 - accuracy: 0.9798 - val_loss: 1.6306 - val_accuracy: 0.6417\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.61021\n",
      "Epoch 34/100\n",
      " - 4s - loss: 0.0745 - accuracy: 0.9746 - val_loss: 1.4548 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.61021\n",
      "Epoch 35/100\n",
      " - 3s - loss: 0.0515 - accuracy: 0.9829 - val_loss: 1.5169 - val_accuracy: 0.6833\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.61021\n",
      "Epoch 36/100\n",
      " - 3s - loss: 0.0746 - accuracy: 0.9750 - val_loss: 1.7963 - val_accuracy: 0.5917\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.61021\n",
      "Epoch 37/100\n",
      " - 3s - loss: 0.0318 - accuracy: 0.9899 - val_loss: 1.7870 - val_accuracy: 0.5917\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.61021\n",
      "Epoch 38/100\n",
      " - 3s - loss: 0.0315 - accuracy: 0.9912 - val_loss: 2.0111 - val_accuracy: 0.6250\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.61021\n",
      "Epoch 39/100\n",
      " - 3s - loss: 0.0674 - accuracy: 0.9785 - val_loss: 1.4746 - val_accuracy: 0.6333\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.61021\n",
      "Epoch 40/100\n",
      " - 3s - loss: 0.0351 - accuracy: 0.9886 - val_loss: 1.6651 - val_accuracy: 0.6167\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.61021\n",
      "Epoch 41/100\n",
      " - 3s - loss: 0.0269 - accuracy: 0.9904 - val_loss: 1.8718 - val_accuracy: 0.6083\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.61021\n",
      "Epoch 42/100\n",
      " - 4s - loss: 0.0365 - accuracy: 0.9868 - val_loss: 2.1066 - val_accuracy: 0.5833\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.61021\n",
      "Epoch 43/100\n",
      " - 4s - loss: 0.1188 - accuracy: 0.9724 - val_loss: 1.7890 - val_accuracy: 0.6167\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.61021\n",
      "Epoch 44/100\n",
      " - 3s - loss: 0.0792 - accuracy: 0.9851 - val_loss: 1.4688 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.61021\n",
      "Epoch 45/100\n",
      " - 4s - loss: 0.0439 - accuracy: 0.9855 - val_loss: 1.9231 - val_accuracy: 0.6167\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.61021\n",
      "Epoch 46/100\n",
      " - 4s - loss: 0.1102 - accuracy: 0.9750 - val_loss: 1.5242 - val_accuracy: 0.6083\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.61021\n",
      "Epoch 47/100\n",
      " - 4s - loss: 0.0640 - accuracy: 0.9829 - val_loss: 1.5907 - val_accuracy: 0.5917\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.61021\n",
      "Epoch 48/100\n",
      " - 4s - loss: 0.0263 - accuracy: 0.9934 - val_loss: 1.7475 - val_accuracy: 0.6083\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.61021\n",
      "Epoch 49/100\n",
      " - 4s - loss: 0.0482 - accuracy: 0.9868 - val_loss: 1.8947 - val_accuracy: 0.6250\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.61021\n",
      "Epoch 50/100\n",
      " - 4s - loss: 0.0729 - accuracy: 0.9772 - val_loss: 1.3060 - val_accuracy: 0.6583\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.61021\n",
      "Epoch 51/100\n",
      " - 3s - loss: 0.0336 - accuracy: 0.9895 - val_loss: 1.6169 - val_accuracy: 0.5917\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.61021\n",
      "Epoch 52/100\n",
      " - 3s - loss: 0.0578 - accuracy: 0.9763 - val_loss: 1.7001 - val_accuracy: 0.5583\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.61021\n",
      "Epoch 53/100\n",
      " - 3s - loss: 0.0483 - accuracy: 0.9873 - val_loss: 1.9646 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.61021\n",
      "Epoch 54/100\n",
      " - 3s - loss: 0.0420 - accuracy: 0.9882 - val_loss: 2.2661 - val_accuracy: 0.6333\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.61021\n",
      "Epoch 55/100\n",
      " - 3s - loss: 0.0568 - accuracy: 0.9811 - val_loss: 1.6212 - val_accuracy: 0.6167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00055: val_loss did not improve from 0.61021\n",
      "Epoch 56/100\n",
      " - 4s - loss: 0.0302 - accuracy: 0.9925 - val_loss: 1.4872 - val_accuracy: 0.6333\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.61021\n",
      "Epoch 57/100\n",
      " - 3s - loss: 0.0212 - accuracy: 0.9956 - val_loss: 1.9136 - val_accuracy: 0.6083\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.61021\n",
      "Epoch 58/100\n",
      " - 3s - loss: 0.0102 - accuracy: 0.9969 - val_loss: 2.2319 - val_accuracy: 0.6250\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.61021\n",
      "Epoch 59/100\n",
      " - 3s - loss: 0.0519 - accuracy: 0.9912 - val_loss: 1.9107 - val_accuracy: 0.6167\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.61021\n",
      "Epoch 60/100\n",
      " - 4s - loss: 0.0286 - accuracy: 0.9886 - val_loss: 2.0094 - val_accuracy: 0.6083\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.61021\n",
      "Epoch 61/100\n",
      " - 3s - loss: 0.0306 - accuracy: 0.9886 - val_loss: 1.7359 - val_accuracy: 0.6167\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.61021\n",
      "Epoch 62/100\n",
      " - 3s - loss: 0.0195 - accuracy: 0.9921 - val_loss: 1.9284 - val_accuracy: 0.6083\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.61021\n",
      "Epoch 63/100\n",
      " - 4s - loss: 0.0422 - accuracy: 0.9908 - val_loss: 1.5943 - val_accuracy: 0.6417\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.61021\n",
      "Epoch 64/100\n",
      " - 4s - loss: 0.0190 - accuracy: 0.9925 - val_loss: 1.8920 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.61021\n",
      "Epoch 65/100\n",
      " - 3s - loss: 0.0106 - accuracy: 0.9974 - val_loss: 1.9668 - val_accuracy: 0.6333\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.61021\n",
      "Epoch 66/100\n",
      " - 3s - loss: 0.0164 - accuracy: 0.9952 - val_loss: 1.9917 - val_accuracy: 0.5750\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.61021\n",
      "Epoch 67/100\n",
      " - 3s - loss: 0.0169 - accuracy: 0.9930 - val_loss: 2.0164 - val_accuracy: 0.6250\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.61021\n",
      "Epoch 68/100\n",
      " - 3s - loss: 0.0153 - accuracy: 0.9961 - val_loss: 2.2249 - val_accuracy: 0.5917\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.61021\n",
      "Epoch 69/100\n",
      " - 4s - loss: 0.0311 - accuracy: 0.9912 - val_loss: 1.9983 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.61021\n",
      "Epoch 70/100\n",
      " - 4s - loss: 0.0384 - accuracy: 0.9886 - val_loss: 2.1617 - val_accuracy: 0.6167\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.61021\n",
      "Epoch 71/100\n",
      " - 4s - loss: 0.0446 - accuracy: 0.9864 - val_loss: 1.6682 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.61021\n",
      "Epoch 72/100\n",
      " - 4s - loss: 0.0231 - accuracy: 0.9934 - val_loss: 1.7847 - val_accuracy: 0.6583\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.61021\n",
      "Epoch 73/100\n",
      " - 4s - loss: 0.0320 - accuracy: 0.9890 - val_loss: 1.5913 - val_accuracy: 0.6917\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.61021\n",
      "Epoch 74/100\n",
      " - 4s - loss: 0.0719 - accuracy: 0.9886 - val_loss: 1.8463 - val_accuracy: 0.6167\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.61021\n",
      "Epoch 75/100\n",
      " - 4s - loss: 0.0219 - accuracy: 0.9921 - val_loss: 1.7312 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.61021\n",
      "Epoch 76/100\n",
      " - 4s - loss: 0.0309 - accuracy: 0.9904 - val_loss: 1.7593 - val_accuracy: 0.6833\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.61021\n",
      "Epoch 77/100\n",
      " - 4s - loss: 0.0281 - accuracy: 0.9908 - val_loss: 1.8518 - val_accuracy: 0.6333\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.61021\n",
      "Epoch 78/100\n",
      " - 4s - loss: 0.0168 - accuracy: 0.9939 - val_loss: 2.1416 - val_accuracy: 0.6417\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.61021\n",
      "Epoch 79/100\n",
      " - 4s - loss: 0.0150 - accuracy: 0.9947 - val_loss: 1.9296 - val_accuracy: 0.6417\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.61021\n",
      "Epoch 80/100\n",
      " - 4s - loss: 0.0621 - accuracy: 0.9825 - val_loss: 2.6200 - val_accuracy: 0.5833\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.61021\n",
      "Epoch 81/100\n",
      " - 4s - loss: 0.0632 - accuracy: 0.9904 - val_loss: 1.6550 - val_accuracy: 0.6583\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.61021\n",
      "Epoch 82/100\n",
      " - 4s - loss: 0.0220 - accuracy: 0.9952 - val_loss: 2.0765 - val_accuracy: 0.6167\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.61021\n",
      "Epoch 83/100\n",
      " - 3s - loss: 0.0187 - accuracy: 0.9917 - val_loss: 1.8171 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.61021\n",
      "Epoch 84/100\n",
      " - 3s - loss: 0.0261 - accuracy: 0.9908 - val_loss: 1.7889 - val_accuracy: 0.6583\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.61021\n",
      "Epoch 85/100\n",
      " - 4s - loss: 0.0652 - accuracy: 0.9811 - val_loss: 2.4261 - val_accuracy: 0.6417\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.61021\n",
      "Epoch 86/100\n",
      " - 3s - loss: 0.0769 - accuracy: 0.9877 - val_loss: 1.6392 - val_accuracy: 0.6417\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.61021\n",
      "Epoch 87/100\n",
      " - 3s - loss: 0.0282 - accuracy: 0.9943 - val_loss: 2.1209 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.61021\n",
      "Epoch 88/100\n",
      " - 3s - loss: 0.0417 - accuracy: 0.9904 - val_loss: 2.7267 - val_accuracy: 0.6167\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.61021\n",
      "Epoch 89/100\n",
      " - 3s - loss: 0.0454 - accuracy: 0.9882 - val_loss: 2.3233 - val_accuracy: 0.6083\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.61021\n",
      "Epoch 90/100\n",
      " - 3s - loss: 0.0533 - accuracy: 0.9860 - val_loss: 2.1381 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.61021\n",
      "Epoch 91/100\n",
      " - 3s - loss: 0.0237 - accuracy: 0.9925 - val_loss: 1.8075 - val_accuracy: 0.6583\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.61021\n",
      "Epoch 92/100\n",
      " - 3s - loss: 0.0175 - accuracy: 0.9925 - val_loss: 2.3080 - val_accuracy: 0.6500\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.61021\n",
      "Epoch 93/100\n",
      " - 3s - loss: 0.0335 - accuracy: 0.9904 - val_loss: 2.0473 - val_accuracy: 0.6417\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.61021\n",
      "Epoch 94/100\n",
      " - 3s - loss: 0.0245 - accuracy: 0.9912 - val_loss: 1.8888 - val_accuracy: 0.6583\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.61021\n",
      "Epoch 95/100\n",
      " - 4s - loss: 0.0138 - accuracy: 0.9934 - val_loss: 2.2493 - val_accuracy: 0.6083\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.61021\n",
      "Epoch 96/100\n",
      " - 3s - loss: 0.0171 - accuracy: 0.9943 - val_loss: 2.3633 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.61021\n",
      "Epoch 97/100\n",
      " - 3s - loss: 0.0356 - accuracy: 0.9899 - val_loss: 1.8963 - val_accuracy: 0.6250\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.61021\n",
      "Epoch 98/100\n",
      " - 3s - loss: 0.0231 - accuracy: 0.9930 - val_loss: 1.9160 - val_accuracy: 0.6167\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.61021\n",
      "Epoch 99/100\n",
      " - 4s - loss: 0.0195 - accuracy: 0.9965 - val_loss: 1.9761 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.61021\n",
      "Epoch 100/100\n",
      " - 3s - loss: 0.0341 - accuracy: 0.9904 - val_loss: 2.1303 - val_accuracy: 0.6083\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.61021\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath = 'MLP_new.weights.best.hdf5', verbose = 1, save_best_only = True)\n",
    "hist1 = model2.fit( X_train, y_train, epochs = 100, batch_size=32, validation_split = 0.05, callbacks = [checkpointer], verbose = 2, shuffle = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 0s 190us/step\n",
      "Accuracy:  0.5425000190734863\n"
     ]
    }
   ],
   "source": [
    "score = model2.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 200us/step\n",
      "Accuracy:  0.4749999940395355\n"
     ]
    }
   ],
   "source": [
    "score = model2.evaluate(X_test[:200, :], y_test[:200], verbose=1)\n",
    "print(\"Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 205us/step\n",
      "Accuracy:  0.6100000143051147\n"
     ]
    }
   ],
   "source": [
    "score = model2.evaluate(X_test[200:400, :], y_test[200:400], verbose=1)\n",
    "print(\"Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280/280 [==============================] - 0s 189us/step\n",
      "Accuracy:  0.6035714149475098\n"
     ]
    }
   ],
   "source": [
    "#score = model2.evaluate(X_test, y_test, verbose=1)\n",
    "#print(\"Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict3 = [1 if a>=0.5 else 0 for a in model2.predict(X_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6035714285714285\n",
      "Precision = 0.5935483870967742\n",
      "Recall = 0.6571428571428571\n",
      "F1 Score = 0.623728813559322\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = {}\\nPrecision = {}\\nRecall = {}\\nF1 Score = {}\".format(metrics.accuracy_score(y_test, predict3), \n",
    "                                                                         metrics.precision_score(y_test, predict3),\n",
    "                                                                         metrics.recall_score(y_test, predict3),\n",
    "                                                                         metrics.f1_score(y_test, predict3)))\n",
    "#score_p.append([metrics.accuracy_score(y_test, predict3), metrics.precision_score(y_test, predict3),metrics.recall_score(y_test, predict3),metrics.f1_score(y_test, predict3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_lstm = np.asarray(np.reshape(X_train, (X_train.shape[0], 70, 1)))\n",
    "X_test_lstm = np.asarray(np.reshape(X_test, (X_test.shape[0], 70, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(LSTM(256,input_shape=(70, 1), return_sequences=True))\n",
    "model3.add(LSTM(256))\n",
    "model3.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 70, 256)           264192    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 789,761\n",
      "Trainable params: 789,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2280 samples, validate on 120 samples\n",
      "Epoch 1/100\n",
      " - 43s - loss: 0.6943 - accuracy: 0.4925 - val_loss: 0.6815 - val_accuracy: 0.5417\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.68153, saving model to LSTM.weights.best.hdf5\n",
      "Epoch 2/100\n",
      " - 45s - loss: 0.6902 - accuracy: 0.5307 - val_loss: 0.6847 - val_accuracy: 0.5833\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.68153\n",
      "Epoch 3/100\n",
      " - 44s - loss: 0.6925 - accuracy: 0.5254 - val_loss: 0.6737 - val_accuracy: 0.5750\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.68153 to 0.67373, saving model to LSTM.weights.best.hdf5\n",
      "Epoch 4/100\n",
      " - 45s - loss: 0.6898 - accuracy: 0.5316 - val_loss: 0.6940 - val_accuracy: 0.4500\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.67373\n",
      "Epoch 5/100\n",
      " - 45s - loss: 0.6919 - accuracy: 0.5316 - val_loss: 0.6837 - val_accuracy: 0.5167\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.67373\n",
      "Epoch 6/100\n",
      " - 45s - loss: 0.6878 - accuracy: 0.5373 - val_loss: 0.6812 - val_accuracy: 0.5917\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.67373\n",
      "Epoch 7/100\n",
      " - 46s - loss: 0.6890 - accuracy: 0.5206 - val_loss: 0.6755 - val_accuracy: 0.5917\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.67373\n",
      "Epoch 8/100\n",
      " - 45s - loss: 0.6894 - accuracy: 0.5048 - val_loss: 0.6838 - val_accuracy: 0.4917\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.67373\n",
      "Epoch 9/100\n",
      " - 46s - loss: 0.6898 - accuracy: 0.5325 - val_loss: 0.6744 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.67373\n",
      "Epoch 10/100\n",
      " - 51s - loss: 0.6873 - accuracy: 0.5189 - val_loss: 0.6913 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.67373\n",
      "Epoch 11/100\n",
      " - 52s - loss: 0.6856 - accuracy: 0.5320 - val_loss: 0.6893 - val_accuracy: 0.4833\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.67373\n",
      "Epoch 12/100\n",
      " - 48s - loss: 0.6835 - accuracy: 0.5461 - val_loss: 0.6731 - val_accuracy: 0.4917\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.67373 to 0.67306, saving model to LSTM.weights.best.hdf5\n",
      "Epoch 13/100\n",
      " - 48s - loss: 0.6842 - accuracy: 0.5553 - val_loss: 0.6719 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.67306 to 0.67186, saving model to LSTM.weights.best.hdf5\n",
      "Epoch 14/100\n",
      " - 52s - loss: 0.6901 - accuracy: 0.5338 - val_loss: 0.6923 - val_accuracy: 0.4417\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.67186\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-f9e92a597aa8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcheckpointer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'LSTM.weights.best.hdf5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_best_only\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_lstm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\Diplom_New\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Diplom_New\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Diplom_New\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Diplom_New\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath = 'LSTM.weights.best.hdf5', verbose = 1, save_best_only = True)\n",
    "hist = model3.fit(X_train_lstm, y_train, epochs = 100, batch_size=32, validation_split = 0.05, callbacks = [checkpointer], verbose = 2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 1s 6ms/step\n",
      "Accuracy:  0.47999998927116394\n"
     ]
    }
   ],
   "source": [
    "score = model3.evaluate(X_test_lstm, y_test, verbose=1)\n",
    "print(\"Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict4 =  [1 if a>0.5 else 0 for a in model3.predict(X_test_lstm)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.48\n",
      "Precision = 0.0\n",
      "Recall = 0.0\n",
      "F1 Score = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\Diplom_New\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = {}\\nPrecision = {}\\nRecall = {}\\nF1 Score = {}\".format(metrics.accuracy_score(y_test, predict4), \n",
    "                                                                         metrics.precision_score(y_test, predict4),\n",
    "                                                                         metrics.recall_score(y_test, predict4),\n",
    "                                                                         metrics.f1_score(y_test, predict4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Model 4 (Tunning LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_23 (LSTM)               (None, 98, 256)           264192    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 98, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_24 (LSTM)               (None, 98, 256)           525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 98, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_25 (LSTM)               (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,315,073\n",
      "Trainable params: 1,315,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model5 = Sequential()\n",
    "model5.add(LSTM(256,input_shape=(98, 1), return_sequences=True))\n",
    "model5.add(LeakyReLU(alpha=0.3))\n",
    "model5.add(LSTM(256, return_sequences=True))\n",
    "model5.add(LeakyReLU(alpha=0.3))\n",
    "model5.add(LSTM(256))\n",
    "model5.add(LeakyReLU(alpha=0.3))\n",
    "model5.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model5.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model4.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', auroc])\n",
    "#model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 530 samples, validate on 28 samples\n",
      "Epoch 1/100\n",
      " - 27s - loss: 0.6949 - accuracy: 0.4849 - val_loss: 0.6965 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69647, saving model to LSTM_new_new.weights.best.hdf5\n",
      "Epoch 2/100\n",
      " - 26s - loss: 0.6930 - accuracy: 0.5113 - val_loss: 0.6965 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.69647\n",
      "Epoch 3/100\n",
      " - 26s - loss: 0.6931 - accuracy: 0.5057 - val_loss: 0.7005 - val_accuracy: 0.3571\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.69647\n",
      "Epoch 4/100\n",
      " - 27s - loss: 0.6932 - accuracy: 0.5019 - val_loss: 0.6958 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.69647 to 0.69578, saving model to LSTM_new_new.weights.best.hdf5\n",
      "Epoch 5/100\n",
      " - 30s - loss: 0.6942 - accuracy: 0.4981 - val_loss: 0.6936 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.69578 to 0.69361, saving model to LSTM_new_new.weights.best.hdf5\n",
      "Epoch 6/100\n",
      " - 27s - loss: 0.6954 - accuracy: 0.5038 - val_loss: 0.6955 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.69361\n",
      "Epoch 7/100\n",
      " - 27s - loss: 0.6932 - accuracy: 0.5170 - val_loss: 0.6963 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.69361\n",
      "Epoch 8/100\n",
      " - 27s - loss: 0.6927 - accuracy: 0.5170 - val_loss: 0.6967 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.69361\n",
      "Epoch 9/100\n",
      " - 27s - loss: 0.6918 - accuracy: 0.5132 - val_loss: 0.6955 - val_accuracy: 0.4286\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.69361\n",
      "Epoch 10/100\n",
      " - 27s - loss: 0.6941 - accuracy: 0.5132 - val_loss: 0.6955 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.69361\n",
      "Epoch 11/100\n",
      " - 27s - loss: 0.6906 - accuracy: 0.5151 - val_loss: 0.6970 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.69361\n",
      "Epoch 12/100\n",
      " - 27s - loss: 0.6917 - accuracy: 0.5226 - val_loss: 0.6986 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.69361\n",
      "Epoch 13/100\n",
      " - 27s - loss: 0.6913 - accuracy: 0.5170 - val_loss: 0.6905 - val_accuracy: 0.4643\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.69361 to 0.69050, saving model to LSTM_new_new.weights.best.hdf5\n",
      "Epoch 14/100\n",
      " - 30s - loss: 0.7073 - accuracy: 0.5000 - val_loss: 0.7399 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.69050\n",
      "Epoch 15/100\n",
      " - 34s - loss: 0.6972 - accuracy: 0.4943 - val_loss: 0.6945 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.69050\n",
      "Epoch 16/100\n",
      " - 28s - loss: 0.6950 - accuracy: 0.4925 - val_loss: 0.6975 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.69050\n",
      "Epoch 17/100\n",
      " - 27s - loss: 0.6939 - accuracy: 0.4981 - val_loss: 0.6922 - val_accuracy: 0.6071\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.69050\n",
      "Epoch 18/100\n",
      " - 32s - loss: 0.6943 - accuracy: 0.5094 - val_loss: 0.7031 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.69050\n",
      "Epoch 19/100\n",
      " - 30s - loss: 0.6932 - accuracy: 0.5113 - val_loss: 0.6969 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.69050\n",
      "Epoch 20/100\n",
      " - 28s - loss: 0.6939 - accuracy: 0.4698 - val_loss: 0.6946 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.69050\n",
      "Epoch 21/100\n",
      " - 28s - loss: 0.6934 - accuracy: 0.5113 - val_loss: 0.7003 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.69050\n",
      "Epoch 22/100\n",
      " - 28s - loss: 0.6933 - accuracy: 0.5113 - val_loss: 0.6997 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.69050\n",
      "Epoch 23/100\n",
      " - 28s - loss: 0.6939 - accuracy: 0.4698 - val_loss: 0.6934 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.69050\n",
      "Epoch 24/100\n",
      " - 28s - loss: 0.6932 - accuracy: 0.5113 - val_loss: 0.6993 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.69050\n",
      "Epoch 25/100\n",
      " - 29s - loss: 0.6932 - accuracy: 0.5113 - val_loss: 0.6971 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.69050\n",
      "Epoch 26/100\n",
      " - 35s - loss: 0.6929 - accuracy: 0.5113 - val_loss: 0.6974 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.69050\n",
      "Epoch 27/100\n",
      " - 31s - loss: 0.6932 - accuracy: 0.5113 - val_loss: 0.6954 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.69050\n",
      "Epoch 28/100\n",
      " - 30s - loss: 0.6933 - accuracy: 0.5113 - val_loss: 0.6951 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.69050\n",
      "Epoch 29/100\n",
      " - 36s - loss: 0.6929 - accuracy: 0.5113 - val_loss: 0.6983 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.69050\n",
      "Epoch 30/100\n",
      " - 30s - loss: 0.6930 - accuracy: 0.5113 - val_loss: 0.6979 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.69050\n",
      "Epoch 31/100\n",
      " - 31s - loss: 0.6930 - accuracy: 0.5113 - val_loss: 0.6986 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.69050\n",
      "Epoch 32/100\n",
      " - 34s - loss: 0.6930 - accuracy: 0.5113 - val_loss: 0.6963 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.69050\n",
      "Epoch 33/100\n",
      " - 35s - loss: 0.6929 - accuracy: 0.5113 - val_loss: 0.6982 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.69050\n",
      "Epoch 34/100\n",
      " - 30s - loss: 0.6929 - accuracy: 0.5113 - val_loss: 0.6969 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.69050\n",
      "Epoch 35/100\n",
      " - 29s - loss: 0.6931 - accuracy: 0.5113 - val_loss: 0.6988 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.69050\n",
      "Epoch 36/100\n",
      " - 28s - loss: 0.6929 - accuracy: 0.5113 - val_loss: 0.6987 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.69050\n",
      "Epoch 37/100\n",
      " - 29s - loss: 0.6928 - accuracy: 0.5113 - val_loss: 0.6969 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.69050\n",
      "Epoch 38/100\n",
      " - 31s - loss: 0.6929 - accuracy: 0.5113 - val_loss: 0.6961 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.69050\n",
      "Epoch 39/100\n",
      " - 27s - loss: 0.6930 - accuracy: 0.5113 - val_loss: 0.6982 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.69050\n",
      "Epoch 40/100\n",
      " - 28s - loss: 0.6928 - accuracy: 0.5113 - val_loss: 0.6979 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.69050\n",
      "Epoch 41/100\n",
      " - 30s - loss: 0.6927 - accuracy: 0.5113 - val_loss: 0.6975 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.69050\n",
      "Epoch 42/100\n",
      " - 33s - loss: 0.6928 - accuracy: 0.5113 - val_loss: 0.6969 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.69050\n",
      "Epoch 43/100\n",
      " - 32s - loss: 0.6926 - accuracy: 0.5113 - val_loss: 0.6979 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.69050\n",
      "Epoch 44/100\n",
      " - 31s - loss: 0.6928 - accuracy: 0.5113 - val_loss: 0.6966 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.69050\n",
      "Epoch 45/100\n",
      " - 38s - loss: 0.6927 - accuracy: 0.5113 - val_loss: 0.6965 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.69050\n",
      "Epoch 46/100\n",
      " - 36s - loss: 0.6927 - accuracy: 0.5113 - val_loss: 0.6979 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.69050\n",
      "Epoch 47/100\n",
      " - 30s - loss: 0.6926 - accuracy: 0.5113 - val_loss: 0.6970 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.69050\n",
      "Epoch 48/100\n",
      " - 30s - loss: 0.6927 - accuracy: 0.5113 - val_loss: 0.6964 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.69050\n",
      "Epoch 49/100\n",
      " - 33s - loss: 0.6928 - accuracy: 0.5113 - val_loss: 0.6974 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.69050\n",
      "Epoch 50/100\n",
      " - 32s - loss: 0.6927 - accuracy: 0.5113 - val_loss: 0.6973 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.69050\n",
      "Epoch 51/100\n",
      " - 31s - loss: 0.6927 - accuracy: 0.5113 - val_loss: 0.6971 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.69050\n",
      "Epoch 52/100\n",
      " - 33s - loss: 0.6926 - accuracy: 0.5113 - val_loss: 0.6968 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.69050\n",
      "Epoch 53/100\n",
      " - 30s - loss: 0.6926 - accuracy: 0.5113 - val_loss: 0.6963 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.69050\n",
      "Epoch 54/100\n",
      " - 32s - loss: 0.6927 - accuracy: 0.5113 - val_loss: 0.6965 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.69050\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 34s - loss: 0.6925 - accuracy: 0.5113 - val_loss: 0.6964 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.69050\n",
      "Epoch 56/100\n",
      " - 30s - loss: 0.6929 - accuracy: 0.5113 - val_loss: 0.6969 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.69050\n",
      "Epoch 57/100\n",
      " - 32s - loss: 0.6926 - accuracy: 0.5113 - val_loss: 0.6968 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.69050\n",
      "Epoch 58/100\n",
      " - 31s - loss: 0.6925 - accuracy: 0.5113 - val_loss: 0.6960 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.69050\n",
      "Epoch 59/100\n",
      " - 31s - loss: 0.6925 - accuracy: 0.5113 - val_loss: 0.6965 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.69050\n",
      "Epoch 60/100\n",
      " - 32s - loss: 0.6927 - accuracy: 0.5113 - val_loss: 0.6966 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.69050\n",
      "Epoch 61/100\n",
      " - 33s - loss: 0.6926 - accuracy: 0.5113 - val_loss: 0.6978 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.69050\n",
      "Epoch 62/100\n",
      " - 30s - loss: 0.6927 - accuracy: 0.5113 - val_loss: 0.6966 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.69050\n",
      "Epoch 63/100\n",
      " - 33s - loss: 0.6926 - accuracy: 0.5113 - val_loss: 0.6977 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.69050\n",
      "Epoch 64/100\n",
      " - 32s - loss: 0.6926 - accuracy: 0.5113 - val_loss: 0.6978 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.69050\n",
      "Epoch 65/100\n",
      " - 31s - loss: 0.6931 - accuracy: 0.5113 - val_loss: 0.6976 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.69050\n",
      "Epoch 66/100\n",
      " - 31s - loss: 0.6929 - accuracy: 0.5113 - val_loss: 0.6978 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.69050\n",
      "Epoch 67/100\n",
      " - 31s - loss: 0.6930 - accuracy: 0.5113 - val_loss: 0.6976 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.69050\n",
      "Epoch 68/100\n",
      " - 31s - loss: 0.6928 - accuracy: 0.5113 - val_loss: 0.6983 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.69050\n",
      "Epoch 69/100\n",
      " - 33s - loss: 0.6931 - accuracy: 0.5113 - val_loss: 0.6983 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.69050\n",
      "Epoch 70/100\n",
      " - 30s - loss: 0.6936 - accuracy: 0.5113 - val_loss: 0.6992 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.69050\n",
      "Epoch 71/100\n",
      " - 30s - loss: 0.6929 - accuracy: 0.5113 - val_loss: 0.6976 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.69050\n",
      "Epoch 72/100\n",
      " - 31s - loss: 0.6929 - accuracy: 0.5113 - val_loss: 0.6984 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.69050\n",
      "Epoch 73/100\n",
      " - 32s - loss: 0.6926 - accuracy: 0.5113 - val_loss: 0.6979 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.69050\n",
      "Epoch 74/100\n",
      " - 32s - loss: 0.6929 - accuracy: 0.5113 - val_loss: 0.6975 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.69050\n",
      "Epoch 75/100\n",
      " - 31s - loss: 0.6930 - accuracy: 0.5113 - val_loss: 0.6971 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.69050\n",
      "Epoch 76/100\n",
      " - 29s - loss: 0.6926 - accuracy: 0.5113 - val_loss: 0.6976 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.69050\n",
      "Epoch 77/100\n",
      " - 31s - loss: 0.6929 - accuracy: 0.5113 - val_loss: 0.6970 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.69050\n",
      "Epoch 78/100\n",
      " - 32s - loss: 0.6930 - accuracy: 0.5113 - val_loss: 0.6981 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.69050\n",
      "Epoch 79/100\n",
      " - 31s - loss: 0.6927 - accuracy: 0.5113 - val_loss: 0.6968 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.69050\n",
      "Epoch 80/100\n",
      " - 33s - loss: 0.6929 - accuracy: 0.5113 - val_loss: 0.6985 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.69050\n",
      "Epoch 81/100\n",
      " - 30s - loss: 0.6928 - accuracy: 0.5113 - val_loss: 0.6967 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.69050\n",
      "Epoch 82/100\n",
      " - 31s - loss: 0.6928 - accuracy: 0.5113 - val_loss: 0.6970 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.69050\n",
      "Epoch 83/100\n",
      " - 33s - loss: 0.6927 - accuracy: 0.5113 - val_loss: 0.6985 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.69050\n",
      "Epoch 84/100\n",
      " - 31s - loss: 0.6928 - accuracy: 0.5113 - val_loss: 0.6978 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.69050\n",
      "Epoch 85/100\n",
      " - 34s - loss: 0.6932 - accuracy: 0.5113 - val_loss: 0.6971 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.69050\n",
      "Epoch 86/100\n",
      " - 31s - loss: 0.6930 - accuracy: 0.5113 - val_loss: 0.6985 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.69050\n",
      "Epoch 87/100\n",
      " - 28s - loss: 0.6929 - accuracy: 0.5113 - val_loss: 0.6980 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.69050\n",
      "Epoch 88/100\n",
      " - 28s - loss: 0.6933 - accuracy: 0.5113 - val_loss: 0.6973 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.69050\n",
      "Epoch 89/100\n",
      " - 28s - loss: 0.6932 - accuracy: 0.5113 - val_loss: 0.6975 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.69050\n",
      "Epoch 90/100\n",
      " - 28s - loss: 0.6930 - accuracy: 0.5113 - val_loss: 0.6969 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.69050\n",
      "Epoch 91/100\n",
      " - 28s - loss: 0.6928 - accuracy: 0.5113 - val_loss: 0.6976 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.69050\n",
      "Epoch 92/100\n",
      " - 28s - loss: 0.6940 - accuracy: 0.5113 - val_loss: 0.6985 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.69050\n",
      "Epoch 93/100\n",
      " - 29s - loss: 0.6932 - accuracy: 0.5113 - val_loss: 0.6971 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.69050\n",
      "Epoch 94/100\n",
      " - 28s - loss: 0.6931 - accuracy: 0.5113 - val_loss: 0.6974 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.69050\n",
      "Epoch 95/100\n",
      " - 28s - loss: 0.6929 - accuracy: 0.5113 - val_loss: 0.6986 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.69050\n",
      "Epoch 96/100\n",
      " - 29s - loss: 0.6928 - accuracy: 0.5113 - val_loss: 0.6981 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.69050\n",
      "Epoch 97/100\n",
      " - 28s - loss: 0.6929 - accuracy: 0.5113 - val_loss: 0.6986 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.69050\n",
      "Epoch 98/100\n",
      " - 29s - loss: 0.6930 - accuracy: 0.5113 - val_loss: 0.6991 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.69050\n",
      "Epoch 99/100\n",
      " - 28s - loss: 0.6928 - accuracy: 0.5113 - val_loss: 0.6975 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.69050\n",
      "Epoch 100/100\n",
      " - 29s - loss: 0.6929 - accuracy: 0.5113 - val_loss: 0.6983 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.69050\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath = 'LSTM_new_new.weights.best.hdf5', verbose = 1, save_best_only = True)\n",
    "hist = model5.fit(X_train_lstm, y_train, epochs = 100, batch_size= 32, validation_split = 0.05, callbacks = [checkpointer], verbose = 2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 1s 17ms/step\n",
      "Accuracy:  0.4516128897666931\n"
     ]
    }
   ],
   "source": [
    "score = model5.evaluate(X_test_lstm, y_test, verbose=1)\n",
    "print(\"Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predict5 =  [1 if a>0.5 else 0 for a in model5.predict(X_test_lstm)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.45161290322580644\n",
      "Precision = 0.0\n",
      "Recall = 0.0\n",
      "F1 Score = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\Diplom_New\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = {}\\nPrecision = {}\\nRecall = {}\\nF1 Score = {}\".format(metrics.accuracy_score(y_test, predict5),\n",
    "                                                                         metrics.precision_score(y_test, predict5),\n",
    "                                                                         metrics.recall_score(y_test, predict5),\n",
    "                                                                         metrics.f1_score(y_test, predict5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Appr 2 (Pasha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_0_0</th>\n",
       "      <th>min_0_0</th>\n",
       "      <th>max_0_0</th>\n",
       "      <th>mean_1_0</th>\n",
       "      <th>min_1_0</th>\n",
       "      <th>max_1_0</th>\n",
       "      <th>mean_2_0</th>\n",
       "      <th>min_2_0</th>\n",
       "      <th>max_2_0</th>\n",
       "      <th>mean_3_0</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_11_9</th>\n",
       "      <th>min_11_9</th>\n",
       "      <th>max_11_9</th>\n",
       "      <th>mean_12_9</th>\n",
       "      <th>min_12_9</th>\n",
       "      <th>max_12_9</th>\n",
       "      <th>mean_13_9</th>\n",
       "      <th>min_13_9</th>\n",
       "      <th>max_13_9</th>\n",
       "      <th>gesture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4179.1025</td>\n",
       "      <td>4167.308</td>\n",
       "      <td>4187.051</td>\n",
       "      <td>4194.2307</td>\n",
       "      <td>4177.821</td>\n",
       "      <td>4203.974</td>\n",
       "      <td>4200.2436</td>\n",
       "      <td>4193.333</td>\n",
       "      <td>4204.231</td>\n",
       "      <td>4198.1024</td>\n",
       "      <td>...</td>\n",
       "      <td>4250.8461</td>\n",
       "      <td>4236.667</td>\n",
       "      <td>4258.333</td>\n",
       "      <td>4237.1409</td>\n",
       "      <td>4215.128</td>\n",
       "      <td>4243.333</td>\n",
       "      <td>4221.8206</td>\n",
       "      <td>4203.462</td>\n",
       "      <td>4231.026</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4198.8719</td>\n",
       "      <td>4189.103</td>\n",
       "      <td>4209.872</td>\n",
       "      <td>4100.8974</td>\n",
       "      <td>4090.513</td>\n",
       "      <td>4113.205</td>\n",
       "      <td>4225.3334</td>\n",
       "      <td>4218.590</td>\n",
       "      <td>4232.179</td>\n",
       "      <td>4169.7948</td>\n",
       "      <td>...</td>\n",
       "      <td>4242.3974</td>\n",
       "      <td>4224.615</td>\n",
       "      <td>4262.308</td>\n",
       "      <td>4212.4617</td>\n",
       "      <td>4182.949</td>\n",
       "      <td>4231.667</td>\n",
       "      <td>4189.9744</td>\n",
       "      <td>4168.333</td>\n",
       "      <td>4212.692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4252.4357</td>\n",
       "      <td>4206.410</td>\n",
       "      <td>4277.179</td>\n",
       "      <td>4252.8588</td>\n",
       "      <td>4199.615</td>\n",
       "      <td>4281.154</td>\n",
       "      <td>4209.6923</td>\n",
       "      <td>4187.949</td>\n",
       "      <td>4221.410</td>\n",
       "      <td>4223.6155</td>\n",
       "      <td>...</td>\n",
       "      <td>4246.4616</td>\n",
       "      <td>4227.821</td>\n",
       "      <td>4264.487</td>\n",
       "      <td>4201.7949</td>\n",
       "      <td>4183.590</td>\n",
       "      <td>4221.282</td>\n",
       "      <td>4198.9358</td>\n",
       "      <td>4178.205</td>\n",
       "      <td>4215.897</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4196.5897</td>\n",
       "      <td>4189.103</td>\n",
       "      <td>4205.128</td>\n",
       "      <td>4217.5898</td>\n",
       "      <td>4206.282</td>\n",
       "      <td>4230.385</td>\n",
       "      <td>4199.9744</td>\n",
       "      <td>4192.436</td>\n",
       "      <td>4210.513</td>\n",
       "      <td>4199.5641</td>\n",
       "      <td>...</td>\n",
       "      <td>4256.8461</td>\n",
       "      <td>4245.897</td>\n",
       "      <td>4270.897</td>\n",
       "      <td>4217.1795</td>\n",
       "      <td>4204.103</td>\n",
       "      <td>4231.154</td>\n",
       "      <td>4214.2563</td>\n",
       "      <td>4203.974</td>\n",
       "      <td>4228.718</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4216.5385</td>\n",
       "      <td>4211.282</td>\n",
       "      <td>4230.000</td>\n",
       "      <td>4250.7693</td>\n",
       "      <td>4243.333</td>\n",
       "      <td>4258.077</td>\n",
       "      <td>4173.7307</td>\n",
       "      <td>4165.769</td>\n",
       "      <td>4177.821</td>\n",
       "      <td>4200.6281</td>\n",
       "      <td>...</td>\n",
       "      <td>4224.4102</td>\n",
       "      <td>4210.641</td>\n",
       "      <td>4242.179</td>\n",
       "      <td>4162.3590</td>\n",
       "      <td>4146.667</td>\n",
       "      <td>4177.949</td>\n",
       "      <td>4183.6411</td>\n",
       "      <td>4164.359</td>\n",
       "      <td>4201.410</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 421 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_0_0   min_0_0   max_0_0   mean_1_0   min_1_0   max_1_0   mean_2_0  \\\n",
       "0  4179.1025  4167.308  4187.051  4194.2307  4177.821  4203.974  4200.2436   \n",
       "1  4198.8719  4189.103  4209.872  4100.8974  4090.513  4113.205  4225.3334   \n",
       "2  4252.4357  4206.410  4277.179  4252.8588  4199.615  4281.154  4209.6923   \n",
       "3  4196.5897  4189.103  4205.128  4217.5898  4206.282  4230.385  4199.9744   \n",
       "4  4216.5385  4211.282  4230.000  4250.7693  4243.333  4258.077  4173.7307   \n",
       "\n",
       "    min_2_0   max_2_0   mean_3_0  ...  mean_11_9  min_11_9  max_11_9  \\\n",
       "0  4193.333  4204.231  4198.1024  ...  4250.8461  4236.667  4258.333   \n",
       "1  4218.590  4232.179  4169.7948  ...  4242.3974  4224.615  4262.308   \n",
       "2  4187.949  4221.410  4223.6155  ...  4246.4616  4227.821  4264.487   \n",
       "3  4192.436  4210.513  4199.5641  ...  4256.8461  4245.897  4270.897   \n",
       "4  4165.769  4177.821  4200.6281  ...  4224.4102  4210.641  4242.179   \n",
       "\n",
       "   mean_12_9  min_12_9  max_12_9  mean_13_9  min_13_9  max_13_9  gesture  \n",
       "0  4237.1409  4215.128  4243.333  4221.8206  4203.462  4231.026        1  \n",
       "1  4212.4617  4182.949  4231.667  4189.9744  4168.333  4212.692        1  \n",
       "2  4201.7949  4183.590  4221.282  4198.9358  4178.205  4215.897        1  \n",
       "3  4217.1795  4204.103  4231.154  4214.2563  4203.974  4228.718        1  \n",
       "4  4162.3590  4146.667  4177.949  4183.6411  4164.359  4201.410        1  \n",
       "\n",
       "[5 rows x 421 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pasha = pd.read_csv('full_data_transformed_shuffle.csv').drop(columns = ['Unnamed: 0'])\n",
    "df_pasha.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(620, 420) (620,)\n"
     ]
    }
   ],
   "source": [
    "X = df_pasha.drop(columns = ['gesture'])\n",
    "y = df_pasha['gesture']\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "589 31\n"
     ]
    }
   ],
   "source": [
    "X,y = shuffle(X, y, random_state=0)\n",
    "\n",
    "train_size = int(len(X) * 0.95)\n",
    "test_size = len(X) - train_size\n",
    "train, test = X.iloc[0:train_size], X.iloc[train_size:len(X)]\n",
    "print(train_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(589, 420) (31, 420)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train)\n",
    "X_test = scaler.transform(test)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(589,) (31,)\n"
     ]
    }
   ],
   "source": [
    "y_train = y.iloc[0:train_size]\n",
    "y_test = y.iloc[train_size:len(y)]\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 1000)              421000    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 1001      \n",
      "=================================================================\n",
      "Total params: 2,424,001\n",
      "Trainable params: 2,424,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model21 = Sequential()\n",
    "model21.add(Dense(1000, activation = 'relu', input_shape = (X_train.shape[1], )))\n",
    "model21.add(Dropout(0.2))\n",
    "model21.add(Dense(1000, activation = 'relu'))\n",
    "model21.add(Dropout(0.2))\n",
    "model21.add(Dense(1000, activation = 'relu'))\n",
    "model21.add(Dropout(0.2))\n",
    "model21.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model21.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model21.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 559 samples, validate on 30 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 1.0102 - accuracy: 0.4866 - val_loss: 0.8693 - val_accuracy: 0.2667\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.86930, saving model to MLP_new.weights.best.hdf5\n",
      "Epoch 2/100\n",
      " - 1s - loss: 1.0417 - accuracy: 0.5224 - val_loss: 0.9183 - val_accuracy: 0.2000\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.86930\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.8243 - accuracy: 0.5188 - val_loss: 1.1243 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.86930\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.7474 - accuracy: 0.5886 - val_loss: 0.8278 - val_accuracy: 0.4000\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.86930 to 0.82779, saving model to MLP_new.weights.best.hdf5\n",
      "Epoch 5/100\n",
      " - 1s - loss: 0.7634 - accuracy: 0.5599 - val_loss: 0.7172 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.82779 to 0.71723, saving model to MLP_new.weights.best.hdf5\n",
      "Epoch 6/100\n",
      " - 1s - loss: 0.6905 - accuracy: 0.5581 - val_loss: 0.7863 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.71723\n",
      "Epoch 7/100\n",
      " - 1s - loss: 0.6895 - accuracy: 0.5617 - val_loss: 0.7264 - val_accuracy: 0.3000\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.71723\n",
      "Epoch 8/100\n",
      " - 1s - loss: 0.6714 - accuracy: 0.5796 - val_loss: 0.7313 - val_accuracy: 0.4000\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.71723\n",
      "Epoch 9/100\n",
      " - 1s - loss: 0.6630 - accuracy: 0.6351 - val_loss: 0.7653 - val_accuracy: 0.3000\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.71723\n",
      "Epoch 10/100\n",
      " - 1s - loss: 0.6275 - accuracy: 0.6816 - val_loss: 0.7001 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.71723 to 0.70011, saving model to MLP_new.weights.best.hdf5\n",
      "Epoch 11/100\n",
      " - 1s - loss: 0.7409 - accuracy: 0.6154 - val_loss: 0.8199 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.70011\n",
      "Epoch 12/100\n",
      " - 1s - loss: 0.7265 - accuracy: 0.6190 - val_loss: 0.8105 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.70011\n",
      "Epoch 13/100\n",
      " - 1s - loss: 0.6500 - accuracy: 0.6243 - val_loss: 0.8029 - val_accuracy: 0.3000\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.70011\n",
      "Epoch 14/100\n",
      " - 1s - loss: 0.7081 - accuracy: 0.6225 - val_loss: 0.8236 - val_accuracy: 0.3667\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.70011\n",
      "Epoch 15/100\n",
      " - 1s - loss: 0.5997 - accuracy: 0.6941 - val_loss: 0.8675 - val_accuracy: 0.3000\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.70011\n",
      "Epoch 16/100\n",
      " - 1s - loss: 0.6452 - accuracy: 0.6995 - val_loss: 0.9184 - val_accuracy: 0.2667\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.70011\n",
      "Epoch 17/100\n",
      " - 1s - loss: 0.5756 - accuracy: 0.6852 - val_loss: 0.8934 - val_accuracy: 0.4000\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.70011\n",
      "Epoch 18/100\n",
      " - 1s - loss: 0.5353 - accuracy: 0.7442 - val_loss: 0.8623 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.70011\n",
      "Epoch 19/100\n",
      " - 1s - loss: 0.4636 - accuracy: 0.7674 - val_loss: 0.9207 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.70011\n",
      "Epoch 20/100\n",
      " - 1s - loss: 0.4502 - accuracy: 0.8050 - val_loss: 1.0160 - val_accuracy: 0.5333\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.70011\n",
      "Epoch 21/100\n",
      " - 1s - loss: 0.3606 - accuracy: 0.8354 - val_loss: 1.1896 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.70011\n",
      "Epoch 22/100\n",
      " - 1s - loss: 0.3482 - accuracy: 0.8462 - val_loss: 1.2694 - val_accuracy: 0.5333\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.70011\n",
      "Epoch 23/100\n",
      " - 1s - loss: 0.3240 - accuracy: 0.8605 - val_loss: 1.3940 - val_accuracy: 0.5333\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.70011\n",
      "Epoch 24/100\n",
      " - 1s - loss: 0.3549 - accuracy: 0.8658 - val_loss: 1.4536 - val_accuracy: 0.4000\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.70011\n",
      "Epoch 25/100\n",
      " - 1s - loss: 0.3272 - accuracy: 0.8605 - val_loss: 1.6455 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.70011\n",
      "Epoch 26/100\n",
      " - 1s - loss: 0.2907 - accuracy: 0.8694 - val_loss: 1.4297 - val_accuracy: 0.5333\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.70011\n",
      "Epoch 27/100\n",
      " - 1s - loss: 0.3094 - accuracy: 0.8766 - val_loss: 1.3437 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.70011\n",
      "Epoch 28/100\n",
      " - 1s - loss: 0.2997 - accuracy: 0.8676 - val_loss: 1.5145 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.70011\n",
      "Epoch 29/100\n",
      " - 1s - loss: 0.2081 - accuracy: 0.9159 - val_loss: 1.9109 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.70011\n",
      "Epoch 30/100\n",
      " - 1s - loss: 0.2000 - accuracy: 0.9267 - val_loss: 1.6574 - val_accuracy: 0.5333\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.70011\n",
      "Epoch 31/100\n",
      " - 1s - loss: 0.1786 - accuracy: 0.9195 - val_loss: 1.8244 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.70011\n",
      "Epoch 32/100\n",
      " - 1s - loss: 0.3266 - accuracy: 0.8784 - val_loss: 1.5065 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.70011\n",
      "Epoch 33/100\n",
      " - 1s - loss: 0.3404 - accuracy: 0.8497 - val_loss: 1.8395 - val_accuracy: 0.5333\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.70011\n",
      "Epoch 34/100\n",
      " - 1s - loss: 0.3915 - accuracy: 0.8801 - val_loss: 1.6047 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.70011\n",
      "Epoch 35/100\n",
      " - 1s - loss: 0.2552 - accuracy: 0.8819 - val_loss: 2.0980 - val_accuracy: 0.5333\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.70011\n",
      "Epoch 36/100\n",
      " - 1s - loss: 0.1919 - accuracy: 0.9338 - val_loss: 2.8022 - val_accuracy: 0.4000\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.70011\n",
      "Epoch 37/100\n",
      " - 1s - loss: 0.1802 - accuracy: 0.9267 - val_loss: 2.2167 - val_accuracy: 0.5333\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.70011\n",
      "Epoch 38/100\n",
      " - 1s - loss: 0.1199 - accuracy: 0.9445 - val_loss: 2.1259 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.70011\n",
      "Epoch 39/100\n",
      " - 1s - loss: 0.1075 - accuracy: 0.9553 - val_loss: 2.7055 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.70011\n",
      "Epoch 40/100\n",
      " - 1s - loss: 0.2633 - accuracy: 0.9338 - val_loss: 2.3642 - val_accuracy: 0.6333\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.70011\n",
      "Epoch 41/100\n",
      " - 1s - loss: 0.1547 - accuracy: 0.9463 - val_loss: 2.4735 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.70011\n",
      "Epoch 42/100\n",
      " - 1s - loss: 0.1488 - accuracy: 0.9284 - val_loss: 2.5926 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.70011\n",
      "Epoch 43/100\n",
      " - 1s - loss: 0.1328 - accuracy: 0.9499 - val_loss: 2.6437 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.70011\n",
      "Epoch 44/100\n",
      " - 1s - loss: 0.1038 - accuracy: 0.9606 - val_loss: 2.7488 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.70011\n",
      "Epoch 45/100\n",
      " - 1s - loss: 0.0768 - accuracy: 0.9660 - val_loss: 2.9040 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.70011\n",
      "Epoch 46/100\n",
      " - 1s - loss: 0.1131 - accuracy: 0.9589 - val_loss: 3.0096 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.70011\n",
      "Epoch 47/100\n",
      " - 1s - loss: 0.2073 - accuracy: 0.9141 - val_loss: 3.2882 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.70011\n",
      "Epoch 48/100\n",
      " - 1s - loss: 0.1792 - accuracy: 0.9374 - val_loss: 3.1022 - val_accuracy: 0.4000\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.70011\n",
      "Epoch 49/100\n",
      " - 1s - loss: 0.1196 - accuracy: 0.9481 - val_loss: 2.8149 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.70011\n",
      "Epoch 50/100\n",
      " - 1s - loss: 0.1640 - accuracy: 0.9463 - val_loss: 2.9881 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.70011\n",
      "Epoch 51/100\n",
      " - 2s - loss: 0.1863 - accuracy: 0.9320 - val_loss: 2.4831 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.70011\n",
      "Epoch 52/100\n",
      " - 1s - loss: 0.1119 - accuracy: 0.9445 - val_loss: 3.3979 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.70011\n",
      "Epoch 53/100\n",
      " - 1s - loss: 0.1170 - accuracy: 0.9499 - val_loss: 3.5439 - val_accuracy: 0.4000\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.70011\n",
      "Epoch 54/100\n",
      " - 2s - loss: 0.2059 - accuracy: 0.9374 - val_loss: 2.4555 - val_accuracy: 0.5333\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.70011\n",
      "Epoch 55/100\n",
      " - 1s - loss: 0.1329 - accuracy: 0.9499 - val_loss: 2.9668 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00055: val_loss did not improve from 0.70011\n",
      "Epoch 56/100\n",
      " - 1s - loss: 0.0879 - accuracy: 0.9571 - val_loss: 2.8862 - val_accuracy: 0.5333\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.70011\n",
      "Epoch 57/100\n",
      " - 2s - loss: 0.0594 - accuracy: 0.9714 - val_loss: 3.4537 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.70011\n",
      "Epoch 58/100\n",
      " - 1s - loss: 0.1189 - accuracy: 0.9678 - val_loss: 2.8673 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.70011\n",
      "Epoch 59/100\n",
      " - 1s - loss: 0.0914 - accuracy: 0.9571 - val_loss: 3.2805 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.70011\n",
      "Epoch 60/100\n",
      " - 1s - loss: 0.1650 - accuracy: 0.9481 - val_loss: 2.8145 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.70011\n",
      "Epoch 61/100\n",
      " - 1s - loss: 0.1646 - accuracy: 0.9392 - val_loss: 3.5998 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.70011\n",
      "Epoch 62/100\n",
      " - 1s - loss: 0.1355 - accuracy: 0.9428 - val_loss: 3.0149 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.70011\n",
      "Epoch 63/100\n",
      " - 1s - loss: 0.0980 - accuracy: 0.9589 - val_loss: 3.5267 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.70011\n",
      "Epoch 64/100\n",
      " - 1s - loss: 0.0757 - accuracy: 0.9732 - val_loss: 3.5657 - val_accuracy: 0.6333\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.70011\n",
      "Epoch 65/100\n",
      " - 1s - loss: 0.1314 - accuracy: 0.9624 - val_loss: 4.3120 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.70011\n",
      "Epoch 66/100\n",
      " - 1s - loss: 0.3654 - accuracy: 0.9374 - val_loss: 3.8525 - val_accuracy: 0.4000\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.70011\n",
      "Epoch 67/100\n",
      " - 1s - loss: 0.3780 - accuracy: 0.8605 - val_loss: 2.3638 - val_accuracy: 0.4000\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.70011\n",
      "Epoch 68/100\n",
      " - 1s - loss: 0.5443 - accuracy: 0.8819 - val_loss: 3.3002 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.70011\n",
      "Epoch 69/100\n",
      " - 1s - loss: 0.5807 - accuracy: 0.8766 - val_loss: 2.8248 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.70011\n",
      "Epoch 70/100\n",
      " - 1s - loss: 0.3194 - accuracy: 0.8748 - val_loss: 3.0838 - val_accuracy: 0.3667\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.70011\n",
      "Epoch 71/100\n",
      " - 1s - loss: 0.1756 - accuracy: 0.9231 - val_loss: 2.9616 - val_accuracy: 0.5333\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.70011\n",
      "Epoch 72/100\n",
      " - 1s - loss: 0.1675 - accuracy: 0.9320 - val_loss: 3.2051 - val_accuracy: 0.5333\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.70011\n",
      "Epoch 73/100\n",
      " - 1s - loss: 0.3264 - accuracy: 0.9302 - val_loss: 3.2407 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.70011\n",
      "Epoch 74/100\n",
      " - 1s - loss: 0.1556 - accuracy: 0.9571 - val_loss: 3.2435 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.70011\n",
      "Epoch 75/100\n",
      " - 1s - loss: 0.1172 - accuracy: 0.9624 - val_loss: 3.1618 - val_accuracy: 0.5333\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.70011\n",
      "Epoch 76/100\n",
      " - 1s - loss: 0.0856 - accuracy: 0.9553 - val_loss: 3.8555 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.70011\n",
      "Epoch 77/100\n",
      " - 1s - loss: 0.0752 - accuracy: 0.9750 - val_loss: 3.9288 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.70011\n",
      "Epoch 78/100\n",
      " - 1s - loss: 0.1719 - accuracy: 0.9338 - val_loss: 3.8960 - val_accuracy: 0.3667\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.70011\n",
      "Epoch 79/100\n",
      " - 1s - loss: 0.1422 - accuracy: 0.9517 - val_loss: 3.8636 - val_accuracy: 0.4000\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.70011\n",
      "Epoch 80/100\n",
      " - 1s - loss: 0.4078 - accuracy: 0.9410 - val_loss: 5.7105 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.70011\n",
      "Epoch 81/100\n",
      " - 1s - loss: 0.3045 - accuracy: 0.9177 - val_loss: 3.3020 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.70011\n",
      "Epoch 82/100\n",
      " - 1s - loss: 0.1729 - accuracy: 0.9356 - val_loss: 3.9142 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.70011\n",
      "Epoch 83/100\n",
      " - 1s - loss: 0.1023 - accuracy: 0.9481 - val_loss: 4.5999 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.70011\n",
      "Epoch 84/100\n",
      " - 1s - loss: 0.1334 - accuracy: 0.9678 - val_loss: 3.9553 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.70011\n",
      "Epoch 85/100\n",
      " - 1s - loss: 0.1143 - accuracy: 0.9624 - val_loss: 3.4360 - val_accuracy: 0.3667\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.70011\n",
      "Epoch 86/100\n",
      " - 1s - loss: 0.1145 - accuracy: 0.9624 - val_loss: 3.8392 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.70011\n",
      "Epoch 87/100\n",
      " - 1s - loss: 0.0793 - accuracy: 0.9732 - val_loss: 4.2481 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.70011\n",
      "Epoch 88/100\n",
      " - 1s - loss: 0.0710 - accuracy: 0.9750 - val_loss: 4.6161 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.70011\n",
      "Epoch 89/100\n",
      " - 1s - loss: 0.0816 - accuracy: 0.9714 - val_loss: 4.4988 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.70011\n",
      "Epoch 90/100\n",
      " - 1s - loss: 0.0571 - accuracy: 0.9750 - val_loss: 4.2508 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.70011\n",
      "Epoch 91/100\n",
      " - 1s - loss: 0.0537 - accuracy: 0.9839 - val_loss: 4.8563 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.70011\n",
      "Epoch 92/100\n",
      " - 1s - loss: 0.0553 - accuracy: 0.9750 - val_loss: 4.6458 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.70011\n",
      "Epoch 93/100\n",
      " - 1s - loss: 0.1554 - accuracy: 0.9624 - val_loss: 4.5922 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.70011\n",
      "Epoch 94/100\n",
      " - 1s - loss: 0.0628 - accuracy: 0.9767 - val_loss: 4.7945 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.70011\n",
      "Epoch 95/100\n",
      " - 1s - loss: 0.0961 - accuracy: 0.9660 - val_loss: 4.4586 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.70011\n",
      "Epoch 96/100\n",
      " - 1s - loss: 0.0492 - accuracy: 0.9839 - val_loss: 4.1135 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.70011\n",
      "Epoch 97/100\n",
      " - 1s - loss: 0.0479 - accuracy: 0.9803 - val_loss: 5.1307 - val_accuracy: 0.4000\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.70011\n",
      "Epoch 98/100\n",
      " - 1s - loss: 0.0355 - accuracy: 0.9785 - val_loss: 5.5080 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.70011\n",
      "Epoch 99/100\n",
      " - 1s - loss: 0.0431 - accuracy: 0.9821 - val_loss: 4.9265 - val_accuracy: 0.4000\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.70011\n",
      "Epoch 100/100\n",
      " - 1s - loss: 0.0325 - accuracy: 0.9803 - val_loss: 5.3250 - val_accuracy: 0.4000\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.70011\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath = 'MLP_new.weights.best.hdf5', verbose = 1, save_best_only = True)\n",
    "hist21 = model21.fit(X_train, y_train, epochs = 100, batch_size=32, validation_split = 0.05, callbacks = [checkpointer], verbose = 2, shuffle = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy: 0.612903 \n"
     ]
    }
   ],
   "source": [
    "model21.load_weights('MLP_new.weights.best.hdf5')\n",
    "predict21 = [1 if a>=0.5 else 0 for a in model21.predict(X_test)]\n",
    "acc = metrics.accuracy_score(y_test, predict21)\n",
    "print(\"Classification accuracy: %f \" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 322us/step\n",
      "Accuracy:  0.6129032373428345\n"
     ]
    }
   ],
   "source": [
    "model21.load_weights('MLP_new.weights.best.hdf5')\n",
    "score = model21.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train_lstm = np.asarray(np.reshape(X_train, (X_train.shape[0], 420, 1)))\n",
    "X_test_lstm = np.asarray(np.reshape(X_test, (X_test.shape[0], 420, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_28 (LSTM)               (None, 420, 256)          264192    \n",
      "_________________________________________________________________\n",
      "lstm_29 (LSTM)               (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 789,761\n",
      "Trainable params: 789,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model32 = Sequential()\n",
    "model32.add(LSTM(256,input_shape=(420, 1), return_sequences=True))\n",
    "model32.add(LSTM(256))\n",
    "model32.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model32.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model32.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 559 samples, validate on 30 samples\n",
      "Epoch 1/100\n",
      " - 107s - loss: 0.6963 - accuracy: 0.4830 - val_loss: 0.7022 - val_accuracy: 0.4000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.70219, saving model to LSTM.weights.best.hdf5\n",
      "Epoch 2/100\n",
      " - 94s - loss: 0.6931 - accuracy: 0.5116 - val_loss: 0.7063 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.70219\n",
      "Epoch 3/100\n",
      " - 106s - loss: 0.6917 - accuracy: 0.4973 - val_loss: 0.7113 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.70219\n",
      "Epoch 4/100\n",
      " - 94s - loss: 0.6922 - accuracy: 0.5116 - val_loss: 0.7057 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.70219\n",
      "Epoch 5/100\n",
      " - 96s - loss: 0.6918 - accuracy: 0.4741 - val_loss: 0.7055 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.70219\n",
      "Epoch 6/100\n",
      " - 98s - loss: 0.6902 - accuracy: 0.4955 - val_loss: 0.7128 - val_accuracy: 0.4000\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.70219\n",
      "Epoch 7/100\n",
      " - 104s - loss: 0.6908 - accuracy: 0.5313 - val_loss: 0.7169 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.70219\n",
      "Epoch 8/100\n",
      " - 111s - loss: 0.6895 - accuracy: 0.5098 - val_loss: 0.7237 - val_accuracy: 0.3667\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.70219\n",
      "Epoch 9/100\n",
      " - 94s - loss: 0.6908 - accuracy: 0.5009 - val_loss: 0.7210 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.70219\n",
      "Epoch 10/100\n",
      " - 93s - loss: 0.6890 - accuracy: 0.5134 - val_loss: 0.7194 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.70219\n",
      "Epoch 11/100\n",
      " - 98s - loss: 0.6892 - accuracy: 0.5027 - val_loss: 0.7139 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.70219\n",
      "Epoch 12/100\n",
      " - 96s - loss: 0.6895 - accuracy: 0.5027 - val_loss: 0.7178 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.70219\n",
      "Epoch 13/100\n",
      " - 94s - loss: 0.6886 - accuracy: 0.4794 - val_loss: 0.7185 - val_accuracy: 0.5333\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.70219\n",
      "Epoch 14/100\n",
      " - 99s - loss: 0.6894 - accuracy: 0.5081 - val_loss: 0.7189 - val_accuracy: 0.4000\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.70219\n",
      "Epoch 15/100\n",
      " - 101s - loss: 0.6885 - accuracy: 0.5170 - val_loss: 0.7489 - val_accuracy: 0.3667\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.70219\n",
      "Epoch 16/100\n",
      " - 106s - loss: 0.6887 - accuracy: 0.4687 - val_loss: 0.7240 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.70219\n",
      "Epoch 17/100\n",
      " - 108s - loss: 0.7017 - accuracy: 0.5152 - val_loss: 0.7024 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.70219\n",
      "Epoch 18/100\n",
      " - 98s - loss: 0.7109 - accuracy: 0.4651 - val_loss: 0.6954 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.70219 to 0.69541, saving model to LSTM.weights.best.hdf5\n",
      "Epoch 19/100\n",
      " - 106s - loss: 0.6993 - accuracy: 0.5170 - val_loss: 0.7139 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.69541\n",
      "Epoch 20/100\n",
      " - 96s - loss: 0.6950 - accuracy: 0.5098 - val_loss: 0.6886 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.69541 to 0.68856, saving model to LSTM.weights.best.hdf5\n",
      "Epoch 21/100\n",
      " - 110s - loss: 0.7080 - accuracy: 0.4687 - val_loss: 0.6844 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.68856 to 0.68437, saving model to LSTM.weights.best.hdf5\n",
      "Epoch 22/100\n",
      " - 98s - loss: 0.7021 - accuracy: 0.4848 - val_loss: 0.6844 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.68437\n",
      "Epoch 23/100\n",
      " - 115s - loss: 0.7015 - accuracy: 0.5170 - val_loss: 0.7082 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.68437\n",
      "Epoch 24/100\n",
      " - 107s - loss: 0.6999 - accuracy: 0.4848 - val_loss: 0.7123 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.68437\n",
      "Epoch 25/100\n",
      " - 96s - loss: 0.6959 - accuracy: 0.4973 - val_loss: 0.6917 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.68437\n",
      "Epoch 26/100\n",
      " - 105s - loss: 0.6952 - accuracy: 0.4866 - val_loss: 0.6904 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.68437\n",
      "Epoch 27/100\n",
      " - 100s - loss: 0.6945 - accuracy: 0.5009 - val_loss: 0.6882 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.68437\n",
      "Epoch 28/100\n",
      " - 115s - loss: 0.7034 - accuracy: 0.5098 - val_loss: 0.7235 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.68437\n",
      "Epoch 29/100\n",
      " - 119s - loss: 0.6946 - accuracy: 0.5188 - val_loss: 0.6864 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.68437\n",
      "Epoch 30/100\n",
      " - 106s - loss: 0.6938 - accuracy: 0.5170 - val_loss: 0.7130 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.68437\n",
      "Epoch 31/100\n",
      " - 105s - loss: 0.7025 - accuracy: 0.4973 - val_loss: 0.6843 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.68437 to 0.68428, saving model to LSTM.weights.best.hdf5\n",
      "Epoch 32/100\n",
      " - 108s - loss: 0.6954 - accuracy: 0.5027 - val_loss: 0.7047 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.68428\n",
      "Epoch 33/100\n",
      " - 123s - loss: 0.6933 - accuracy: 0.5116 - val_loss: 0.6872 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.68428\n",
      "Epoch 34/100\n",
      " - 125s - loss: 0.6974 - accuracy: 0.4919 - val_loss: 0.7202 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.68428\n",
      "Epoch 35/100\n",
      " - 109s - loss: 0.7029 - accuracy: 0.4758 - val_loss: 0.6939 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.68428\n",
      "Epoch 36/100\n",
      " - 107s - loss: 0.6947 - accuracy: 0.4669 - val_loss: 0.7034 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.68428\n",
      "Epoch 37/100\n",
      " - 89s - loss: 0.6994 - accuracy: 0.5295 - val_loss: 0.6842 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.68428 to 0.68424, saving model to LSTM.weights.best.hdf5\n",
      "Epoch 38/100\n",
      " - 89s - loss: 0.7052 - accuracy: 0.4884 - val_loss: 0.7226 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.68424\n",
      "Epoch 39/100\n",
      " - 88s - loss: 0.7020 - accuracy: 0.4866 - val_loss: 0.6896 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.68424\n",
      "Epoch 40/100\n",
      " - 91s - loss: 0.6948 - accuracy: 0.4955 - val_loss: 0.6949 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.68424\n",
      "Epoch 41/100\n",
      " - 88s - loss: 0.6968 - accuracy: 0.4848 - val_loss: 0.6977 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.68424\n",
      "Epoch 42/100\n",
      " - 89s - loss: 0.6940 - accuracy: 0.5098 - val_loss: 0.6912 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.68424\n",
      "Epoch 43/100\n",
      " - 89s - loss: 0.6954 - accuracy: 0.5027 - val_loss: 0.6964 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.68424\n",
      "Epoch 44/100\n",
      " - 90s - loss: 0.6975 - accuracy: 0.4830 - val_loss: 0.6899 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.68424\n",
      "Epoch 45/100\n",
      " - 90s - loss: 0.6931 - accuracy: 0.5134 - val_loss: 0.7020 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.68424\n",
      "Epoch 46/100\n",
      " - 89s - loss: 0.6956 - accuracy: 0.5027 - val_loss: 0.6978 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.68424\n",
      "Epoch 47/100\n",
      " - 90s - loss: 0.6955 - accuracy: 0.4562 - val_loss: 0.6973 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.68424\n",
      "Epoch 48/100\n",
      " - 91s - loss: 0.6973 - accuracy: 0.4669 - val_loss: 0.6966 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.68424\n",
      "Epoch 49/100\n",
      " - 89s - loss: 0.6970 - accuracy: 0.5027 - val_loss: 0.6952 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.68424\n",
      "Epoch 50/100\n",
      " - 92s - loss: 0.6956 - accuracy: 0.4973 - val_loss: 0.6933 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.68424\n",
      "Epoch 51/100\n",
      " - 90s - loss: 0.6940 - accuracy: 0.4812 - val_loss: 0.6966 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.68424\n",
      "Epoch 52/100\n",
      " - 91s - loss: 0.6966 - accuracy: 0.5027 - val_loss: 0.6950 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.68424\n",
      "Epoch 53/100\n",
      " - 91s - loss: 0.6946 - accuracy: 0.4758 - val_loss: 0.6929 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.68424\n",
      "Epoch 54/100\n",
      " - 92s - loss: 0.6946 - accuracy: 0.4973 - val_loss: 0.6925 - val_accuracy: 0.5667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00054: val_loss did not improve from 0.68424\n",
      "Epoch 55/100\n",
      " - 126s - loss: 0.6940 - accuracy: 0.5027 - val_loss: 0.6977 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.68424\n",
      "Epoch 56/100\n",
      " - 95s - loss: 0.6957 - accuracy: 0.4741 - val_loss: 0.7003 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.68424\n",
      "Epoch 57/100\n",
      " - 92s - loss: 0.6954 - accuracy: 0.4866 - val_loss: 0.6909 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.68424\n",
      "Epoch 58/100\n",
      " - 96s - loss: 0.6949 - accuracy: 0.4687 - val_loss: 0.6928 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.68424\n",
      "Epoch 59/100\n",
      " - 92s - loss: 0.6943 - accuracy: 0.4919 - val_loss: 0.6949 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.68424\n",
      "Epoch 60/100\n",
      " - 92s - loss: 0.6968 - accuracy: 0.4687 - val_loss: 0.6894 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.68424\n",
      "Epoch 61/100\n",
      " - 90s - loss: 0.6967 - accuracy: 0.4830 - val_loss: 0.6920 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.68424\n",
      "Epoch 62/100\n",
      " - 92s - loss: 0.6945 - accuracy: 0.4884 - val_loss: 0.6938 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.68424\n",
      "Epoch 63/100\n",
      " - 92s - loss: 0.6982 - accuracy: 0.4741 - val_loss: 0.7020 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.68424\n",
      "Epoch 64/100\n",
      " - 92s - loss: 0.6953 - accuracy: 0.5027 - val_loss: 0.6941 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.68424\n",
      "Epoch 65/100\n",
      " - 93s - loss: 0.6943 - accuracy: 0.5009 - val_loss: 0.6895 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.68424\n",
      "Epoch 66/100\n",
      " - 93s - loss: 0.6950 - accuracy: 0.4472 - val_loss: 0.6907 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.68424\n",
      "Epoch 67/100\n",
      " - 91s - loss: 0.6948 - accuracy: 0.4669 - val_loss: 0.6954 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.68424\n",
      "Epoch 68/100\n",
      " - 93s - loss: 0.6939 - accuracy: 0.4705 - val_loss: 0.6958 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.68424\n",
      "Epoch 69/100\n",
      " - 92s - loss: 0.6935 - accuracy: 0.4866 - val_loss: 0.6944 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.68424\n",
      "Epoch 70/100\n",
      " - 93s - loss: 0.6950 - accuracy: 0.4741 - val_loss: 0.6954 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.68424\n",
      "Epoch 71/100\n",
      " - 92s - loss: 0.6959 - accuracy: 0.4633 - val_loss: 0.7006 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.68424\n",
      "Epoch 72/100\n",
      " - 92s - loss: 0.6992 - accuracy: 0.4723 - val_loss: 0.6868 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.68424\n",
      "Epoch 73/100\n",
      " - 92s - loss: 0.6957 - accuracy: 0.4812 - val_loss: 0.6953 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.68424\n",
      "Epoch 74/100\n",
      " - 92s - loss: 0.6952 - accuracy: 0.4866 - val_loss: 0.6930 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.68424\n",
      "Epoch 75/100\n",
      " - 10899s - loss: 0.6937 - accuracy: 0.4884 - val_loss: 0.6982 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.68424\n",
      "Epoch 76/100\n",
      " - 23878s - loss: 0.6951 - accuracy: 0.4794 - val_loss: 0.6934 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.68424\n",
      "Epoch 77/100\n",
      " - 96s - loss: 0.6953 - accuracy: 0.4937 - val_loss: 0.6901 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.68424\n",
      "Epoch 78/100\n",
      " - 99s - loss: 0.6962 - accuracy: 0.4848 - val_loss: 0.6957 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.68424\n",
      "Epoch 79/100\n",
      " - 97s - loss: 0.6951 - accuracy: 0.4902 - val_loss: 0.6904 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.68424\n",
      "Epoch 80/100\n",
      " - 93s - loss: 0.6933 - accuracy: 0.5116 - val_loss: 0.7004 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.68424\n",
      "Epoch 81/100\n",
      " - 93s - loss: 0.6978 - accuracy: 0.4669 - val_loss: 0.6951 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.68424\n",
      "Epoch 82/100\n",
      " - 93s - loss: 0.6948 - accuracy: 0.4955 - val_loss: 0.6919 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.68424\n",
      "Epoch 83/100\n",
      " - 93s - loss: 0.6946 - accuracy: 0.4580 - val_loss: 0.6967 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.68424\n",
      "Epoch 84/100\n",
      " - 93s - loss: 0.6940 - accuracy: 0.4955 - val_loss: 0.6926 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.68424\n",
      "Epoch 85/100\n",
      " - 95s - loss: 0.6958 - accuracy: 0.4919 - val_loss: 0.6986 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.68424\n",
      "Epoch 86/100\n",
      " - 93s - loss: 0.6952 - accuracy: 0.4937 - val_loss: 0.6927 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.68424\n",
      "Epoch 87/100\n",
      " - 93s - loss: 0.6947 - accuracy: 0.5027 - val_loss: 0.6940 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.68424\n",
      "Epoch 88/100\n",
      " - 94s - loss: 0.6950 - accuracy: 0.4919 - val_loss: 0.6977 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.68424\n",
      "Epoch 89/100\n",
      " - 93s - loss: 0.6949 - accuracy: 0.4866 - val_loss: 0.6882 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.68424\n",
      "Epoch 90/100\n",
      " - 93s - loss: 0.6951 - accuracy: 0.4812 - val_loss: 0.6936 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.68424\n",
      "Epoch 91/100\n",
      " - 94s - loss: 0.6956 - accuracy: 0.4669 - val_loss: 0.6967 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.68424\n",
      "Epoch 92/100\n",
      " - 94s - loss: 0.6933 - accuracy: 0.4919 - val_loss: 0.6917 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.68424\n",
      "Epoch 93/100\n",
      " - 93s - loss: 0.6963 - accuracy: 0.4830 - val_loss: 0.6907 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.68424\n",
      "Epoch 94/100\n",
      " - 95s - loss: 0.6941 - accuracy: 0.4776 - val_loss: 0.6922 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.68424\n",
      "Epoch 95/100\n",
      " - 95s - loss: 0.6938 - accuracy: 0.4973 - val_loss: 0.6943 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.68424\n",
      "Epoch 96/100\n",
      " - 96s - loss: 0.6953 - accuracy: 0.4919 - val_loss: 0.6906 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.68424\n",
      "Epoch 97/100\n",
      " - 105s - loss: 0.6936 - accuracy: 0.4973 - val_loss: 0.6961 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.68424\n",
      "Epoch 98/100\n",
      " - 94s - loss: 0.6946 - accuracy: 0.5116 - val_loss: 0.6898 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.68424\n",
      "Epoch 99/100\n",
      " - 95s - loss: 0.6944 - accuracy: 0.4741 - val_loss: 0.6948 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.68424\n",
      "Epoch 100/100\n",
      " - 96s - loss: 0.6961 - accuracy: 0.4723 - val_loss: 0.6980 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.68424\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath = 'LSTM.weights.best.hdf5', verbose = 1, save_best_only = True)\n",
    "hist = model32.fit(X_train_lstm, y_train, epochs = 100, batch_size=32, validation_split = 0.05, callbacks = [checkpointer], verbose = 2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy: 0.483871 \n"
     ]
    }
   ],
   "source": [
    "model32.load_weights('LSTM.weights.best.hdf5')\n",
    "predict21 = [1 if a>=0.5 else 0 for a in model32.predict(X_test_lstm)]\n",
    "acc = metrics.accuracy_score(y_test, predict21)\n",
    "print(\"Classification accuracy: %f \" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 1s 44ms/step\n",
      "Accuracy:  0.4838709533214569\n"
     ]
    }
   ],
   "source": [
    "model32.load_weights('LSTM.weights.best.hdf5')\n",
    "score = model32.evaluate(X_test_lstm, y_test, verbose=1)\n",
    "print(\"Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Appr 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99900, 14) (999,)\n"
     ]
    }
   ],
   "source": [
    "x = df.drop(columns = ['gesture', 'sample_num'])\n",
    "y = df.groupby(['sample_num']).agg(['min','max','sum','mean','std']).reset_index()['gesture']['min']\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89900, 14) (10000, 14)\n"
     ]
    }
   ],
   "source": [
    "train_size = int(999 * 0.9)\n",
    "test_size = len(y) - train_size\n",
    "train, test = x.iloc[0:(train_size*100)], x.iloc[train_size*100:len(x)]\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89900, 14) (10000, 14)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(train)\n",
    "x_test = scaler.transform(test)\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(899, 14, 100) (100, 14, 100)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.asarray(np.reshape(np.array(x_train), (int(x_train.shape[0]/100), 14, 100)))\n",
    "x_test = np.asarray(np.reshape(np.array(x_test), (int(x_test.shape[0]/100), 14, 100)))\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(899,) (100,)\n"
     ]
    }
   ],
   "source": [
    "y_train = y.iloc[0:train_size]\n",
    "y_test = y.iloc[train_size:len(y)]\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 14, 256)           365568    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 891,137\n",
      "Trainable params: 891,137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(LSTM(256,input_shape=(14, 100), return_sequences=True))\n",
    "model4.add(LSTM(256))\n",
    "model4.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model4.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 854 samples, validate on 45 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 0.7153 - accuracy: 0.5129 - val_loss: 0.7623 - val_accuracy: 0.0222\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.76228, saving model to LSTM_appr3.weights.best.hdf5\n",
      "Epoch 2/100\n",
      " - 3s - loss: 0.6860 - accuracy: 0.5445 - val_loss: 0.8326 - val_accuracy: 0.1111\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.76228\n",
      "Epoch 3/100\n",
      " - 3s - loss: 0.6686 - accuracy: 0.5925 - val_loss: 0.8580 - val_accuracy: 0.1556\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.76228\n",
      "Epoch 4/100\n",
      " - 3s - loss: 0.6642 - accuracy: 0.6171 - val_loss: 0.7170 - val_accuracy: 0.4222\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.76228 to 0.71696, saving model to LSTM_appr3.weights.best.hdf5\n",
      "Epoch 5/100\n",
      " - 3s - loss: 0.6291 - accuracy: 0.6534 - val_loss: 0.8738 - val_accuracy: 0.3556\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.71696\n",
      "Epoch 6/100\n",
      " - 3s - loss: 0.5996 - accuracy: 0.6756 - val_loss: 1.0886 - val_accuracy: 0.1556\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.71696\n",
      "Epoch 7/100\n",
      " - 3s - loss: 0.5404 - accuracy: 0.7143 - val_loss: 0.8422 - val_accuracy: 0.4889\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.71696\n",
      "Epoch 8/100\n",
      " - 3s - loss: 0.4932 - accuracy: 0.7600 - val_loss: 0.6672 - val_accuracy: 0.5778\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.71696 to 0.66722, saving model to LSTM_appr3.weights.best.hdf5\n",
      "Epoch 9/100\n",
      " - 3s - loss: 0.4461 - accuracy: 0.7799 - val_loss: 1.0530 - val_accuracy: 0.5111\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.66722\n",
      "Epoch 10/100\n",
      " - 3s - loss: 0.3459 - accuracy: 0.8478 - val_loss: 0.8730 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.66722\n",
      "Epoch 11/100\n",
      " - 3s - loss: 0.2876 - accuracy: 0.8923 - val_loss: 1.5077 - val_accuracy: 0.4222\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.66722\n",
      "Epoch 12/100\n",
      " - 3s - loss: 0.1924 - accuracy: 0.9297 - val_loss: 2.3852 - val_accuracy: 0.2667\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.66722\n",
      "Epoch 13/100\n",
      " - 4s - loss: 0.1682 - accuracy: 0.9344 - val_loss: 1.7180 - val_accuracy: 0.3778\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.66722\n",
      "Epoch 14/100\n",
      " - 5s - loss: 0.1149 - accuracy: 0.9555 - val_loss: 2.3510 - val_accuracy: 0.3556\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.66722\n",
      "Epoch 15/100\n",
      " - 4s - loss: 0.0753 - accuracy: 0.9754 - val_loss: 2.6390 - val_accuracy: 0.4000\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.66722\n",
      "Epoch 16/100\n",
      " - 4s - loss: 0.0548 - accuracy: 0.9813 - val_loss: 3.0625 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.66722\n",
      "Epoch 17/100\n",
      " - 4s - loss: 0.0455 - accuracy: 0.9859 - val_loss: 2.6597 - val_accuracy: 0.4889\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.66722\n",
      "Epoch 18/100\n",
      " - 4s - loss: 0.0400 - accuracy: 0.9836 - val_loss: 3.3025 - val_accuracy: 0.4000\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.66722\n",
      "Epoch 19/100\n",
      " - 4s - loss: 0.0291 - accuracy: 0.9930 - val_loss: 3.5558 - val_accuracy: 0.3778\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.66722\n",
      "Epoch 20/100\n",
      " - 3s - loss: 0.0578 - accuracy: 0.9801 - val_loss: 2.8058 - val_accuracy: 0.4444\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.66722\n",
      "Epoch 21/100\n",
      " - 4s - loss: 0.0330 - accuracy: 0.9918 - val_loss: 2.8242 - val_accuracy: 0.4889\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.66722\n",
      "Epoch 22/100\n",
      " - 3s - loss: 0.0146 - accuracy: 0.9965 - val_loss: 3.3733 - val_accuracy: 0.4444\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.66722\n",
      "Epoch 23/100\n",
      " - 3s - loss: 0.0068 - accuracy: 0.9977 - val_loss: 3.4081 - val_accuracy: 0.4889\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.66722\n",
      "Epoch 24/100\n",
      " - 4s - loss: 0.0067 - accuracy: 0.9965 - val_loss: 3.0727 - val_accuracy: 0.4444\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.66722\n",
      "Epoch 25/100\n",
      " - 4s - loss: 0.0043 - accuracy: 0.9988 - val_loss: 3.7805 - val_accuracy: 0.4889\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.66722\n",
      "Epoch 26/100\n",
      " - 3s - loss: 7.2793e-04 - accuracy: 1.0000 - val_loss: 3.7242 - val_accuracy: 0.4889\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.66722\n",
      "Epoch 27/100\n",
      " - 3s - loss: 2.7942e-04 - accuracy: 1.0000 - val_loss: 3.7931 - val_accuracy: 0.4889\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.66722\n",
      "Epoch 28/100\n",
      " - 3s - loss: 2.0288e-04 - accuracy: 1.0000 - val_loss: 3.8484 - val_accuracy: 0.4889\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.66722\n",
      "Epoch 29/100\n",
      " - 3s - loss: 1.6163e-04 - accuracy: 1.0000 - val_loss: 3.9115 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.66722\n",
      "Epoch 30/100\n",
      " - 4s - loss: 1.3626e-04 - accuracy: 1.0000 - val_loss: 3.9615 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.66722\n",
      "Epoch 31/100\n",
      " - 4s - loss: 1.1767e-04 - accuracy: 1.0000 - val_loss: 4.0017 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.66722\n",
      "Epoch 32/100\n",
      " - 3s - loss: 1.0424e-04 - accuracy: 1.0000 - val_loss: 4.0471 - val_accuracy: 0.4444\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.66722\n",
      "Epoch 33/100\n",
      " - 3s - loss: 9.2696e-05 - accuracy: 1.0000 - val_loss: 4.0880 - val_accuracy: 0.4444\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.66722\n",
      "Epoch 34/100\n",
      " - 3s - loss: 8.3475e-05 - accuracy: 1.0000 - val_loss: 4.1203 - val_accuracy: 0.4444\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.66722\n",
      "Epoch 35/100\n",
      " - 4s - loss: 7.5923e-05 - accuracy: 1.0000 - val_loss: 4.1428 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.66722\n",
      "Epoch 36/100\n",
      " - 4s - loss: 6.9618e-05 - accuracy: 1.0000 - val_loss: 4.1765 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.66722\n",
      "Epoch 37/100\n",
      " - 4s - loss: 6.3914e-05 - accuracy: 1.0000 - val_loss: 4.2077 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.66722\n",
      "Epoch 38/100\n",
      " - 4s - loss: 5.9072e-05 - accuracy: 1.0000 - val_loss: 4.2284 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.66722\n",
      "Epoch 39/100\n",
      " - 4s - loss: 5.4843e-05 - accuracy: 1.0000 - val_loss: 4.2544 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.66722\n",
      "Epoch 40/100\n",
      " - 5s - loss: 5.1123e-05 - accuracy: 1.0000 - val_loss: 4.2758 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.66722\n",
      "Epoch 41/100\n",
      " - 4s - loss: 4.7853e-05 - accuracy: 1.0000 - val_loss: 4.2991 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.66722\n",
      "Epoch 42/100\n",
      " - 4s - loss: 4.4862e-05 - accuracy: 1.0000 - val_loss: 4.3207 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.66722\n",
      "Epoch 43/100\n",
      " - 4s - loss: 4.2297e-05 - accuracy: 1.0000 - val_loss: 4.3438 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.66722\n",
      "Epoch 44/100\n",
      " - 4s - loss: 3.9866e-05 - accuracy: 1.0000 - val_loss: 4.3631 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.66722\n",
      "Epoch 45/100\n",
      " - 3s - loss: 3.7676e-05 - accuracy: 1.0000 - val_loss: 4.3758 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.66722\n",
      "Epoch 46/100\n",
      " - 3s - loss: 3.5657e-05 - accuracy: 1.0000 - val_loss: 4.4006 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.66722\n",
      "Epoch 47/100\n",
      " - 3s - loss: 3.3777e-05 - accuracy: 1.0000 - val_loss: 4.4181 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.66722\n",
      "Epoch 48/100\n",
      " - 3s - loss: 3.2096e-05 - accuracy: 1.0000 - val_loss: 4.4316 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.66722\n",
      "Epoch 49/100\n",
      " - 3s - loss: 3.0554e-05 - accuracy: 1.0000 - val_loss: 4.4472 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.66722\n",
      "Epoch 50/100\n",
      " - 3s - loss: 2.9143e-05 - accuracy: 1.0000 - val_loss: 4.4695 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.66722\n",
      "Epoch 51/100\n",
      " - 3s - loss: 2.7722e-05 - accuracy: 1.0000 - val_loss: 4.4879 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.66722\n",
      "Epoch 52/100\n",
      " - 3s - loss: 2.6528e-05 - accuracy: 1.0000 - val_loss: 4.4987 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.66722\n",
      "Epoch 53/100\n",
      " - 3s - loss: 2.5330e-05 - accuracy: 1.0000 - val_loss: 4.5136 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.66722\n",
      "Epoch 54/100\n",
      " - 3s - loss: 2.4288e-05 - accuracy: 1.0000 - val_loss: 4.5317 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.66722\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: 2.3248e-05 - accuracy: 1.0000 - val_loss: 4.5432 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.66722\n",
      "Epoch 56/100\n",
      " - 3s - loss: 2.2311e-05 - accuracy: 1.0000 - val_loss: 4.5618 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.66722\n",
      "Epoch 57/100\n",
      " - 3s - loss: 2.1420e-05 - accuracy: 1.0000 - val_loss: 4.5728 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.66722\n",
      "Epoch 58/100\n",
      " - 3s - loss: 2.0562e-05 - accuracy: 1.0000 - val_loss: 4.5849 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.66722\n",
      "Epoch 59/100\n",
      " - 3s - loss: 1.9794e-05 - accuracy: 1.0000 - val_loss: 4.5967 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.66722\n",
      "Epoch 60/100\n",
      " - 3s - loss: 1.9059e-05 - accuracy: 1.0000 - val_loss: 4.6114 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.66722\n",
      "Epoch 61/100\n",
      " - 3s - loss: 1.8371e-05 - accuracy: 1.0000 - val_loss: 4.6276 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.66722\n",
      "Epoch 62/100\n",
      " - 3s - loss: 1.7677e-05 - accuracy: 1.0000 - val_loss: 4.6371 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.66722\n",
      "Epoch 63/100\n",
      " - 3s - loss: 1.7101e-05 - accuracy: 1.0000 - val_loss: 4.6499 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.66722\n",
      "Epoch 64/100\n",
      " - 3s - loss: 1.6463e-05 - accuracy: 1.0000 - val_loss: 4.6560 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.66722\n",
      "Epoch 65/100\n",
      " - 3s - loss: 1.5909e-05 - accuracy: 1.0000 - val_loss: 4.6717 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.66722\n",
      "Epoch 66/100\n",
      " - 3s - loss: 1.5364e-05 - accuracy: 1.0000 - val_loss: 4.6800 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.66722\n",
      "Epoch 67/100\n",
      " - 3s - loss: 1.4872e-05 - accuracy: 1.0000 - val_loss: 4.6932 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.66722\n",
      "Epoch 68/100\n",
      " - 3s - loss: 1.4367e-05 - accuracy: 1.0000 - val_loss: 4.7056 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.66722\n",
      "Epoch 69/100\n",
      " - 3s - loss: 1.3918e-05 - accuracy: 1.0000 - val_loss: 4.7150 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.66722\n",
      "Epoch 70/100\n",
      " - 3s - loss: 1.3468e-05 - accuracy: 1.0000 - val_loss: 4.7252 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.66722\n",
      "Epoch 71/100\n",
      " - 3s - loss: 1.3038e-05 - accuracy: 1.0000 - val_loss: 4.7378 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.66722\n",
      "Epoch 72/100\n",
      " - 3s - loss: 1.2651e-05 - accuracy: 1.0000 - val_loss: 4.7467 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.66722\n",
      "Epoch 73/100\n",
      " - 3s - loss: 1.2269e-05 - accuracy: 1.0000 - val_loss: 4.7613 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.66722\n",
      "Epoch 74/100\n",
      " - 3s - loss: 1.1888e-05 - accuracy: 1.0000 - val_loss: 4.7692 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.66722\n",
      "Epoch 75/100\n",
      " - 3s - loss: 1.1539e-05 - accuracy: 1.0000 - val_loss: 4.7809 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.66722\n",
      "Epoch 76/100\n",
      " - 4s - loss: 1.1194e-05 - accuracy: 1.0000 - val_loss: 4.7935 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.66722\n",
      "Epoch 77/100\n",
      " - 3s - loss: 1.0878e-05 - accuracy: 1.0000 - val_loss: 4.8010 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.66722\n",
      "Epoch 78/100\n",
      " - 3s - loss: 1.0565e-05 - accuracy: 1.0000 - val_loss: 4.8118 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.66722\n",
      "Epoch 79/100\n",
      " - 3s - loss: 1.0269e-05 - accuracy: 1.0000 - val_loss: 4.8219 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.66722\n",
      "Epoch 80/100\n",
      " - 3s - loss: 9.9757e-06 - accuracy: 1.0000 - val_loss: 4.8297 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.66722\n",
      "Epoch 81/100\n",
      " - 4s - loss: 9.7027e-06 - accuracy: 1.0000 - val_loss: 4.8396 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.66722\n",
      "Epoch 82/100\n",
      " - 3s - loss: 9.4447e-06 - accuracy: 1.0000 - val_loss: 4.8503 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.66722\n",
      "Epoch 83/100\n",
      " - 4s - loss: 9.2000e-06 - accuracy: 1.0000 - val_loss: 4.8563 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.66722\n",
      "Epoch 84/100\n",
      " - 3s - loss: 8.9442e-06 - accuracy: 1.0000 - val_loss: 4.8712 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.66722\n",
      "Epoch 85/100\n",
      " - 4s - loss: 8.7101e-06 - accuracy: 1.0000 - val_loss: 4.8791 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.66722\n",
      "Epoch 86/100\n",
      " - 3s - loss: 8.4819e-06 - accuracy: 1.0000 - val_loss: 4.8854 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.66722\n",
      "Epoch 87/100\n",
      " - 3s - loss: 8.2642e-06 - accuracy: 1.0000 - val_loss: 4.8955 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.66722\n",
      "Epoch 88/100\n",
      " - 3s - loss: 8.0481e-06 - accuracy: 1.0000 - val_loss: 4.9084 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.66722\n",
      "Epoch 89/100\n",
      " - 3s - loss: 7.8469e-06 - accuracy: 1.0000 - val_loss: 4.9167 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.66722\n",
      "Epoch 90/100\n",
      " - 3s - loss: 7.6545e-06 - accuracy: 1.0000 - val_loss: 4.9250 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.66722\n",
      "Epoch 91/100\n",
      " - 3s - loss: 7.4586e-06 - accuracy: 1.0000 - val_loss: 4.9316 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.66722\n",
      "Epoch 92/100\n",
      " - 3s - loss: 7.2751e-06 - accuracy: 1.0000 - val_loss: 4.9405 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.66722\n",
      "Epoch 93/100\n",
      " - 3s - loss: 7.0977e-06 - accuracy: 1.0000 - val_loss: 4.9504 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.66722\n",
      "Epoch 94/100\n",
      " - 3s - loss: 6.9262e-06 - accuracy: 1.0000 - val_loss: 4.9560 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.66722\n",
      "Epoch 95/100\n",
      " - 4s - loss: 6.7648e-06 - accuracy: 1.0000 - val_loss: 4.9673 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.66722\n",
      "Epoch 96/100\n",
      " - 3s - loss: 6.5978e-06 - accuracy: 1.0000 - val_loss: 4.9734 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.66722\n",
      "Epoch 97/100\n",
      " - 4s - loss: 6.4471e-06 - accuracy: 1.0000 - val_loss: 4.9831 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.66722\n",
      "Epoch 98/100\n",
      " - 3s - loss: 6.2907e-06 - accuracy: 1.0000 - val_loss: 4.9916 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.66722\n",
      "Epoch 99/100\n",
      " - 3s - loss: 6.1511e-06 - accuracy: 1.0000 - val_loss: 4.9995 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.66722\n",
      "Epoch 100/100\n",
      " - 4s - loss: 6.0047e-06 - accuracy: 1.0000 - val_loss: 5.0061 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.66722\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath = 'LSTM_appr3.weights.best.hdf5', verbose = 1, save_best_only = True)\n",
    "hist = model4.fit(x_train, y_train, epochs = 100, batch_size=32, validation_split = 0.05, callbacks = [checkpointer], verbose = 2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 1ms/step\n",
      "Accuracy:  0.5099999904632568\n"
     ]
    }
   ],
   "source": [
    "model4.load_weights('LSTM_appr3.weights.best.hdf5')\n",
    "score = model4.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
